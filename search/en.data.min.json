[{"id":0,"href":"/usage/why-redrock/","title":"Why Redrock Postgres?","parent":"Usage","content":"Why Postgres?     Postgres is an advanced relational database management system that supports an extended subset of the SQL standard, including transactions, foreign keys, subqueries, triggers, user-defined types and functions. In short, Postgres has already gained a good reputation for data consistency, stability, high performance, feature rich and developer friendly.\nWhy Redrock Postgres?     Although Postgres has performed well in most senarios, there are still some areas for improvement, and that\u0026rsquo;s exactly what Redrock Postgres has done to achieve:\n Better control over bloat. Redrock Postgres will prevent bloat by allowing in-place updates in common cases and by reusing space as soon as a transaction that has performed a delete or non-in-place-update has committed. In short, whenever possible, we’ll avoid creating bloat in the first place. Reduce write amplification both by avoiding rewrites of heap pages and by making it possible to do an update that touches indexed columns without updating every index. Reduce the tuple size by shrinking the tuple header. Delete marking in indexes: This will allow inplace updates even when index columns are updated and additionally with this we can avoid the need for a dedicated vacuum process to perform retail deletes. Also, index only scan will be more stable. Statement level rollback, a transaction behavior compatible to those popular relational databases. No vacuum. No freeze, No transaction id wraparound.  "},{"id":1,"href":"/usage/getting-started/","title":"Getting Started","parent":"Usage","content":"   Installation Architectural Fundamentals Accessing Database Whats next?      Installation     Before you can use Redrock Postgres you need to install it, of course. It is possible that PostgreSQL is already installed at your site, either because it was included in your operating system distribution or because the system administrator already installed it. If that is the case, you should obtain information from the operating system documentation or your system administrator about how to access PostgreSQL.\nIf you are not sure whether PostgreSQL is already available or whether you can use it for your experimentation then you can install it yourself. Doing so is not hard and it can be a good exercise.\nIf you are installing Redrock Postgres yourself, then refer to Installation Guide for instructions on installation, and return to this guide when the installation is complete. Be sure to follow closely the section about setting up the appropriate environment variables.\nIf your site administrator has not set things up in the default way, you might have some more work to do. For example, if the database server machine is a remote machine, you will need to set the PGHOST environment variable to the name of the database server machine. The environment variable PGPORT might also have to be set. The bottom line is this: if you try to start an application program and it complains that it cannot connect to the database, you should consult your site administrator or, if that is you, the documentation to make sure that your environment is properly set up. If you did not understand the preceding paragraph then read the next section.\nArchitectural Fundamentals     Before we proceed, you should understand the basic PostgreSQL system architecture. Understanding how the parts of PostgreSQL interact will make this chapter somewhat clearer.\nIn database jargon, PostgreSQL uses a client/server model. A PostgreSQL session consists of the following cooperating processes (programs):\n A server process, which manages the database files, accepts connections to the database from client applications, and performs database actions on behalf of the clients. The database server program is called postgres. The user\u0026rsquo;s client (frontend) application that wants to perform database operations. Client applications can be very diverse in nature: a client could be a text-oriented tool, a graphical application, a web server that accesses the database to display web pages, or a specialized database maintenance tool. Some client applications are supplied with the PostgreSQL distribution; most are developed by users.  As is typical of client/server applications, the client and the server can be on different hosts. In that case they communicate over a TCP/IP network connection. You should keep this in mind, because the files that can be accessed on a client machine might not be accessible (or might only be accessible using a different file name) on the database server machine.\nThe PostgreSQL server can handle multiple concurrent connections from clients. To achieve this it starts (“forks”) a new process for each connection. From that point on, the client and the new server process communicate without intervention by the original postgres process. Thus, the supervisor server process is always running, waiting for client connections, whereas client and associated server processes come and go. (All of this is of course invisible to the user. We only mention it here for completeness.)\nAccessing Database     Once you have created a database, you can access it by:\n Running the PostgreSQL interactive terminal program, called psql, which allows you to interactively enter, edit, and execute SQL commands. Using an existing graphical frontend tool like pgAdmin or an office suite with ODBC or JDBC support to create and manipulate a database. These possibilities are not covered in this tutorial. Writing a custom application, using one of the several available language bindings. These possibilities are discussed further in Part IV.  You probably want to start up psql to try the examples in this tutorial. It can be activated for the postgres database by typing the command:\n$ psql postgres If you do not supply the database name then it will default to your user account name.\nIn psql, you will be greeted with the following message:\npsql (12.1) Type \u0026#34;help\u0026#34; for help. postgres=\u0026gt; The last line could also be:\npostgres=# That would mean you are a database superuser, which is most likely the case if you installed the PostgreSQL instance yourself. Being a superuser means that you are not subject to access controls. For the purposes of this tutorial that is not important.\nThe last line printed out by psql is the prompt, and it indicates that psql is listening to you and that you can type SQL queries into a work space maintained by psql. Try out these commands:\npostgres=\u0026gt; SELECT version(); version ------------------------------------------------------------------------------------------ PostgreSQL 12.1 on x86_64-pc-linux-gnu, compiled by gcc (Debian 4.9.2-10) 4.9.2, 64-bit (1 row) postgres=\u0026gt; SELECT current_date; date ------------ 2016-01-07 (1 row) postgres=\u0026gt; SELECT 2 + 2; ?column? ---------- 4 (1 row) The psql program has a number of internal commands that are not SQL commands. They begin with the backslash character, “\\”. For example, you can get help on the syntax of various PostgreSQL SQL commands by typing:\npostgres=\u0026gt; \\h To get out of psql, type:\npostgres=\u0026gt; \\q and psql will quit and return you to your command shell. (For more internal commands, type \\? at the psql prompt.) The full capabilities of psql are documented in psql. In this tutorial we will not use these features explicitly, but you can use them yourself when it is helpful.\nWhats next?     There are a lot more things to discover. To get more information, please refer to PostgreSQL Documentation.\n"},{"id":2,"href":"/installation/windows/","title":"Installing on Windows","parent":"Installation","content":"To perform an installation using the graphical installation wizard, you must have superuser or administrator privileges.\nThe following section walks you through installing PostgreSQL on a Windows host.\nTo start the installation wizard, assume sufficient privileges and double-click the installer icon; if prompted, provide a password.\nNote that in some versions of Windows, to invoke the installer with Administrator privileges, you need to right-click on the installer icon and select Run as Administrator from the context menu.\nThe Redrock 12 Setup Welcome window opens. Click Next to continue.\n    Fig. 1: The Redrock 12 Setup Welcome dialog    The Choose Install Location window opens. Accept the default installation directory, or specify an alternate location and click Next to continue.\n    Fig. 2: Choose Install Location dialog    Click Next to continue.\nThe Choose Data Directory window opens. Accept the default location or specify the name of the alternate directory in which you wish to store the data files.\n    Fig. 3: Choose Data Directory dialog    Click Next to continue.\nThe Server Options window opens.\n    Fig. 4: The Server Options dialog    PostgreSQL uses the Port field to specify the port number on which the server should listen. The default listener port is 5432.\nUse the Locale field to specify the locale that will be used by the new database cluster. The Default is the operating system locale.\nUse the Superuser field to specify the database superuser name. The default superuser name is postgres.\nUse the Password field to specify the database superuser password. The specified password should conform to enough complexities. After entering a password in the Password field, and confirming the password in the Retype Password field.\nClick Next to continue.\n    Fig. 5: Setting Configuration Parameters dialog    The Setting Configuration Parameters dialog will guides you to optimize server performance by setting some important configuration parameters. Maybe you can let the installer tune configuration parameters automatically, and click Next to continue.\n    Fig. 6: The Choose Start Menu Folder    During the installation, the setup wizard confirms the installation progress of PostgreSQL via a series of progress bars.\n    Fig. 7: The Installing dialog    When the Completing Redrock 12 Setup window appears, Congratulations! Click Finish to complete the PostgreSQL installation.\n    Fig. 8: Completing Redrock 12 Setup    "},{"id":3,"href":"/usage/","title":"Usage","parent":"Welcome to Redrock Documentation","content":""},{"id":4,"href":"/installation/","title":"Installation","parent":"Welcome to Redrock Documentation","content":"Table of Contents\n   Installing on Windows      Installing on Red Hat      Installing on Debian      The following sections of the chapter describe how to install the distribution that you choose.\n"},{"id":5,"href":"/features/","title":"Features","parent":"Welcome to Redrock Documentation","content":""},{"id":6,"href":"/installation/redhat/","title":"Installing on Red Hat","parent":"Installation","content":"   Install Packages  Prerequisite Install Redrock Postgres   Post-installation commands  Data Directory Change Data Directory Initialize Startup   Control service After installation      Install Packages     Prerequisite     Log in to the host with your root account, and run the following commands to query installed packages, make sure none postgresql related packages already installed:\n# rpm -qa | grep postgresql Install Redrock Postgres     Download the Redrock Postgres Package for Red Hat or CentOS, and run the following commands to install:\n# tar xf redrock-12.1-1.el8.x86_64-bundle.tar # cd redrock-12.1-1.el8.x86_64-bundle # rpm -ivh redrock-12.1-1.el8.x86_64.rpm redrock-libs-12.1-1.el8.x86_64.rpm redrock-server-12.1-1.el8.x86_64.rpm Other packages can be installed according to your needs.\nPost-installation commands     After installing the packages, a database needs to be initialized and configured.\nIn the commands below, the value of version includes the major version of PostgreSQL, e.g., 12\nData Directory     The PostgreSQL data directory contains all of the data files for the database. The variable PGDATA is used to reference this directory.\nThe default data directory is:\n/var/lib/pgsql/\u0026lt;version\u0026gt;/data For example:\n/var/lib/pgsql/12/data Change Data Directory     If you do not want to use the default data directory, you can go to a custom mount point (eg: /u01) and create a folder pgdata with postgres permissions:\n# cd /u01 # mkdir pgdata # chown postgres:postgres pgdata Then, edit the postgresql service:\n# systemctl edit postgresql-12.service Go to the custom mount point that has the majority of the disk space, copy and paste the following into that file:\n[Service] Environment=PGDATA=/u01/pgdata Initialize     The first command (only needed once) is to initialize the database in PGDATA.\nIf the previous command did not work, try directly calling the setup binary, located in a similar naming scheme:\n# /usr/pgsql-\u0026lt;version\u0026gt;/bin/postgresql-\u0026lt;version\u0026gt;-setup initdb For versions 12, use:\n# /usr/pgsql-12/bin/postgresql-12-setup initdb Startup     If you want PostgreSQL to start automatically when the OS starts, do the following:\n# systemctl enable postgresql-12.service Control service     To control the database service, use:\n# systemctl \u0026lt;command\u0026gt; postgresql-12.service where command can be:\n enable : enable automatical start start : start the database stop : stop the database restart : stop/start the database; used to read changes to core configuration files reload : reload configuration files while keeping database running  E.g. to control version 12 database service, use:\n# systemctl enable postgresql-12.service # systemctl start postgresql-12.service After installation     Modify the pg_hba.conf file in data directory to define what authentication method should be used from all networks to the PostgreSQL server and modify the localhost authentication method (change from indent to md5 and change from localhost to accept all incoming requests). Find the lines below:\n# IPv4 local connections: host all all 127.0.0.1/32 ident And change it to:\n# IPv4 local connections: host all all 0.0.0.0/0 md5 Modify the postgresql.conf file in data directory to allow connections from all hosts by uncommenting the following line and adding an * instead of localhost:\nlisten_addresses = \u0026#39;*\u0026#39; Restart the database service to reload configurations:\n# systemctl restart postgresql-12.service In a production environment, you should also set up TLS-secured communication, and you should consider setting up data replication or snapshot-based backups. Consult the PostgreSQL online manual for these settings.\n"},{"id":7,"href":"/advanced/","title":"Advanced","parent":"Welcome to Redrock Documentation","content":""},{"id":8,"href":"/usage/configuration/","title":"Configuration","parent":"Usage","content":"Overview of all modified and new server configuration options based on PostgreSQL.\n   Resource Consumption Write Ahead Log Run-time Statistics Automatic Analyzing      Resource Consumption     These settings control the resource consumption of the database server.\n  threaded_execution (boolean)\nControls whether to enable the multithreaded Postgres model. In Redrock Postgres, the multithreaded Postgres model enables Postgres processes on UNIX and Linux to run as operating system threads in separate address spaces.\nBy default, session related backend processes on Windows always use threaded execution; the remaining background processes run as operating system processes. Thus, an \u0026ldquo;Postgres process\u0026rdquo; is not always equivalent to an \u0026ldquo;operating system process.\u0026rdquo;\n  Write Ahead Log     There are several WAL-related configuration parameters that affect database performance.\n  data_checksums (boolean)\nDetermines whether to calculate a checksum when writing a data block (a number calculated from all the bytes stored in the block) and store it in the page header of every data block when writing it to disk. Checksums are verified when a block is read - only if this parameter is on and the last write of the block stored a checksum. Besides, Postgres also verifies the checksum before a change application from update/delete statements and recomputes it after the change is applied. In addition, Postgres gives every WAL record a checksum before writing it to disk.\nIf this parameter is set to off, data checksums are turned off.\nChecksums allow Postgres to detect corruption caused by underlying disks, storage systems, or I/O systems. Turning on data checksums causes only an additional 1% to 2% overhead.\n  Run-time Statistics     These parameters control server-wide statistics collection features. When statistics collection is enabled, the data that is produced can be accessed via the pg_stat and pg_statio family of system views. Refer to Statistics Views for more information.\n  track_wait_events (boolean)\nEnables the collection of information on the wait events occured in each session, include occured times and timing information. This parameter is on by default. Note that even when enabled, this information is not visible to all users, only to superusers and the user owning the session being reported on, so it should not represent a security risk. Only superusers can change this setting.\n  Automatic Analyzing     These settings control the behavior of the autoanalyze feature. Refer to Section 24.1.6 for more information. Note that many of these settings can be overridden on a per-table basis; see Storage Parameters.\n  autoanalyze (boolean)\nControls whether the server should run the autoanalyze launcher daemon. This is on by default; however, track_counts must also be enabled for autoanalyze to work. This parameter can only be set in the postgresql.conf file or on the server command line; however, autoanalyzing can be disabled for individual tables by changing table storage parameters.\n  log_autoanalyze_min_duration (integer)\nCauses each action executed by autoanalyze to be logged if it ran for at least the specified amount of time. Setting this to zero logs all autoanalyze actions. -1 (the default) disables logging autoanalyze actions. If this value is specified without units, it is taken as milliseconds. For example, if you set this to 250ms then all automatic analyzes that run 250ms or longer will be logged. In addition, when this parameter is set to any value other than -1, a message will be logged if an autoanalyze action is skipped due to a conflicting lock or a concurrently dropped relation. Enabling this parameter can be helpful in tracking autoanalyze activity. This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.\n  autoanalyze_max_workers (integer)\nSpecifies the maximum number of autoanalyze processes (other than the autoanalyze launcher) that may be running at any one time. The default is three. This parameter can only be set at server start.\n  autoanalyze_naptime (integer)\nSpecifies the minimum delay between autoanalyze runs on any given database. In each round the daemon examines the database and issues ANALYZE commands as needed for tables in that database. If this value is specified without units, it is taken as seconds. The default is one minute (1min). This parameter can only be set in the postgresql.conf file or on the server command line.\n  autoanalyze_base_threshold (integer)\nSpecifies the minimum number of inserted, updated or deleted tuples needed to trigger an ANALYZE in any one table. The default is 50 tuples. This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.\n  autoanalyze_scale_factor (floating point)\nSpecifies a fraction of the table size to add to autoanalyze_base_threshold when deciding whether to trigger an ANALYZE. The default is 0.1 (10% of table size). This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.\n  "},{"id":9,"href":"/installation/debian/","title":"Installing on Debian","parent":"Installation","content":"   Install Packages  Prerequisite Install Redrock Postgres   Control service After installation      Install Packages     Prerequisite     Log in to the host with your root account, and run the following commands to install dependent packages:\n# apt install postgresql-common postgresql-client-common Install Redrock Postgres     Download the Redrock Postgres Package for Debian, and run the following commands to install:\n# tar xf redrock-12.1-1.debian10.x86_64-bundle.tar # cd redrock-12.1-1.debian10.x86_64-bundle # dpkg -i libpq5_12.1-1_amd64.deb redrock-client_12.1-1_amd64.deb redrock_12.1-1_amd64.deb Other packages can be installed according to your needs.\nControl service     To control the database service, use:\n# systemctl \u0026lt;command\u0026gt; postgresql where command can be:\n enable : enable automatical start start : start the database stop : stop the database restart : stop/start the database; used to read changes to core configuration files reload : reload configuration files while keeping database running  After installation     Modify the pg_hba.conf file in configuration directory (default: /etc/postgresql/12/main/) to define what authentication method should be used from all networks to the PostgreSQL server and modify the localhost authentication method (change from indent to md5 and change from localhost to accept all incoming requests). Find the lines below:\n# IPv4 local connections: host all all 127.0.0.1/32 ident And change it to:\n# IPv4 local connections: host all all 0.0.0.0/0 md5 Modify the postgresql.conf file in configuration directory (default: /etc/postgresql/12/main/ ) to allow connections from all hosts by uncommenting the following line and adding an * instead of localhost:\nlisten_addresses = \u0026#39;*\u0026#39; Restart the database service to reload configurations:\n# systemctl restart postgresql In a production environment, you should also set up TLS-secured communication, and you should consider setting up data replication or snapshot-based backups. Consult the PostgreSQL online manual for these settings.\n"},{"id":10,"href":"/admin/","title":"Server Administration","parent":"Welcome to Redrock Documentation","content":"This part covers topics that are of interest to a Redrock Postgres database administrator.\nTable of Contents\n   Subscribing DDL Commands      "},{"id":11,"href":"/advanced/textsearch/","title":"Chinese Text Search","parent":"Advanced","content":"Full Text Searching (or just text search) provides the capability to identify natural-language documents that satisfy a query, and optionally to sort them by relevance to the query. The most common type of search is to find all documents containing given query terms and return them in order of their similarity to the query. Notions of query and similarity are very flexible and depend on the specific application. The simplest search considers query as a set of words and similarity as the frequency of query words in the document.\nParsers     Text search parsers are responsible for splitting raw document text into tokens and identifying each token\u0026rsquo;s type, where the set of possible types is defined by the parser itself. Note that a parser does not modify the text at all — it simply identifies plausible word boundaries. Because of this limited scope, there is less need for application-specific custom parsers than there is for custom dictionaries. At present Redrock Postgres provides just two built-in parser, which has been found to be useful for a wide range of applications.\nThe built-in chinese text parser is named pg_catalog.cjkparser.\nConfigurations     Full text search functionality includes the ability to do many more things: skip indexing certain words (stop words), process synonyms, and use sophisticated parsing, e.g., parse based on more than just white space. This functionality is controlled by text search configurations. PostgreSQL comes with predefined configurations for many languages, and you can easily create your own configurations. (psql\u0026rsquo;s \\dF command shows all available configurations.)\nThe built-in chinese text search configuration is named pg_catalog.chinese.\nTesting and Debugging Text Search     The behavior of a custom text search configuration can easily become confusing. The functions described in this section are useful for testing text search objects. You can test a complete configuration, or test parsers and dictionaries separately.\nConfiguration Testing     The function ts_debug allows easy testing of a text search configuration.\nSELECT * FROM ts_debug(\u0026#39;chinese\u0026#39;, \u0026#39;关系数据库是一种用于存储相互关联的数据记录并提供数据访问的数据库。\u0026#39;);\ralias | description | token | dictionaries | dictionary | lexemes\r------------+--------------+--------+--------------+------------+----------\rcjk_words | CJK words | 关系 | {chinese} | chinese | {关系}\rcjk_words | CJK words | 数据库 | {chinese} | chinese | {数据库}\rcjk_words | CJK words | 是 | {chinese} | chinese | {是}\rcjk_words | CJK words | 一种 | {chinese} | chinese | {一种}\rcjk_words | CJK words | 用于 | {chinese} | chinese | {用于}\rcjk_words | CJK words | 存储 | {chinese} | chinese | {存储}\rcjk_words | CJK words | 相互 | {chinese} | chinese | {相互}\rcjk_words | CJK words | 关联 | {chinese} | chinese | {关联}\rcjk_words | CJK words | 的 | {chinese} | chinese | {的}\rcjk_words | CJK words | 数据 | {chinese} | chinese | {数据}\rcjk_words | CJK words | 记录 | {chinese} | chinese | {记录}\rcjk_words | CJK words | 并 | {chinese} | chinese | {并}\rcjk_words | CJK words | 提供 | {chinese} | chinese | {提供}\rcjk_words | CJK words | 数据 | {chinese} | chinese | {数据}\rcjk_words | CJK words | 访问 | {chinese} | chinese | {访问}\rcjk_words | CJK words | 的 | {chinese} | chinese | {的}\rcjk_words | CJK words | 数据库 | {chinese} | chinese | {数据库}\rpunc_words | punctuations | 。 | {chinese} | chinese | {。} Parser Testing     The function ts_parse allows direct testing of a text search parser.\nSELECT * FROM ts_parse(\u0026#39;cjkparser\u0026#39;, \u0026#39;关系数据库是一种用于存储相互关联的数据记录并提供数据访问的数据库。\u0026#39;);\rtokid | token\r-------+--------\r1 | 关系\r1 | 数据库\r1 | 是\r1 | 一种\r1 | 用于\r1 | 存储\r1 | 相互\r1 | 关联\r1 | 的\r1 | 数据\r1 | 记录\r1 | 并\r1 | 提供\r1 | 数据\r1 | 访问\r1 | 的\r1 | 数据库\r18 | 。 "},{"id":12,"href":"/advanced/plscheme/","title":"PL/Scheme","parent":"Advanced","content":"PL/Scheme is a PostgreSQL procedural language handler for Scheme programming language. PL/scheme uses Chibi Scheme in the background as its Scheme interpreter. With lots of builtin SRFIs and complete R7RS compliancy of Chibi Scheme, PL/scheme can power up PostgreSQL procedures in a Lisp style.\nPL/Scheme Functions and Arguments     To create a function in the PL/Scheme language, use the standard CREATE FUNCTION syntax:\nCREATE FUNCTION funcname (argument-types) RETURNS return-type -- function attributes can go here AS $$ # PL/Scheme function body goes here $$ LANGUAGE plscheme; The body of the function is ordinary Scheme code. In fact, the PL/Scheme glue code wraps it inside a Scheme subroutine. A PL/Scheme function is called in a scalar context, so it can\u0026rsquo;t return a list. You can return non-scalar values (arrays, records, and sets) by returning a reference, as discussed below.\nIn a PL/Scheme procedure, any return value from the Scheme code is ignored.\nPL/Scheme also supports anonymous code blocks called with the DO statement:\nDO $$ # PL/Scheme code $$ LANGUAGE plscheme; An anonymous code block receives no arguments, and whatever value it might return is discarded. Otherwise it behaves just like a function.\nThe syntax of the CREATE FUNCTION command requires the function body to be written as a string constant. It is usually most convenient to use dollar quoting (see Section 4.1.2.4) for the string constant. If you choose to use escape string syntax E'', you must double any single quote marks (') and backslashes (\\) used in the body of the function (see Section 4.1.2.1).\nArguments and results are handled as in any other Scheme subroutine: arguments are passed as Scheme variables, and a result value is returned as the last expression evaluated in the function.\nFor example, a function returning the greater of two integer values could be defined as:\nCREATE FUNCTION scheme_max (a integer, b integer) RETURNS integer AS $$ (if (\u0026gt; a b) a b) $$ LANGUAGE plscheme; "},{"id":13,"href":"/features/undo/","title":"Undo","parent":"Features","content":"   Undos and Transactions Transaction Rollback Managing Undos      Redrock Postgres maintains records of the actions of transactions, collectively known as undo data. Redrock Postgres uses undo to do the following:\n Roll back an active transaction Recover a terminated transaction Provide read consistency  Redrock Postgres stores undo data inside the database rather than in external logs. Undo data is stored in blocks that are updated just like data blocks, with changes to these blocks generating redo. In this way, Redrock Postgres can efficiently access undo data without needing to read external logs.\nUndo data is stored in an independent tablespace. Redrock Postgres provides a fully automated mechanism, known as automatic undo management mode, for managing undos and space.\nUndos and Transactions     When a transaction starts, the database binds (assigns) the transaction to an undo segment, and therefore to a transaction table.\nMultiple active transactions can write concurrently to the same undo or to different undos. For example, transactions T1 and T2 can both write to undo U1, or T1 can write to U1 while T2 writes to undo U2.\nTransaction Rollback     When a ROLLBACK statement is issued, the database uses undo records to roll back changes made to the database by the uncommitted transaction. During recovery, the database rolls back any uncommitted changes applied from the online redo log to the data files. Undo records provide read consistency by maintaining the before image of the data for users accessing data at the same time that another user is changing it.\nManaging Undos     After the database cluster data directory is initialized, 4 cluster-level undos and 8 database-level undos are generated by default in the database. Users can use the following statement to view the undo information in the current database:\nSELECT c.relname, c.relisshared, u.undoid, u.undrelid, pg_size_pretty(pg_relation_size(c.oid)) as undsize FROM pg_undo u LEFT JOIN pg_class c ON u.undrelid = c.oid; "},{"id":14,"href":"/features/recyclebin/","title":"Recycle Bin","parent":"Features","content":"   What Is the Recycle Bin?  Object Naming in the Recycle Bin   Viewing and Querying Objects in the Recycle Bin Purging Objects in the Recycle Bin Restoring Tables from the Recycle Bin      When you drop a table, the database does not immediately remove the space associated with the table. The database renames the table and places it and any associated objects in a recycle bin, where, in case the table was dropped in error, it can be recovered at a later time.\nWhat Is the Recycle Bin?     The recycle bin is actually a data dictionary table containing information about dropped objects. Dropped tables and any associated objects such as indexes, constraints, types, and so on are not removed and still occupy space.\nThey continue to count against user space quotas, until specifically purged from the recycle bin or the unlikely situation where they must be purged by the database because of tablespace space constraints.\nEach user can be thought of as having his own recycle bin, because, unless a user has the SUPERUSER privilege, the only objects that the user has access to in the recycle bin are those that the user owns. A user can view the objects in his schema in the recycle bin using the following statement:\nSELECT * FROM pg_catalog.pg_recyclebin WHERE namespace = to_regnamespace(CURRENT_SCHEMA)::oid; Objects dropped by the DDL command are moved to the recycle bin. Dropped objects that are moved to the recycle bin can include the following types of objects:\n Tables Undos Indexes Constraints Types Functions  When you drop a tablespace, the objects in the tablespace are not placed in the recycle bin and the database purges any entries in the recycle bin for objects located in the tablespace. Likewise:\n When you drop a user, any objects belonging to the user are not placed in the recycle bin and any objects in the recycle bin are purged. When you drop a schema, any objects belonging to the schema are not placed in the recycle bin and any objects in the recycle bin are purged.  Object Naming in the Recycle Bin     When a dropped table is moved to the recycle bin, the table and its associated objects are given system-generated names. This is necessary to avoid name conflicts that may arise if multiple tables have the same name. This could occur under the following circumstances:\n A user drops a table, re-creates it with the same name, then drops it again. Two users have tables with the same name, and both users drop their tables.  The renaming convention is as follows:\npg_dropped_[object_type]_[object_id] where:\n object_type is the object type name, eg: table, index, constraint, type, function. object_id is a unique identifier for this object, which makes the recycle bin name unique in current database.  Viewing and Querying Objects in the Recycle Bin     You can query system catalog pg_recyclebin to identify the name that the database has assigned to a dropped object, as shown in the following example:\nSELECT objname, oldname FROM pg_catalog.pg_recyclebin WHERE namespace = to_regnamespace(\u0026#39;hr\u0026#39;)::oid; objname | oldname -----------------------+----------- pg_dropped_table_18762 | employees You can also view the contents of the recycle bin using the GUI management tool like DBeaver, pgAdmin 4, expand the system schema pg_recyclebin under a specified database to view all the dropped objects.\nYou can query objects that are in the recycle bin, just as you can query other objects. However, you must specify the name of the object as it is identified in the recycle bin. For example:\nSELECT * FROM pg_recyclebin.pg_dropped_table_18762; Purging Objects in the Recycle Bin     If you decide that you are never going to restore an item from the recycle bin, then you can use the VACUUM statement to remove the items and their associated objects from the recycle bin and release their storage space. You need the same privileges as if you were dropping the item.\nWhen you use the VACUUM statement to purge a table, you can use the name that the table is known by in the recycle bin or the original name of the table. The recycle bin name can be obtained from system catalog pg_recyclebin as shown in \u0026ldquo;Viewing and Querying Objects in the Recycle Bin\u0026rdquo;. The following hypothetical example purges the table hr.employees, which was renamed to pg_recyclebin.pg_dropped_table_18762 when it was placed in the recycle bin:\nVACUUM pg_recyclebin.pg_dropped_table_18762; You can achieve the same result with the following statement:\nVACUUM hr.employees; If you have the SUPERUSER privilege or you are the current database owner, then you can purge the entire recycle bin, and release space for objects, by using the following statement:\nVACUUM; You can also use the VACUUM statement to purge an index or undo from the recycle bin.\nRestoring Tables from the Recycle Bin     Use the CREATE TABLE \u0026hellip; LIKE statement to recover objects from the recycle bin.\nYou should specify the name of the table in the recycle bin. The recycle bin name can be obtained from system catalog pg_recyclebin as shown in \u0026ldquo;Viewing and Querying Objects in the Recycle Bin\u0026rdquo;. To use the CREATE TABLE \u0026hellip; LIKE statement, you need the SELECT privileges required to access the dropped table.\nThe following example restores employees table and assigns to it a new name:\nCREATE TABLE hr.employees2 (LIKE pg_recyclebin.pg_dropped_table_18762 INCLUDING ALL); INSERT INTO hr.employees2 SELECT * FROM pg_recyclebin.pg_dropped_table_18762; The system-generated recycle bin name is very useful if you have dropped a table multiple times. For example, suppose you have three versions of the employees table in the recycle bin and you want to recover the second version. You can query the recycle bin and then restore to the appropriate system-generated name, as shown in the following example. Including the drop time in the query can help you verify that you are restoring the correct table.\nSELECT objname, oldname, droptime FROM pg_catalog.pg_recyclebin; objname | oldname | droptime -----------------------+-----------+-------------------- pg_dropped_table_18762 | employees | 2006-02-05:21:05:52 pg_dropped_table_18924 | employees | 2006-02-05:21:25:13 pg_dropped_table_19510 | employees | 2006-02-05:22:05:53 CREATE TABLE hr.employees2 (LIKE pg_recyclebin.pg_dropped_table_18924 INCLUDING ALL); INSERT INTO hr.employees2 SELECT * FROM pg_recyclebin.pg_dropped_table_18924; "},{"id":15,"href":"/features/statement-atomicity/","title":"Statement Level Atomicity","parent":"Features","content":"Sample Transaction: Account Debit and Credit     To illustrate the concept of a transaction, consider a banking database. When a customer transfers money from a savings account to a checking account, the transaction must consist of three separate operations:\n  Decrement the savings account\n  Increment the checking account\n  Record the transaction in the transaction journal\n  Redrock Postgres must allow for two situations. If all three SQL statements maintain the accounts in proper balance, then the effects of the transaction can be applied to the database. However, if a problem such as insufficient funds, invalid account number, or a hardware failure prevents one or two of the statements in the transaction from completing, then the database must roll back the entire transaction so that the balance of all accounts is correct.\nFigure 1 illustrates a banking transaction. The first statement subtracts $500 from savings account 3209. The second statement adds $500 to checking account 3208. The third statement inserts a record of the transfer into the journal table. The final statement commits the transaction.\n    Figure 1 A Banking Transaction    Statement Level Atomicity     Redrock Postgres supports statement-level atomicity, which means that a SQL statement is an atomic unit of work and either completely succeeds or completely fails.\nA successful statement is different from a committed transaction. A single SQL statement executes successfully if the database parses and runs it without error as an atomic unit, as when all rows are changed in a multirow update.\nIf a SQL statement causes an error during execution, then it is not successful and so all effects of the statement are rolled back. This operation is a statement-level rollback. This operation has the following characteristics:\n  A SQL statement that does not succeed causes the loss only of work it would have performed itself.\nThe unsuccessful statement does not cause the loss of any work that preceded it in the current transaction. For example, if the execution of the second UPDATE statement in Figure 1 causes an error and is rolled back, then the work performed by the first UPDATE statement is not rolled back. The first UPDATE statement can be committed or rolled back explicitly by the user.\n  The effect of the rollback is as if the statement had never been run.\nAny side effects of an atomic statement, for example, triggers invoked upon execution of the statement, are considered part of the atomic statement. Either all work generated as part of the atomic statement succeeds or none does.\n  An example of an error causing a statement-level rollback is an attempt to insert a duplicate primary key. Single SQL statements involved in a deadlock, which is competition for the same data, can also cause a statement-level rollback. However, errors discovered during SQL statement parsing, such as a syntax error, have not yet been run and so do not cause a statement-level rollback.\n"},{"id":16,"href":"/features/network-tablespace/","title":"Network Attached Tablespace","parent":"Features","content":"   Planning Deploying Compute Server Deploying Storage Server Create a Network Attached Database Create a Network Attached Tablespace      Redrock Postgres supports network attached tablespaces. Based on this capability, Redrock Postgres can deploy database computing and storage separately.\nPlanning     Prepare the computing server and storage server environments. The recommended environment configurations are as follows:\n Computing servers should use high processor and memory configurations. The storage server should use the storage device with high I/O throughput, and the storage space must meet the data storage requirements. Use high-bandwidth and low-latency networks between computing servers and storage servers.  If you need to create multiple network tablespaces on the database server, you need to prepare multiple storage servers.\nDeploying Compute Server     To deploy a compute server, perform the following steps:\n Install the software package by referring to Redrock Postgres Installation Guide. Stop the server, if it\u0026rsquo;s running. Perform the backup, using any convenient file-system-backup tool such as tar or cpio (not pg_dump or pg_dumpall). Start the server.  Deploying Storage Server     To deploy the storage server, perform the following steps:\n Install the software package by referring to Redrock Postgres Installation Guide. Stop the server, if it\u0026rsquo;s running. Remove all existing files and subdirectories under the cluster data directory. Restore the database files from your file system backup. Be sure that they are restored with the right ownership (the database system user, not root!) and with the right permissions. Set the listening address and port in postgresql.conf and create a storage.signal file in the cluster data directory. Modify pg_hba.conf to allow connections from the compute server. Start the server.  Create a Network Attached Database     Log in to the computing server, connect to the postgres database in the database service as the database superuser, and create a network attached database. The following is a simple configuration example:\nCREATE DATABASE netdb STORAGE remote LOCATION\r\u0026#39;host=192.168.1.50 port=5432 user=postgres password=pgpass\u0026#39;;   In Redrock Postgres, creating a database creates a default tablespace in the database, and the newly created database in the example above produces a default network tablespace.  Create a Network Attached Tablespace     Use a database superuser to connect to the newly created database netdb in the database service to create a network attached tablespace. The following is a simple example of a configuration that creates two network attached tablespaces using two additional storage servers:\nCREATE TABLESPACE netts1 STORAGE remote LOCATION\r\u0026#39;host=192.168.1.51 port=5432 user=postgres password=pgpass\u0026#39;;\rCREATE TABLESPACE netts2 STORAGE remote LOCATION\r\u0026#39;host=192.168.1.52 port=5432 user=postgres password=pgpass\u0026#39;; You can then use these network attached tablespaces to create tables and indexes.\n"},{"id":17,"href":"/commands/","title":"SQL Commands","parent":"Welcome to Redrock Documentation","content":"This part contains reference information for the SQL commands supported by Redrock Postgres. By “SQL” the language in general is meant; information about the standards conformance and compatibility of each command can be found on the respective reference page.\nTable of Contents\n ALTER UNDO — change the definition of an undo CREATE UNDO — define a new undo DROP UNDO — remove an undo VACUUM — garbage-collect a database  "},{"id":18,"href":"/advanced/monitor-stats/","title":"Statistics Views","parent":"Advanced","content":"   Statistics Collection Configuration Viewing Statistics pg_stat_activity pg_stat_wait_event pg_stat_replication pg_stat_database pg_stat_all_tables pg_stat_all_undos pg_stat_all_indexes pg_statio_all_tables pg_statio_all_undos pg_statio_all_indexes pg_statio_all_sequences      Redrock Postgres\u0026rsquo;s statistics collector is a subsystem that supports collection and reporting of information about server activity. Presently, the collector can count accesses to tables and indexes in both disk-block and individual-row terms. It also tracks the total number of rows in each table, and information about analyze actions for each table. It can also count calls to user-defined functions and the total time spent in each one.\nRedrock Postgres also supports reporting dynamic information about exactly what is going on in the system right now, such as the exact command currently being executed by other server processes, and which other connections exist in the system. This facility is independent of the collector process.\nStatistics Collection Configuration     Since collection of statistics adds some overhead to query execution, the system can be configured to collect or not collect information. This is controlled by configuration parameters that are normally set in postgresql.conf. (See Chapter 19 for details about setting configuration parameters.)\nThe parameter track_activities enables monitoring of the current command being executed by any server process.\nThe parameter track_counts controls whether statistics are collected about table and index accesses.\nThe parameter track_functions enables tracking of usage of user-defined functions.\nThe parameter track_io_timing enables monitoring of block read and write times.\nThe parameter track_wait_events controls whether statistics are collected about wait events.\nNormally these parameters are set in postgresql.conf so that they apply to all server processes, but it is possible to turn them on or off in individual sessions using the SET command. (To prevent ordinary users from hiding their activity from the administrator, only superusers are allowed to change these parameters with SET.)\nThe statistics collector transmits the collected information to other PostgreSQL processes through temporary files. These files are stored in the directory named by the stats_temp_directory parameter, pg_tmp/stats by default. For better performance, stats_temp_directory can be pointed at a RAM-based file system, decreasing physical I/O requirements. When the server shuts down cleanly, a permanent copy of the statistics data is stored in the pg_stat subdirectory, so that statistics can be retained across server restarts. When recovery is performed at server start (e.g., after immediate shutdown, server crash, and point-in-time recovery), all statistics counters are reset.\nViewing Statistics     Several predefined views, listed in Table 1, are available to show the current state of the system. There are also several other views, listed in Table 2, available to show the results of statistics collection. Alternatively, one can build custom views using the underlying statistics functions, as discussed in Section 27.2.3.\nWhen using the statistics to monitor collected data, it is important to realize that the information does not update instantaneously. Each individual server process transmits new statistical counts to the collector just before going idle; so a query or transaction still in progress does not affect the displayed totals. Also, the collector itself emits a new report at most once per PGSTAT_STAT_INTERVAL milliseconds (500 ms unless altered while building the server). So the displayed information lags behind actual activity. However, current-query information collected by track_activities is always up-to-date.\nAnother important point is that when a server process is asked to display any of these statistics, it first fetches the most recent report emitted by the collector process and then continues to use this snapshot for all statistical views and functions until the end of its current transaction. So the statistics will show static information as long as you continue the current transaction. Similarly, information about the current queries of all sessions is collected when any such information is first requested within a transaction, and the same information will be displayed throughout the transaction. This is a feature, not a bug, because it allows you to perform several queries on the statistics and correlate the results without worrying that the numbers are changing underneath you. But if you want to see new results with each query, be sure to do the queries outside any transaction block. Alternatively, you can invoke pg_stat_clear_snapshot(), which will discard the current transaction\u0026rsquo;s statistics snapshot (if any). The next use of statistical information will cause a new snapshot to be fetched.\nA transaction can also see its own statistics (as yet untransmitted to the collector) in the views pg_stat_xact_all_tables, pg_stat_xact_all_undos, pg_stat_xact_sys_tables, pg_stat_xact_user_tables, and pg_stat_xact_user_functions. These numbers do not act as stated above; instead they update continuously throughout the transaction.\nSome of the information in the dynamic statistics views shown in Table 1 is security restricted. Ordinary users can only see all the information about their own sessions (sessions belonging to a role that they are a member of). In rows about other sessions, many columns will be null. Note, however, that the existence of a session and its general properties such as its sessions user and database are visible to all users. Superusers and members of the built-in role pg_read_all_stats (see also Section 21.5) can see all the information about all sessions.\nTable 1. Dynamic Statistics Views\n   View Name Description     pg_stat_activity One row per server process, showing information related to the current activity of that process, such as state and current query. See pg_stat_activity for details.   pg_stat_wait_event One row per wait event, showing statistics about wait information of every occured wait events. See pg_stat_wait_event for details.   pg_stat_replication One row per WAL sender process, showing statistics about replication to that sender\u0026rsquo;s connected standby server. See pg_stat_replication for details.    Table 2. Collected Statistics Views\n   View Name Description     pg_stat_database One row per database, showing database-wide statistics. See pg_stat_database for details.   pg_stat_all_tables One row for each table in the current database, showing statistics about accesses to that specific table. See pg_stat_all_tables for details.   pg_stat_sys_tables Same as pg_stat_all_tables, except that only system tables are shown.   pg_stat_user_tables Same as pg_stat_all_tables, except that only user tables are shown.   pg_stat_xact_all_tables Similar to pg_stat_all_tables, but counts actions taken so far within the current transaction (which are not yet included in pg_stat_all_tables and related views). The columns for numbers of live and dead rows and analyze actions are not present in this view.   pg_stat_xact_sys_tables Same as pg_stat_xact_all_tables, except that only system tables are shown.   pg_stat_xact_user_tables Same as pg_stat_xact_all_tables, except that only user tables are shown.   pg_stat_all_undos One row for each undo in the current database, showing statistics about accesses to that specific undo. See pg_stat_all_undos for details.   pg_stat_xact_all_undos Similar to pg_stat_all_undos, but counts actions taken so far within the current transaction (which are not yet included in pg_stat_all_undos and related views). The columns for numbers of live and dead records are not present in this view.   pg_stat_all_indexes One row for each index in the current database, showing statistics about accesses to that specific index. See pg_stat_all_indexes for details.   pg_stat_sys_indexes Same as pg_stat_all_indexes, except that only indexes on system tables are shown.   pg_stat_user_indexes Same as pg_stat_all_indexes, except that only indexes on user tables are shown.   pg_statio_all_tables One row for each table in the current database, showing statistics about I/O on that specific table. See pg_statio_all_tables for details.   pg_statio_sys_tables Same as pg_statio_all_tables, except that only system tables are shown.   pg_statio_user_tables Same as pg_statio_all_tables, except that only user tables are shown.   pg_statio_all_undos One row for each undo in the current database, showing statistics about I/O on that specific undo. See pg_statio_all_undos for details.   pg_statio_all_indexes One row for each index in the current database, showing statistics about I/O on that specific index. See pg_statio_all_indexes for details.   pg_statio_sys_indexes Same as pg_statio_all_indexes, except that only indexes on system tables are shown.   pg_statio_user_indexes Same as pg_statio_all_indexes, except that only indexes on user tables are shown.   pg_statio_all_sequences One row for each sequence in the current database, showing statistics about I/O on that specific sequence. See pg_statio_all_sequences for details.   pg_statio_sys_sequences Same as pg_statio_all_sequences, except that only system sequences are shown. (Presently, no system sequences are defined, so this view is always empty.)   pg_statio_user_sequences Same as pg_statio_all_sequences, except that only user sequences are shown.    The per-index statistics are particularly useful to determine which indexes are being used and how effective they are.\nThe pg_statio_ views are primarily useful to determine the effectiveness of the buffer cache. When the number of actual disk reads is much smaller than the number of buffer hits, then the cache is satisfying most read requests without invoking a kernel call. However, these statistics do not give the entire story: due to the way in which PostgreSQL handles disk I/O, data that is not in the PostgreSQL buffer cache might still reside in the kernel\u0026rsquo;s I/O cache, and might therefore still be fetched without requiring a physical read. Users interested in obtaining more detailed information on PostgreSQL I/O behavior are advised to use the PostgreSQL statistics collector in combination with operating system utilities that allow insight into the kernel\u0026rsquo;s handling of I/O.\npg_stat_activity     The pg_stat_activity view will have one row per server process, showing information related to the current activity of that process.\nTable 3. pg_stat_activity View\n   Column Type Description     datid oid OID of the database this backend is connected to   datname name Name of the database this backend is connected to   pid integer Database Process ID of this backend   spid integer System Process ID of this backend   tid integer Thread ID of this backend   usesysid oid OID of the user logged into this backend   usename name Name of the user logged into this backend   application_name text Name of the application that is connected to this backend   client_addr inet IP address of the client connected to this backend. If this field is null, it indicates either that the client is connected via a Unix socket on the server machine or that this is an internal process such as autoanalyze.   client_hostname text Host name of the connected client, as reported by a reverse DNS lookup of client_addr. This field will only be non-null for IP connections, and only when log_hostname is enabled.   client_port integer TCP port number that the client is using for communication with this backend, or -1 if a Unix socket is used   backend_start timestamp with time zone Time when this process was started. For client backends, this is the time the client connected to the server.   xact_start timestamp with time zone Time when this process\u0026rsquo; current transaction was started, or null if no transaction is active. If the current query is the first of its transaction, this column is equal to the query_start column.   query_start timestamp with time zone Time when the currently active query was started, or if state is not active, when the last query was started   state_change timestamp with time zone Time when the state was last changed   wait_event_type text The type of event for which the backend is waiting, if any; otherwise NULL. Possible values are:LWLock: The backend is waiting for a lightweight lock. Each such lock protects a particular data structure in shared memory. wait_event will contain a name identifying the purpose of the lightweight lock. (Some locks have specific names; others are part of a group of locks each with a similar purpose.)Lock: The backend is waiting for a heavyweight lock. Heavyweight locks, also known as lock manager locks or simply locks, primarily protect SQL-visible objects such as tables. However, they are also used to ensure mutual exclusion for certain internal operations such as relation extension. wait_event will identify the type of lock awaited.BufferPin: The server process is waiting to access to a data buffer during a period when no other process can be examining that buffer. Buffer pin waits can be protracted if another process holds an open cursor which last read data from the buffer in question.Activity: The server process is idle. This is used by system processes waiting for activity in their main processing loop. wait_event will identify the specific wait point.Extension: The server process is waiting for activity in an extension module. This category is useful for modules to track custom waiting points.Client: The server process is waiting for some activity on a socket from user applications, and that the server expects something to happen that is independent from its internal processes. wait_event will identify the specific wait point.IPC: The server process is waiting for some activity from another process in the server. wait_event will identify the specific wait point.Timeout: The server process is waiting for a timeout to expire. wait_event will identify the specific wait point.IO: The server process is waiting for a IO to complete. wait_event will identify the specific wait point.   wait_event text Wait event name if backend is currently waiting, otherwise NULL. See Table 27.4 for details.   memory_used bigint Memory used by this backend.   state text Current overall state of this backend. Possible values are:active: The backend is executing a query.idle: The backend is waiting for a new client command.idle in transaction: The backend is in a transaction, but is not currently executing a query.idle in transaction (aborted): This state is similar to idle in transaction, except one of the statements in the transaction caused an error.fastpath function call: The backend is executing a fast-path function.disabled: This state is reported if track_activities is disabled in this backend.   backend_xid xid Top-level transaction identifier of this backend, if any.   backend_mintime logicaltime The current backend\u0026rsquo;s mintime horizon.   query text Text of this backend\u0026rsquo;s most recent query. If state is active this field shows the currently executing query. In all other states, it shows the last query that was executed. By default the query text is truncated at 1024 bytes; this value can be changed via the parameter track_activity_query_size.   backend_type text Type of current backend. Possible types are autoanalyze launcher, autoanalyze worker, logical replication launcher, logical replication worker, parallel worker, background writer, client backend, checkpointer, startup, walreceiver, walsender and walwriter. In addition, background workers registered by extensions may have additional types.     The wait_event and state columns are independent. If a backend is in the active state, it may or may not be waiting on some event. If the state is active and wait_event is non-null, it means that a query is being executed, but is being blocked somewhere in the system.  Table 4. Wait Event Types\n   Wait Event Type Description     Activity The server process is idle. This event type indicates a process waiting for activity in its main processing loop. wait_event will identify the specific wait point; see Table 5.   BufferPin The server process is waiting for exclusive access to a data buffer. Buffer pin waits can be protracted if another process holds an open cursor that last read data from the buffer in question. See Table 6.   Client The server process is waiting for activity on a socket connected to a user application. Thus, the server expects something to happen that is independent of its internal processes. wait_event will identify the specific wait point; see Table 7.   Extension The server process is waiting for some condition defined by an extension module. See Table 8.   IO The server process is waiting for an I/O operation to complete. wait_event will identify the specific wait point; see Table 9.   IPC The server process is waiting for some interaction with another server process. wait_event will identify the specific wait point; see Table 10.   Lock The server process is waiting for a heavyweight lock. Heavyweight locks, also known as lock manager locks or simply locks, primarily protect SQL-visible objects such as tables. However, they are also used to ensure mutual exclusion for certain internal operations such as relation extension. wait_event will identify the type of lock awaited; see Table 11.   LWLock The server process is waiting for a lightweight lock. Most such locks protect a particular data structure in shared memory. wait_event will contain a name identifying the purpose of the lightweight lock. (Some locks have specific names; others are part of a group of locks each with a similar purpose.) See Table 12.   Timeout The server process is waiting for a timeout to expire. wait_event will identify the specific wait point; see Table 13.    Table 5. Wait Events of Type Activity\n   Activity Wait Event Description     ArchiverMain Waiting in main loop of archiver process.   AnalyzeLauncherMain Waiting in main loop of autoanalyze launcher process.   BgWriterHibernate Waiting in background writer process, hibernating.   BgWriterMain Waiting in main loop of background writer process.   CheckpointerMain Waiting in main loop of checkpointer process.   LogicalApplyMain Waiting in main loop of logical replication apply process.   LogicalLauncherMain Waiting in main loop of logical replication launcher process.   PgStatMain Waiting in main loop of statistics collector process.   RecoveryWalStream Waiting in main loop of startup process for WAL to arrive, during streaming recovery.   SysLoggerMain Waiting in main loop of syslogger process.   WalReceiverMain Waiting in main loop of WAL receiver process.   WalSenderMain Waiting in main loop of WAL sender process.   WalWriterMain Waiting in main loop of WAL writer process.    Table 6. Wait Events of Type BufferPin\n   BufferPin Wait Event Description     BufferPin Waiting to acquire an exclusive pin on a buffer.    Table 7. Wait Events of Type Client\n   Client Wait Event Description     ClientRead Waiting to read data from the client.   ClientWrite Waiting to write data to the client.   GSSOpenServer Waiting to read data from the client while establishing a GSSAPI session.   LibPQWalReceiverConnect Waiting in WAL receiver to establish connection to remote server.   LibPQWalReceiverReceive Waiting in WAL receiver to receive data from remote server.   LibPQStorageConnect Waiting in compute server to establish connection to remote storage server.   LibPQStorageReceive Waiting in compute server to receive data from remote storage server.   SSLOpenServer Waiting for SSL while attempting connection.   WalSenderWaitForWAL Waiting for WAL to be flushed in WAL sender process.   WalSenderWriteData Waiting for any activity when processing replies from WAL receiver in WAL sender process.    Table 8. Wait Events of Type Extension\n   Extension Wait Event Description     Extension Waiting in an extension.    Table 9. Wait Events of Type IO\n   IO Wait Event Description     BaseBackupRead Waiting for base backup to read from a file.   BufFileRead Waiting for a read from a buffered file.   BufFileWrite Waiting for a write to a buffered file.   BufFileTruncate Waiting for a buffered file to be truncated.   ControlFileRead Waiting for a read from the pg_control file.   ControlFileSync Waiting for the pg_control file to reach durable storage.   ControlFileSyncUpdate Waiting for an update to the pg_control file to reach durable storage.   ControlFileWrite Waiting for a write to the pg_control file.   ControlFileWriteUpdate Waiting for a write to update the pg_control file.   CopyFileRead Waiting for a read during a file copy operation.   CopyFileWrite Waiting for a write during a file copy operation.   DSMFillZeroWrite Waiting to fill a dynamic shared memory backing file with zeroes.   DataFileExtend Waiting for a relation data file to be extended.   DataFileFlush Waiting for a relation data file to reach durable storage.   DataFileImmediateSync Waiting for an immediate synchronization of a relation data file to durable storage.   DataFilePrefetch Waiting for an asynchronous prefetch from a relation data file.   DataFileRead Waiting for a read from a relation data file.   DataFileSync Waiting for changes to a relation data file to reach durable storage.   DataFileTruncate Waiting for a relation data file to be truncated.   DataFileWrite Waiting for a write to a relation data file.   LockFileAddToDataDirRead Waiting for a read while adding a line to the data directory lock file.   LockFileAddToDataDirSync Waiting for data to reach durable storage while adding a line to the data directory lock file.   LockFileAddToDataDirWrite Waiting for a write while adding a line to the data directory lock file.   LockFileCreateRead Waiting to read while creating the data directory lock file.   LockFileCreateSync Waiting for data to reach durable storage while creating the data directory lock file.   LockFileCreateWrite Waiting for a write while creating the data directory lock file.   LockFileReCheckDataDirRead Waiting for a read during recheck of the data directory lock file.   LogicalRewriteCheckpointSync Waiting for logical rewrite mappings to reach durable storage during a checkpoint.   LogicalRewriteMappingSync Waiting for mapping data to reach durable storage during a logical rewrite.   LogicalRewriteMappingWrite Waiting for a write of mapping data during a logical rewrite.   LogicalRewriteSync Waiting for logical rewrite mappings to reach durable storage.   LogicalRewriteTruncate Waiting for truncate of mapping data during a logical rewrite.   LogicalRewriteWrite Waiting for a write of logical rewrite mappings.   RelationMapRead Waiting for a read of the relation map file.   RelationMapSync Waiting for the relation map file to reach durable storage.   RelationMapWrite Waiting for a write to the relation map file.   ReorderBufferRead Waiting for a read during reorder buffer management.   ReorderBufferWrite Waiting for a write during reorder buffer management.   ReorderLogicalMappingRead Waiting for a read of a logical mapping during reorder buffer management.   ReplicationSlotRead Waiting for a read from a replication slot control file.   ReplicationSlotRestoreSync Waiting for a replication slot control file to reach durable storage while restoring it to memory.   ReplicationSlotSync Waiting for a replication slot control file to reach durable storage.   ReplicationSlotWrite Waiting for a write to a replication slot control file.   SLRUFlushSync Waiting for SLRU data to reach durable storage during a checkpoint or database shutdown.   SLRURead Waiting for a read of an SLRU page.   SLRUSync Waiting for SLRU data to reach durable storage following a page write.   SLRUWrite Waiting for a write of an SLRU page.   SnapbuildRead Waiting for a read of a serialized historical catalog snapshot.   SnapbuildSync Waiting for a serialized historical catalog snapshot to reach durable storage.   SnapbuildWrite Waiting for a write of a serialized historical catalog snapshot.   TimelineHistoryFileSync Waiting for a timeline history file received via streaming replication to reach durable storage.   TimelineHistoryFileWrite Waiting for a write of a timeline history file received via streaming replication.   TimelineHistoryRead Waiting for a read of a timeline history file.   TimelineHistorySync Waiting for a newly created timeline history file to reach durable storage.   TimelineHistoryWrite Waiting for a write of a newly created timeline history file.   WALBootstrapSync Waiting for WAL to reach durable storage during bootstrapping.   WALBootstrapWrite Waiting for a write of a WAL page during bootstrapping.   WALCopyRead Waiting for a read when creating a new WAL segment by copying an existing one.   WALCopySync Waiting for a new WAL segment created by copying an existing one to reach durable storage.   WALCopyWrite Waiting for a write when creating a new WAL segment by copying an existing one.   WALInitSync Waiting for a newly initialized WAL file to reach durable storage.   WALInitWrite Waiting for a write while initializing a new WAL file.   WALRead Waiting for a read from a WAL file.   WALSenderTimelineHistoryRead Waiting for a read from a timeline history file during a walsender timeline command.   WALSync Waiting for a WAL file to reach durable storage.   WALSyncMethodAssign Waiting for data to reach durable storage while assigning a new WAL sync method.   WALWrite Waiting for a write to a WAL file.   LogicalChangesRead Waiting for a read from a logical changes file.   LogicalChangesWrite Waiting for a write to a logical changes file.   LogicalSubxactRead Waiting for a read from a logical subxact file.   LogicalSubxactWrite Waiting for a write to a logical subxact file.    Table 10. Wait Events of Type IPC\n   IPC Wait Event Description     AppendReady Waiting for subplan nodes of an Append plan node to be ready.   BackendTermination Waiting for the termination of another backend.   BackupWaitWalArchive Waiting for WAL files required for a backup to be successfully archived.   BgWorkerShutdown Waiting for background worker to shut down.   BgWorkerStartup Waiting for background worker to start up.   BtreePage Waiting for the page number needed to continue a parallel B-tree scan to become available.   BufferIO Waiting for buffer I/O to complete.   CheckpointDone Waiting for a checkpoint to complete.   CheckpointStart Waiting for a checkpoint to start.   ExecuteGather Waiting for activity from a child process while executing a Gather plan node.   LogicalSyncData Waiting for a logical replication remote server to send data for initial table synchronization.   LogicalSyncStateChange Waiting for a logical replication remote server to change state.   MessageQueueInternal Waiting for another process to be attached to a shared message queue.   MessageQueuePutMessage Waiting to write a protocol message to a shared message queue.   MessageQueueReceive Waiting to receive bytes from a shared message queue.   MessageQueueSend Waiting to send bytes to a shared message queue.   ParallelBitmapScan Waiting for parallel bitmap scan to become initialized.   ParallelCreateIndexScan Waiting for parallel CREATE INDEX workers to finish heap scan.   ParallelFinish Waiting for parallel workers to finish computing.   ProcArrayGroupUpdate Waiting for the group leader to clear the transaction ID at end of a parallel operation.   ProcSignalBarrier Waiting for a barrier event to be processed by all backends.   Promote Waiting for standby promotion.   RecoveryPause Waiting for recovery to be resumed.   ReplicationOriginDrop Waiting for a replication origin to become inactive so it can be dropped.   ReplicationSlotDrop Waiting for a replication slot to become inactive so it can be dropped.   SafeSnapshot Waiting to obtain a valid snapshot for a READ ONLY DEFERRABLE transaction.   SyncRep Waiting for confirmation from a remote server during synchronous replication.   WalReceiverExit Waiting for the WAL receiver to exit.   WalReceiverWaitStart Waiting for startup process to send initial data for streaming replication.   XactGroupUpdate Waiting for the group leader to update transaction status at end of a parallel operation.    Table 11. Wait Events of Type Lock\n   Lock Wait Event Description     advisory Waiting to acquire an advisory user lock.   extend Waiting to extend a relation.   object Waiting to acquire a lock on a non-relation database object.   page Waiting to acquire a lock on a page of a relation.   relation Waiting to acquire a lock on a relation.   transactionid Waiting for a transaction to finish.   tuple Waiting to acquire a lock on a tuple.   userlock Waiting to acquire a user lock.   virtualxid Waiting to acquire a virtual transaction ID lock.    Table 12. Wait Events of Type LWLock\n   LWLock Wait Event Description     AddinShmemInit Waiting to manage an extension\u0026rsquo;s space allocation in shared memory.   AutoFile Waiting to update the pgsql.auto.conf file.   Autoanalyze Waiting to read or update the current state of autoanalyze workers.   AutoanalyzeSchedule Waiting to ensure that a table selected for autoanalyze still needs analyzing.   BackgroundWorker Waiting to read or update background worker state.   BufferContent Waiting to access a data page in memory.   BufferIO Waiting to read or write a data page in disk.   BufferMapping Waiting to associate a data block with a buffer in the buffer pool.   CheckpointerComm Waiting to manage fsync requests.   ControlFile Waiting to read or update the pg_control file or create a new WAL file.   DatabaseState Waiting to update state of databases.   DynamicSharedMemoryControl Waiting to read or update dynamic shared memory allocation information.   LockFastPath Waiting to read or update a process\u0026rsquo; fast-path lock information.   LockManager Waiting to read or update information about “heavyweight” locks.   LogicalRepWorker Waiting to read or update the state of logical replication workers.   NotifyBuffer Waiting for I/O on a NOTIFY message SLRU buffer.   NotifyQueue Waiting to read or update NOTIFY messages.   NotifyQueueTail Waiting to update limit on NOTIFY message storage.   NotifySLRU Waiting to access the NOTIFY message SLRU cache.   OidGen Waiting to allocate a new OID.   OldSnapshotTimeMap Waiting to read or update old snapshot control information.   PLogSpace Waiting to update state of network tablespaces.   PLogWrite Waiting for PLOG buffers to be written to disk.   PredicateLockManager Waiting to access predicate lock information used by serializable transactions.   ProcArray Waiting to access the shared per-process data structures (typically, to get a snapshot or report a session\u0026rsquo;s transaction ID).   RelCacheInit Waiting to read or update a pg_internal.init relation cache initialization file.   ReplicationOrigin Waiting to create, drop or use a replication origin.   ReplicationOriginState Waiting to read or update the progress of one replication origin.   ReplicationSlotAllocation Waiting to allocate or free a replication slot.   ReplicationSlotControl Waiting to read or update replication slot state.   ReplicationSlotIO Waiting for I/O on a replication slot.   SerializableFinishedList Waiting to access the list of finished serializable transactions.   SerializablePredicateList Waiting to access the list of predicate locks held by serializable transactions.   SerializableXactHash Waiting to read or update information about serializable transactions.   ShmemIndex Waiting to find or allocate space in shared memory.   SInvalRead Waiting to retrieve messages from the shared catalog invalidation queue.   SInvalWrite Waiting to add a message to the shared catalog invalidation queue.   SyncRep Waiting to read or update information about the state of synchronous replication.   SyncScan Waiting to select the starting location of a synchronized table scan.   TablespaceMap Waiting to create or drop a tablespace.   TwoPhaseState Waiting to read or update the state of prepared transactions.   WALBufMapping Waiting to replace a page in WAL buffers.   WALInsert Waiting to insert WAL data into a memory buffer.   WALWrite Waiting for WAL buffers to be written to disk.     Extensions can add LWLock types to the list shown in Table 12. In some cases, the name assigned by an extension will not be available in all server processes; so an LWLock wait event might be reported as just “extension” rather than the extension-assigned name.  Table 13. Wait Events of Type Timeout\n   Timeout Wait Event Description     BaseBackupThrottle Waiting during base backup when throttling activity.   CheckpointWriteDelay Waiting between writes while performing a checkpoint.   PgSleep Waiting due to a call to pg_sleep or a sibling function.   RecoveryApplyDelay Waiting to apply WAL during recovery because of a delay setting.   RecoveryRetrieveRetryInterval Waiting during recovery when WAL data is not available from any source (pg_wal, archive or stream).   RegisterSyncRequest Waiting while sending synchronization requests to the checkpointer, because the request queue is full.    Here is an example of how wait events can be viewed:\nSELECT pid, wait_event_type, wait_event FROM pg_stat_activity WHERE wait_event is NOT NULL;\rpid | wait_event_type | wait_event ------+-----------------+------------\r2540 | Lock | relation\r6644 | LWLock | ProcArray\r(2 rows) pg_stat_wait_event     The pg_stat_wait_event view will contain one row for each wait event in the current database, showing statistics about wait information of every occured wait events.\nTable 14. pg_stat_wait_event View\n   Column Type Description     wait_event_type text The type of event for which the backend is waiting, if any; otherwise NULL. See Table 4 for details.   wait_event text Wait event name if backend is currently waiting, otherwise NULL. See Table 4 ~ 13 for details.   waits bigint Number of times this event has occured   total_time double precision Total time spent in the event, in milliseconds   min_time double precision Minimum time spent in the event, in milliseconds   max_time double precision Maximum time spent in the event, in milliseconds   mean_time double precision Mean time spent in the event, in milliseconds    pg_stat_replication     The pg_stat_replication view will contain one row per WAL sender process, showing statistics about replication to that sender\u0026rsquo;s connected standby server. Only directly connected standbys are listed; no information is available about downstream standby servers.\nTable 15. pg_stat_replication View\n   Column Type Description     pid integer Process ID of a WAL sender process   usesysid oid OID of the user logged into this WAL sender process   usename name Name of the user logged into this WAL sender process   application_name text Name of the application that is connected to this WAL sender   client_addr inet IP address of the client connected to this WAL sender. If this field is null, it indicates that the client is connected via a Unix socket on the server machine.   client_hostname text Host name of the connected client, as reported by a reverse DNS lookup of client_addr. This field will only be non-null for IP connections, and only when log_hostname is enabled.   client_port integer TCP port number that the client is using for communication with this WAL sender, or -1 if a Unix socket is used   backend_start timestamp with time zone Time when this process was started, i.e., when the client connected to this WAL sender   backend_mintime logicaltime This standby\u0026rsquo;s mintime horizon reported by hot_standby_feedback.   state text Current WAL sender state. Possible values are:startup: This WAL sender is starting up.catchup: This WAL sender\u0026rsquo;s connected standby is catching up with the primary.streaming: This WAL sender is streaming changes after its connected standby server has caught up with the primary.backup: This WAL sender is sending a backup.stopping: This WAL sender is stopping.   sent_lsn pg_lsn Last write-ahead log location sent on this connection   write_lsn pg_lsn Last write-ahead log location written to disk by this standby server   flush_lsn pg_lsn Last write-ahead log location flushed to disk by this standby server   replay_lsn pg_lsn Last write-ahead log location replayed into the database on this standby server   write_lag interval Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written it (but not yet flushed it or applied it). This can be used to gauge the delay that synchronous_commit level remote_write incurred while committing if this server was configured as a synchronous standby.   flush_lag interval Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written and flushed it (but not yet applied it). This can be used to gauge the delay that synchronous_commit level on incurred while committing if this server was configured as a synchronous standby.   replay_lag interval Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written, flushed and applied it. This can be used to gauge the delay that synchronous_commit level remote_apply incurred while committing if this server was configured as a synchronous standby.   sync_priority integer Priority of this standby server for being chosen as the synchronous standby in a priority-based synchronous replication. This has no effect in a quorum-based synchronous replication.   sync_state text Synchronous state of this standby server. Possible values are:async: This standby server is asynchronous.potential: This standby server is now asynchronous, but can potentially become synchronous if one of current synchronous ones fails.sync: This standby server is synchronous.quorum: This standby server is considered as a candidate for quorum standbys.   reply_time timestamp with time zone Send time of last reply message received from standby server    The lag times reported in the pg_stat_replication view are measurements of the time taken for recent WAL to be written, flushed and replayed and for the sender to know about it. These times represent the commit delay that was (or would have been) introduced by each synchronous commit level, if the remote server was configured as a synchronous standby. For an asynchronous standby, the replay_lag column approximates the delay before recent transactions became visible to queries. If the standby server has entirely caught up with the sending server and there is no more WAL activity, the most recently measured lag times will continue to be displayed for a short time and then show NULL.\nLag times work automatically for physical replication. Logical decoding plugins may optionally emit tracking messages; if they do not, the tracking mechanism will simply display NULL lag.\n The reported lag times are not predictions of how long it will take for the standby to catch up with the sending server assuming the current rate of replay. Such a system would show similar times while new WAL is being generated, but would differ when the sender becomes idle. In particular, when the standby has caught up completely, pg_stat_replication shows the time taken to write, flush and replay the most recent reported WAL location rather than zero as some users might expect. This is consistent with the goal of measuring synchronous commit and transaction visibility delays for recent write transactions. To reduce confusion for users expecting a different model of lag, the lag columns revert to NULL after a short time on a fully replayed idle system. Monitoring systems should choose whether to represent this as missing data, zero or continue to display the last known value.  pg_stat_database     The pg_stat_database view will contain one row for each database in the cluster, plus one for the shared objects, showing database-wide statistics.\nTable 16. pg_stat_database View\n   Column Type Description     datid oid OID of this database, or 0 for objects belonging to a shared relation   datname name Name of this database, or NULL for the shared objects.   numbackends integer Number of backends currently connected to this database, or NULL for the shared objects. This is the only column in this view that returns a value reflecting current state; all other columns return the accumulated values since the last reset.   xact_commit bigint Number of transactions in this database that have been committed   xact_rollback bigint Number of transactions in this database that have been rolled back   blks_read bigint Number of disk blocks read in this database   blks_cloned bigint Number of data buffers cloned in this database   blks_hit bigint Number of times disk blocks were found already in the buffer cache, so that a read was not necessary (this only includes hits in the PostgreSQL buffer cache, not the operating system\u0026rsquo;s file system cache)   tup_returned bigint Number of rows returned by queries in this database   tup_fetched bigint Number of rows fetched by queries in this database   tup_inserted bigint Number of rows inserted by queries in this database   tup_updated bigint Number of rows updated by queries in this database   tup_deleted bigint Number of rows deleted by queries in this database   conflicts bigint Number of queries canceled due to conflicts with recovery in this database. (Conflicts occur only on standby servers; see pg_stat_database_conflicts for details.)   temp_files bigint Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.   temp_bytes bigint Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting.   deadlocks bigint Number of deadlocks detected in this database   checksum_failures bigint Number of data page checksum failures detected in this database (or on a shared object), or NULL if data checksums are not enabled.   checksum_last_failure timestamp with time zone Time at which the last data page checksum failure was detected in this database (or on a shared object), or NULL if data checksums are not enabled.   blk_read_time double precision Time spent reading data file blocks by backends in this database, in milliseconds   blk_clone_time double precision Time spent cloning data buffers by backends in this database, in milliseconds   blk_write_time double precision Time spent writing data file blocks by backends in this database, in milliseconds   stats_reset timestamp with time zone Time at which these statistics were last reset    pg_stat_all_tables     The pg_stat_all_tables view will contain one row for each table in the current database (including TOAST tables), showing statistics about accesses to that specific table. The pg_stat_user_tables and pg_stat_sys_tables views contain the same information, but filtered to only show user and system tables respectively.\nTable 17. pg_stat_all_tables View\n   Column Type Description     relid oid OID of a table   schemaname name Name of the schema that this table is in   relname name Name of this table   seq_scan bigint Number of sequential scans initiated on this table   seq_tup_read bigint Number of live rows fetched by sequential scans   idx_scan bigint Number of index scans initiated on this table   idx_tup_fetch bigint Number of live rows fetched by index scans   n_tup_ins bigint Number of rows inserted   n_tup_upd bigint Number of rows updated (includes HOT updated rows)   n_tup_del bigint Number of rows deleted   n_tup_hot_upd bigint Number of rows HOT updated (i.e., with no separate index update required)   n_live_tup bigint Estimated number of live rows   n_dead_tup bigint Estimated number of dead rows   n_mod_since_analyze bigint Estimated number of rows modified since this table was last analyzed   last_analyze timestamp with time zone Last time at which this table was manually analyzed   last_autoanalyze timestamp with time zone Last time at which this table was analyzed by the autoanalyze daemon   analyze_count bigint Number of times this table has been manually analyzed   autoanalyze_count bigint Number of times this table has been analyzed by the autoanalyze daemon    pg_stat_all_undos     The pg_stat_all_undos view will contain one row for each undo in the current database, showing statistics about accesses to that specific undo.\nTable 18. pg_stat_all_undos View\n   Column Type Description     relid oid OID of this undo   schemaname name Name of the schema that this undo is in   relname name Name of this undo   n_tup_fetch bigint Number of records fetched   n_tup_ins bigint Number of records inserted   n_tup_upd bigint Number of records updated   n_tup_del bigint Number of records deleted   n_tup_hot_upd bigint Number of records HOT updated   n_live_tup bigint Estimated number of live records   n_dead_tup bigint Estimated number of dead records    pg_stat_all_indexes     The pg_stat_all_indexes view will contain one row for each index in the current database, showing statistics about accesses to that specific index. The pg_stat_user_indexes and pg_stat_sys_indexes views contain the same information, but filtered to only show user and system indexes respectively.\nTable 19. pg_stat_all_indexes View\n   Column Type Description     relid oid OID of the table for this index   indexrelid oid OID of this index   schemaname name Name of the schema this index is in   relname name Name of the table for this index   indexrelname name Name of this index   idx_scan bigint Number of index scans initiated on this index   idx_tup_read bigint Number of index entries returned by scans on this index   idx_tup_fetch bigint Number of live table rows fetched by simple index scans using this index    Indexes can be used by simple index scans, “bitmap” index scans, and the optimizer. In a bitmap scan the output of several indexes can be combined via AND or OR rules, so it is difficult to associate individual heap row fetches with specific indexes when a bitmap scan is used. Therefore, a bitmap scan increments the pg_stat_all_indexes.idx_tup_read count(s) for the index(es) it uses, and it increments the pg_stat_all_tables.idx_tup_fetch count for the table, but it does not affect pg_stat_all_indexes.idx_tup_fetch. The optimizer also accesses indexes to check for supplied constants whose values are outside the recorded range of the optimizer statistics because the optimizer statistics might be stale.\n The idx_tup_read and idx_tup_fetch counts can be different even without any use of bitmap scans, because idx_tup_read counts index entries retrieved from the index while idx_tup_fetch counts live rows fetched from the table. The latter will be less if any dead or not-yet-committed rows are fetched using the index, or if any heap fetches are avoided by means of an index-only scan.  pg_statio_all_tables     The pg_statio_all_tables view will contain one row for each table in the current database (including TOAST tables), showing statistics about I/O on that specific table. The pg_statio_user_tables and pg_statio_sys_tables views contain the same information, but filtered to only show user and system tables respectively.\nTable 20. pg_statio_all_tables View\n   Column Type Description     relid oid OID of a table   schemaname name Name of the schema that this table is in   relname name Name of this table   heap_blks_read bigint Number of disk blocks read from this table   heap_blks_cloned bigint Number of data buffers cloned from this table   heap_blks_hit bigint Number of buffer hits in this table   idx_blks_read bigint Number of disk blocks read from all indexes on this table   idx_blks_cloned bigint Number of data buffers cloned from all indexes on this table   idx_blks_hit bigint Number of buffer hits in all indexes on this table   toast_blks_read bigint Number of disk blocks read from this table\u0026rsquo;s TOAST table (if any)   toast_blks_cloned bigint Number of data buffers cloned from this table\u0026rsquo;s TOAST table (if any)   toast_blks_hit bigint Number of buffer hits in this table\u0026rsquo;s TOAST table (if any)   tidx_blks_read bigint Number of disk blocks read from this table\u0026rsquo;s TOAST table indexes (if any)   tidx_blks_cloned bigint Number of data buffers cloned from this table\u0026rsquo;s TOAST table indexes (if any)   tidx_blks_hit bigint Number of buffer hits in this table\u0026rsquo;s TOAST table indexes (if any)    pg_statio_all_undos     The pg_statio_all_undos view will contain one row for each undo in the current database, showing statistics about I/O on that specific undo.\nTable 21. pg_statio_all_undos View\n   Column Type Description     relid oid OID of the table for this index   schemaname name Name of the schema this undois in   relname name Name of the table for this undo   undo_blks_read bigint Number of disk blocks read from this undo   undo_blks_cloned bigint Number of data buffers cloned from this undo   undo_blks_hit bigint Number of buffer hits in this undo    pg_statio_all_indexes     The pg_statio_all_indexes view will contain one row for each index in the current database, showing statistics about I/O on that specific index. The pg_statio_user_indexes and pg_statio_sys_indexes views contain the same information, but filtered to only show user and system indexes respectively.\nTable 22. pg_statio_all_indexes View\n   Column Type Description     relid oid OID of the table for this index   indexrelid oid OID of this index   schemaname name Name of the schema this index is in   relname name Name of the table for this index   indexrelname name Name of this index   idx_blks_read bigint Number of disk blocks read from this index   idx_blks_cloned bigint Number of data buffers cloned from this index   idx_blks_hit bigint Number of buffer hits in this index    pg_statio_all_sequences     The pg_statio_all_sequences view will contain one row for each sequence in the current database, showing statistics about I/O on that specific sequence.\nTable 23. pg_statio_all_sequences View\n   Column Type Description     relid oid OID of a sequence   schemaname name Name of the schema this sequence is in   relname name Name of this sequence   blks_read bigint Number of disk blocks read from this sequence   blks_cloned bigint Number of data buffers cloned from this sequence   blks_hit bigint Number of buffer hits in this sequence    "},{"id":19,"href":"/features/event-triggers/","title":"Event Triggers","parent":"Features","content":"   Overview of Event Trigger Behavior Event Trigger Firing Matrix Event Trigger Functions  Capturing Changes at Command End Processing Objects Dropped by a DDL Command Handling a Table Rewrite Event Deparsing a DDL command        Redrock Postgres provides event triggers. Unlike regular triggers, which are attached to a single table and capture only DML events, event triggers are global to a particular database and are capable of capturing DDL events.\nLike regular triggers, event triggers can be written in any procedural language that includes event trigger support, or in C, but not in plain SQL.\nOverview of Event Trigger Behavior     An event trigger fires whenever the event with which it is associated occurs in the database in which it is defined. Currently, the only supported events are ddl_command_start, ddl_command_end, table_rewrite and sql_drop. Support for additional events may be added in future releases.\nThe ddl_command_start event occurs just before the execution of a CREATE, ALTER, DROP, SECURITY LABEL, COMMENT, GRANT or REVOKE command. No check whether the affected object exists or doesn\u0026rsquo;t exist is performed before the event trigger fires. As an exception, however, this event does not occur for DDL commands targeting shared objects — databases — or for commands targeting event triggers themselves. The event trigger mechanism does not support these object types. ddl_command_start also occurs just before the execution of a SELECT INTO command, since this is equivalent to CREATE TABLE AS.\nThe ddl_command_end event occurs just after the execution of this same set of commands. To obtain more details on the DDL operations that took place, use the set-returning function pg_event_trigger_ddl_commands() from the ddl_command_end event trigger code (see Event Trigger Functions). Note that the trigger fires after the actions have taken place (but before the transaction commits), and thus the system catalogs can be read as already changed.\nThe sql_drop event occurs just before the ddl_command_end event trigger for any operation that drops database objects. To list the objects that have been dropped, use the set-returning function pg_event_trigger_dropped_objects() from the sql_drop event trigger code (see Event Trigger Functions). Note that the trigger is executed after the objects have been deleted from the system catalogs, so it\u0026rsquo;s not possible to look them up anymore.\nThe table_rewrite event occurs just before a table is rewritten by some actions of the commands ALTER TABLE and ALTER TYPE. While other control statements are available to rewrite a table, like CLUSTER and VACUUM, the table_rewrite event is not triggered by them.\nEvent triggers (like other functions) cannot be executed in an aborted transaction. Thus, if a DDL command fails with an error, any associated ddl_command_end triggers will not be executed. Conversely, if a ddl_command_start trigger fails with an error, no further event triggers will fire, and no attempt will be made to execute the command itself. Similarly, if a ddl_command_end trigger fails with an error, the effects of the DDL statement will be rolled back, just as they would be in any other case where the containing transaction aborts.\nFor a complete list of commands supported by the event trigger mechanism, see Event Trigger Firing Matrix.\nEvent triggers are created using the command CREATE EVENT TRIGGER. In order to create an event trigger, you must first create a function with the special return type event_trigger. This function need not (and may not) return a value; the return type serves merely as a signal that the function is to be invoked as an event trigger.\nIf more than one event trigger is defined for a particular event, they will fire in alphabetical order by trigger name.\nA trigger definition can also specify a WHEN condition so that, for example, a ddl_command_start trigger can be fired only for particular commands which the user wishes to intercept. A common use of such triggers is to restrict the range of DDL operations which users may perform.\nEvent Trigger Firing Matrix     Table 1 lists all commands for which event triggers are supported.\nTable 1. Event Trigger Support by Command Tag\n   Command Tag ddl_command_start ddl_command_end sql_drop table_rewrite Notes     ALTER AGGREGATE X X - -    ALTER COLLATION X X - -    ALTER CONVERSION X X - -    ALTER DOMAIN X X - -    ALTER DEFAULT PRIVILEGES X X - -    ALTER EXTENSION X X - -    ALTER FOREIGN DATA WRAPPER X X - -    ALTER FOREIGN TABLE X X X -    ALTER FUNCTION X X - -    ALTER LANGUAGE X X - -    ALTER LARGE OBJECT X X - -    ALTER MATERIALIZED VIEW X X - -    ALTER OPERATOR X X - -    ALTER OPERATOR CLASS X X - -    ALTER OPERATOR FAMILY X X - -    ALTER POLICY X X - -    ALTER PROCEDURE X X - -    ALTER PUBLICATION X X - -    ALTER ROLE X X - -    ALTER ROUTINE X X - -    ALTER SCHEMA X X - -    ALTER SEQUENCE X X - -    ALTER SERVER X X - -    ALTER STATISTICS X X - -    ALTER SUBSCRIPTION X X - -    ALTER TABLE X X X X    ALTER TABLESPACE X X - -    ALTER TEXT SEARCH CONFIGURATION X X - -    ALTER TEXT SEARCH DICTIONARY X X - -    ALTER TEXT SEARCH PARSER X X - -    ALTER TEXT SEARCH TEMPLATE X X - -    ALTER TRIGGER X X - -    ALTER TYPE X X - X    ALTER USER X X - -    ALTER USER MAPPING X X - -    ALTER VIEW X X - -    COMMENT X X - - Only for local objects   CREATE ACCESS METHOD X X - -    CREATE AGGREGATE X X - -    CREATE CAST X X - -    CREATE COLLATION X X - -    CREATE CONVERSION X X - -    CREATE DOMAIN X X - -    CREATE EXTENSION X X - -    CREATE FOREIGN DATA WRAPPER X X - -    CREATE FOREIGN TABLE X X - -    CREATE FUNCTION X X - -    CREATE INDEX X X - -    CREATE LANGUAGE X X - -    CREATE MATERIALIZED VIEW X X - -    CREATE OPERATOR X X - -    CREATE OPERATOR CLASS X X - -    CREATE OPERATOR FAMILY X X - -    CREATE POLICY X X - -    CREATE PROCEDURE X X - -    CREATE PUBLICATION X X - -    CREATE ROLE X X - -    CREATE RULE X X - -    CREATE SCHEMA X X - -    CREATE SEQUENCE X X - -    CREATE SERVER X X - -    CREATE STATISTICS X X - -    CREATE SUBSCRIPTION X X - -    CREATE TABLE X X - -    CREATE TABLE AS X X - -    CREATE TABLESPACE X X - -    CREATE TEXT SEARCH CONFIGURATION X X - -    CREATE TEXT SEARCH DICTIONARY X X - -    CREATE TEXT SEARCH PARSER X X - -    CREATE TEXT SEARCH TEMPLATE X X - -    CREATE TRIGGER X X - -    CREATE TYPE X X - -    CREATE USER X X - -    CREATE USER MAPPING X X - -    CREATE VIEW X X - -    DROP ACCESS METHOD X X X -    DROP AGGREGATE X X X -    DROP CAST X X X -    DROP COLLATION X X X -    DROP CONVERSION X X X -    DROP DOMAIN X X X -    DROP EXTENSION X X X -    DROP FOREIGN DATA WRAPPER X X X -    DROP FOREIGN TABLE X X X -    DROP FUNCTION X X X -    DROP INDEX X X X -    DROP LANGUAGE X X X -    DROP MATERIALIZED VIEW X X X -    DROP OPERATOR X X X -    DROP OPERATOR CLASS X X X -    DROP OPERATOR FAMILY X X X -    DROP OWNED X X X -    DROP POLICY X X X -    DROP PROCEDURE X X X -    DROP PUBLICATION X X X -    DROP ROLE X X X -    DROP ROUTINE X X X -    DROP RULE X X X -    DROP SCHEMA X X X -    DROP SEQUENCE X X X -    DROP SERVER X X X -    DROP STATISTICS X X X -    DROP SUBSCRIPTION X X X -    DROP TABLE X X X -    DROP TABLESPACE X X X -    DROP TEXT SEARCH CONFIGURATION X X X -    DROP TEXT SEARCH DICTIONARY X X X -    DROP TEXT SEARCH PARSER X X X -    DROP TEXT SEARCH TEMPLATE X X X -    DROP TRIGGER X X X -    DROP TYPE X X X -    DROP USER X X X -    DROP USER MAPPING X X X -    DROP VIEW X X X -    GRANT X X - - Only for local objects   IMPORT FOREIGN SCHEMA X X - -    REFRESH MATERIALIZED VIEW X X - -    REVOKE X X - - Only for local objects   SECURITY LABEL X X - - Only for local objects   SELECT INTO X X - -     Event Trigger Functions     Redrock Postgres provides these helper functions to retrieve information from event triggers.\nCapturing Changes at Command End     pg_event_trigger_ddl_commands returns a list of DDL commands executed by each user action, when invoked in a function attached to a ddl_command_end event trigger. If called in any other context, an error is raised. pg_event_trigger_ddl_commands returns one row for each base command executed; some commands that are a single SQL sentence may return more than one row. This function returns the following columns:\n   Name Type Description     classid oid OID of catalog the object belongs in   objid oid OID of the object itself   objsubid integer Sub-object ID (e.g., attribute number for a column)   command_tag text Command tag   object_type text Type of the object   schema_name text Name of the schema the object belongs in, if any; otherwise NULL. No quoting is applied.   object_identity text Text rendering of the object identity, schema-qualified. Each identifier included in the identity is quoted if necessary.   in_extension bool True if the command is part of an extension script   command pg_ddl_command A complete representation of the command, in internal format. This cannot be output directly, but it can be passed to other functions to obtain different pieces of information about the command.    Processing Objects Dropped by a DDL Command     pg_event_trigger_dropped_objects returns a list of all objects dropped by the command in whose sql_drop event it is called. If called in any other context, pg_event_trigger_dropped_objects raises an error. pg_event_trigger_dropped_objects returns the following columns:\n   Name Type Description     classid oid OID of catalog the object belonged in   objid oid OID of the object itself   objsubid integer Sub-object ID (e.g., attribute number for a column)   original bool True if this was one of the root object(s) of the deletion   normal bool True if there was a normal dependency relationship in the dependency graph leading to this object   is_temporary bool True if this was a temporary object   object_type text Type of the object   schema_name text Name of the schema the object belonged in, if any; otherwise NULL. No quoting is applied.   object_name text Name of the object, if the combination of schema and name can be used as a unique identifier for the object; otherwise NULL. No quoting is applied, and name is never schema-qualified.   object_identity text Text rendering of the object identity, schema-qualified. Each identifier included in the identity is quoted if necessary.   address_names text[] An array that, together with object_type and address_args, can be used by the pg_get_object_address() function to recreate the object address in a remote server containing an identically named object of the same kind   address_args text[] Complement for address_names    The pg_event_trigger_dropped_objects function can be used in an event trigger like this:\nCREATE FUNCTION test_event_trigger_for_drops() RETURNS event_trigger LANGUAGE plpgsql AS $$ DECLARE obj record; BEGIN FOR obj IN SELECT * FROM pg_event_trigger_dropped_objects() LOOP RAISE NOTICE \u0026#39;% dropped object: % %.% %\u0026#39;, tg_tag, obj.object_type, obj.schema_name, obj.object_name, obj.object_identity; END LOOP; END; $$; CREATE EVENT TRIGGER test_event_trigger_for_drops ON sql_drop EXECUTE PROCEDURE test_event_trigger_for_drops(); Handling a Table Rewrite Event     The functions shown in Table 2 provide information about a table for which a table_rewrite event has just been called. If called in any other context, an error is raised.\nTable 2. Table Rewrite Information\n   Name Return Type Description     pg_event_trigger_table_rewrite_oid() Oid The OID of the table about to be rewritten.   pg_event_trigger_table_rewrite_reason() int The reason code(s) explaining the reason for rewriting. The exact meaning of the codes is release dependent.    The pg_event_trigger_table_rewrite_oid function can be used in an event trigger like this:\nCREATE FUNCTION test_event_trigger_table_rewrite_oid() RETURNS event_trigger LANGUAGE plpgsql AS $$ BEGIN RAISE NOTICE \u0026#39;rewriting table % for reason %\u0026#39;, pg_event_trigger_table_rewrite_oid()::regclass, pg_event_trigger_table_rewrite_reason(); END; $$; CREATE EVENT TRIGGER test_table_rewrite_oid ON table_rewrite EXECUTE PROCEDURE test_event_trigger_table_rewrite_oid(); Deparsing a DDL command     pg_ddl_command_deparse deparses a command from internal format of type pg_ddl_command, and returns a SQL statement in textual representation of type text.\nThe pg_ddl_command_deparse function can be used in an event trigger like this:\nCREATE TABLE ddl_history( ord int, op_time timestamp, username text, command_tag text, object_type text, schema_name text, object_identity text, command text ); CREATE OR REPLACE FUNCTION ddl_trigger_func() RETURNS event_trigger SECURITY DEFINER AS $$ BEGIN INSERT INTO ddl_history SELECT ordinality, now(), current_user, command_tag, object_type, schema_name, object_identity, pg_ddl_command_deparse(command) FROM pg_event_trigger_ddl_commands() WITH ORDINALITY; END; $$ LANGUAGE plpgsql; CREATE EVENT TRIGGER deparse_event_trigger ON ddl_command_end EXECUTE PROCEDURE ddl_trigger_func(); "},{"id":20,"href":"/posts/","title":"News","parent":"Welcome to Redrock Documentation","content":""},{"id":21,"href":"/admin/ddl-subscription/","title":"Subscribing DDL Commands","parent":"Server Administration","content":"   Recording and Publishing DDL Commands Subscribing and Replicating DDL Commands      PostgreSQL provides event triggers. Event triggers are global to a particular database and are capable of capturing DDL events occured in the database. Redrock Postgres makes the following improvements to PostgreSQL so that users can subscribe DDL commands with event triggers:\n In Redrock Postgres, users and tablespaces are database-level objects. Event triggers also support DDL commands for users and tablespace objects. For details, see Event Trigger Firing Matrix. Support capturing events of dropping objects at the end of a command (corresponding to the ddl_command_end event). Add a function pg_ddl_command_deparse to deparse a command from internal format of type pg_ddl_command, and returns a SQL statement in textual representation of type text.  The following example shows how to subscribe and replicate DDL commands occured on the publisher side. You can modify the commands in the example according to the actual situation.\nRecording and Publishing DDL Commands     We need to record DDL commands in a log table with event trigger on the publisher side, and publish the log table.\n  Creating a log table to record DDL commands:\nCREATE SCHEMA IF NOT EXISTS sql_audit; -- create table for ddl record CREATE TABLE IF NOT EXISTS sql_audit.ddl_history( ord int, op_time timestamp, username text, command_tag text, object_type text, schema_name text, object_identity text, command text ); -- grant privileges to all user GRANT USAGE ON SCHEMA sql_audit TO PUBLIC; GRANT SELECT, INSERT ON TABLE sql_audit.ddl_history TO PUBLIC;   Publishing the log table:\nCREATE PUBLICATION ddl_publication FOR TABLE ONLY sql_audit.ddl_history;   Creating a event trigger capturing DDL commands:\n-- create function for event triggers CREATE OR REPLACE FUNCTION sql_audit.ddl_trigger_func() RETURNS event_trigger AS $$ BEGIN INSERT INTO sql_audit.ddl_history SELECT ordinality, now(), current_user, command_tag, object_type, schema_name, object_identity, pg_ddl_command_deparse(command) FROM pg_event_trigger_ddl_commands() WITH ORDINALITY; END; $$ LANGUAGE plpgsql; -- create ddl_command_end event trigger CREATE EVENT TRIGGER deparse_event_trigger ON ddl_command_end EXECUTE PROCEDURE sql_audit.ddl_trigger_func();   After the preceding commands are executed, your DDL commands will be recorded in the table sql_audit.ddl_history.    Subscribing and Replicating DDL Commands     On the publisher side, we recorded the executed DDL commands in the log table sql_audit.ddl_history, and published the table. Subscribers can replicate and execute the DDL commands in the log table.\n  Create the same log table on the subscriber side：\nCREATE SCHEMA IF NOT EXISTS sql_audit; -- create table for ddl record CREATE TABLE IF NOT EXISTS sql_audit.ddl_history( ord int, op_time timestamp, username text, command_tag text, object_type text, schema_name text, object_identity text, command text ); -- grant privileges to all user GRANT USAGE ON SCHEMA sql_audit TO PUBLIC; GRANT SELECT, INSERT ON TABLE sql_audit.ddl_history TO PUBLIC;   Subscribing the log table:\nCREATE SUBSCRIPTION ddl_subscriptin CONNECTION \u0026#39;host=192.168.1.50 port=5432 user=testuser password=pgpass\u0026#39; PUBLICATION ddl_publication;   You need to modify the access information of the publisher in the above SQL command according to the actual deployment environment. In addition, make sure to set the wal_level parameter to logical on the publisher side, and restart the database server for the change to take effect.    Create a trigger for the table sql_audit.ddl_history on the subscriber side to replicate and execute DDL commands.\nCREATE OR REPLACE FUNCTION sql_audit.ddl_sub_trigger_func() RETURNS trigger AS $$ BEGIN EXECUTE NEW.command; RETURN NEW; END; $$ LANGUAGE plpgsql; CREATE TRIGGER ddl_sub_trigger AFTER INSERT ON sql_audit.ddl_history FOR EACH ROW EXECUTE PROCEDURE sql_audit.ddl_sub_trigger_func();   "},{"id":22,"href":"/catalogs/","title":"System Catalogs","parent":"Welcome to Redrock Documentation","content":"Table of Contents\n   pg_recyclebin      pg_ts_lexicon      pg_undo      The system catalogs are the place where a relational database management system stores schema metadata, such as information about tables and columns, and internal bookkeeping information. Redrock Postgres\u0026rsquo;s system catalogs are regular tables. You can drop and recreate the tables, add columns, insert and update values, and severely mess up your system that way. Normally, one should not change the system catalogs by hand, there are normally SQL commands to do that. (For example, CREATE DATABASE inserts a row into the pg_database catalog — and actually creates the database on disk.) There are some exceptions for particularly esoteric operations, but many of those have been made available as SQL commands over time, and so the need for direct manipulation of the system catalogs is ever decreasing.\n"},{"id":23,"href":"/features/postgres-compatibility/","title":"Compatibility with PostgreSQL","parent":"Features","content":"   Unsupported features Features different from PostgreSQL  User Tablespace PID   Unsupported Configuration Options Unsupported Storage Parameters      Redrock Postgres is developed based on PostgreSQL. Redrock Postgres is compatible with PostgreSQL on client protocols and SQL syntax. If your application is developed based on PostgreSQL, then:\n Based on the binary compatibility with PostgreSQL on client protocols, Redrock Postgres can be accessed using the same drivers as PostgreSQL for various programming languages, such as Java, C/C++, .Net, Python, Ruby, Perl, Go, ODBC. Due to SQL compatibility with PostgreSQL, you can migrate the application software to Redrock Postgres without any modification. You can use PostgreSQL-related desktop management tools to access Redrock Postgres, such as DBeaver, pgAdmin 4, and Navicat for PostgreSQL.  That means in most cases, you can use pg_dump to export data from PostgreSQL, uninstall PostgreSQL and install Redrock Postgres, import the exported data into Redrock Postgres, and you are good to go.\nUnsupported features      VACUUM FULL is deprecated and only syntax remains VACUUM FREEZE is deprecated and only syntax remains System Columns related to transaction: xmin, cmin, xmax, cmax Serializable Isolation Level SP-GiST Indexes  Features different from PostgreSQL     User     In PostgreSQL, users are cluster-level objects, while in Redrock Postgres, users are database-level objects. That means when you access different database objects in the same cluster, you may see users with the same name, in fact, they are different user objects belonging to different databases.\n DBA should pay attention to this difference, while developers may not.  Tablespace     In PostgreSQL, tablespaces are cluster-level objects, while in Redrock Postgres, tablespaces are database-level objects. That means when you access different database objects in the same cluster, you may see tablespaces with the same name, in fact, they are different tablespace objects in the respective databases.\n DBA should pay attention to this difference, while developers may not.  PID     Some statistics views, such as pg_stat_activity and pg_locks, contain fields named pid. In PostgreSQL, these fields indicate the operating system process ID. In Redrock Postgres, these fields indicate the database process ID.\n There is a field named spid in pg_stat_activity view, indicating the actual operating system process ID.  Unsupported Configuration Options     If you are using any of the following options in your postgresql.conf file you should remove or rename them.\n vacuum_freeze_table_age vacuum_freeze_min_age vacuum_multixact_freeze_table_age vacuum_multixact_freeze_min_age vacuum_cleanup_index_scale_factor autovacuum renamed to autoanalyze log_autovacuum_min_duration renamed to log_autoanalyze_min_duration autovacuum_max_workers renamed to autoanalyze_max_workers autovacuum_naptime renamed to autoanalyze_naptime autovacuum_vacuum_threshold autovacuum_analyze_threshold renamed to autoanalyze_base_threshold autovacuum_vacuum_scale_factor autovacuum_analyze_scale_factor renamed to autoanalyze_scale_factor autovacuum_freeze_max_age autovacuum_multixact_freeze_max_age autovacuum_vacuum_cost_delay autovacuum_vacuum_cost_limit  Unsupported Storage Parameters     If you are using any of the following Storage Parameters in your table definitions you should remove or rename them.\n autovacuum_enabled, toast.autovacuum_enabled renamed to autoanalyze_enabled, toast.autoanalyze_enabled vacuum_index_cleanup, toast.vacuum_index_cleanup vacuum_truncate, toast.vacuum_truncate autovacuum_vacuum_threshold, toast.autovacuum_vacuum_threshold autovacuum_vacuum_scale_factor, toast.autovacuum_vacuum_scale_factor autovacuum_analyze_threshold renamed to autoanalyze_base_threshold autovacuum_analyze_scale_factor renamed to autoanalyze_scale_factor autovacuum_vacuum_cost_delay, toast.autovacuum_vacuum_cost_delay autovacuum_vacuum_cost_limit, toast.autovacuum_vacuum_cost_limit autovacuum_freeze_min_age, toast.autovacuum_freeze_min_age autovacuum_freeze_max_age, toast.autovacuum_freeze_max_age autovacuum_freeze_table_age, toast.autovacuum_freeze_table_age autovacuum_multixact_freeze_min_age, toast.autovacuum_multixact_freeze_min_age autovacuum_multixact_freeze_max_age, toast.autovacuum_multixact_freeze_max_age autovacuum_multixact_freeze_table_age, toast.autovacuum_multixact_freeze_table_age log_autovacuum_min_duration, toast.log_autovacuum_min_duration renamed to log_autoanalyze_min_duration, toast.log_autoanalyze_min_duration  "},{"id":24,"href":"/posts/initial-release/","title":"Redrock Postgres version 12.1-1 Released","parent":"News","content":"The Redrock Development Team is pleased to announce Redrock Postgres 12 version 12.1-1.\nRedrock is an object-relational database management system (ORDBMS) based on PostgreSQL, developed by the PostgreSQL Global Development Group. It is intentionally designed as an enterprise grade and cloud native database.\nFor more information, please see the website.\nFeature overview:\n Undo: Log record for roll back modified data and transactions. Multitenant: Single database instance runs on a server and serves multiple tenants. Network based tablespace: Devide computing and storage functions in database through network based tablespace. Multithread: The multithreaded database model enables server processes to execute as operating system threads in separate address spaces. Recyclebin: Provide recycle bin to reserve dropped objects to prevent mistake. DDL event tracking: Tracking, recording, publishing and subscribing DDL operations.  "},{"id":25,"href":"/commands/alterundo/","title":"ALTER UNDO","parent":"SQL Commands","content":"ALTER UNDO — change the definition of an undo\nSynopsis     ALTER UNDO [ IF EXISTS ] name RENAME TO new_name\rALTER UNDO [ IF EXISTS ] name SET TABLESPACE tablespace_name\rALTER UNDO name OWNER TO { new_owner | CURRENT_USER | SESSION_USER }\rALTER UNDO [ IF EXISTS ] name SET ( storage_parameter = value [, ... ] )\rALTER UNDO [ IF EXISTS ] name RESET ( storage_parameter [, ... ] ) Description     ALTER UNDO changes the definition of an existing undo. There are several subforms described below. Note that the lock level required may differ for each subform. An ACCESS EXCLUSIVE lock is held unless explicitly noted. When multiple subcommands are listed, the lock held will be the strictest one required from any subcommand.\n  RENAME\nThe RENAME form changes the name of the undo. There is no effect on the stored data.\nRenaming an index acquires a SHARE UPDATE EXCLUSIVE lock.\n  SET TABLESPACE\nThis form changes the undo\u0026rsquo;s tablespace to the specified tablespace and moves the data file(s) associated with the undo to the new tablespace. To change the tablespace of an undo, you must own the undo and have CREATE privilege on the new tablespace.\n  OWNER TO\nThis form changes the undo\u0026rsquo;s owner to the specified owner. To alter the owner, you must also be a direct or indirect member of the new owning role. (Note that superusers have all these privileges automatically.)\n  SET ( storage_parameter=value [, ... ] )\nThis form changes one or more index-method-specific storage parameters for the undo. See CREATE UNDO for details on the available parameters. Note that the undo contents will not be modified immediately by this command; depending on the parameter you might need to clean the undo with VACUUM to get the desired effects.\n  RESET ( storage_parameter [, ... ] )\nThis form resets one or more undo-specific storage parameters to their defaults. As with SET, a VACUUM might be needed to clean the undo entirely.\n  Parameters       IF EXISTS\nDo not throw an error if the undo does not exist. A notice is issued in this case.\n  name\nThe name (possibly schema-qualified) of an existing undo to alter.\n  new_name\nThe new name for the undo.\n  tablespace_name\nThe tablespace to which the undo will be moved.\n  new_owner\nThe new owner of the undo.\n  storage_parameter\nThe name of an undo-specific storage parameter.\n  value\nThe new value for an undo-specific storage parameter. This might be a number or a word depending on the parameter.\n  Examples     To rename an existing undo:\nALTER UNDO undo_abc RENAME TO undo_test; To move an undo to a different tablespace:\nALTER UNDO undo_15 SET TABLESPACE undospace; To change an undo\u0026rsquo;s minimum pages:\nALTER UNDO undo_15 SET (minpages = 131072); Change the owner of undo undo_15:\nALTER UNDO undo_15 OWNER TO mary; Compatibility     ALTER UNDO is a Redrock Postgres extension.\n"},{"id":26,"href":"/commands/createundo/","title":"CREATE UNDO","parent":"SQL Commands","content":"CREATE UNDO — define a new undo\nSynopsis     CREATE UNDO [ IF NOT EXISTS ] name\r[ WITH ( storage_parameter = value [, ... ] ) ]\r[ TABLESPACE tablespace_name ] Description     CREATE UNDO constructs a new undo in the current database. The undo name must be distinct from the name of any existing undo in the current database. Undos are primarily used to hold old version data that has been modified in tables and indexes.\nTo create a undo, the invoking user must have the CREATE privilege for the current database. (Of course, superusers bypass this check.)\nParameters       IF NOT EXISTS\nDo not throw an error if a relation with the same name already exists. A notice is issued in this case. Note that there is no guarantee that the existing undo is anything like the one that would have been created.\n  name\nThe name of the undo to be created. No schema name can be included here; the undo is always created in the schema pg_catalog.\n  storage_parameter\nThe name of an undo-specific storage parameter. See Undo Storage Parameters for details.\n  tablespace_name\nThe tablespace in which to create the undo. If not specified, default_tablespace is consulted.\n  Undo Storage Parameters     The optional WITH clause specifies storage parameters for the undo. Accepted parameters include:\n  initialpages\nNumber of initial pages in undo. The default is 128.\n  minpages\nMinimum number of pages in undo. The default is 128.\n  maxpages\nMaximum number of pages in undo. The default is unlimited. It is just a hint, in fact, Redrock Postgres provides a fully automated mechanism, known as automatic undo management mode, for managing undos and space.\n  Notes     After the database cluster data directory is initialized, 4 cluster-level undos and 8 database-level undos are generated in the database by default, the cluster-level undos is mainly used to modify the data of the shared system catalogs, such as pg_database. The first page of the undo is used to store transaction table items, and a single undo can hold up to about 200 transaction items at the same time.\nHowever, if a single undo processes a large number of transactions at the same time (for example, more than 8), a corresponding page access wait event will occur due to concurrent access violations of the transaction table. In this case, you can use the CREATE UNDO command to create more undos in the current database to improve the transaction processing performance of the system.\nExamples     Create a undo:\nCREATE UNDO undo_13; To create an undo and have the undo reside in the tablespace undospace:\nCREATE UNDO undo_14 TABLESPACE undospace; Compatibility     CREATE UNDO is a Redrock Postgres language extension. There are no provisions for undos in the SQL standard.\n"},{"id":27,"href":"/commands/dropundo/","title":"DROP UNDO","parent":"SQL Commands","content":"DROP UNDO — remove an undo\nSynopsis     DROP UNDO [ IF EXISTS ] name [, ...] [ CASCADE | RESTRICT ] Description     DROP UNDO drops an existing undo from the database system. To execute this command you must be the owner of the undo.\nParameters       IF EXISTS\nDo not throw an error if the undo does not exist. A notice is issued in this case.\n  name\nThe name (optionally schema-qualified) of an undo to remove.\n  CASCADE\nAutomatically drop objects that depend on the undo, and in turn all objects that depend on those objects (see Section 5.14).\n  RESTRICT\nRefuse to drop the undo if any objects depend on it. This is the default.\n  Examples     This command will remove the undo undo_15:\nDROP UNDO undo_15; Compatibility     DROP UNDO is a Redrock Postgres language extension. There are no provisions for undos in the SQL standard.\n"},{"id":28,"href":"/catalogs/recyclebin/","title":"pg_recyclebin","parent":"System Catalogs","content":"The catalog pg_recyclebin contains part of the information about dropped objects. The rest is mostly in pg_class、pg_type、pg_proc and pg_constraint.\nTable. pg_recyclebin Columns\n   Name Type References Description     classid oid pg_class.oid The OID of the system catalog the dropped object is in   objid oid any OID column The OID of the specific dropped object   objsubid oid  When classid is the OID of the system catalog pg_namespace, this is the user OID of the schema owner. For all other object types, this column is 0.   namespace oid pg_namespace.oid The OID of the schema the dropped object is in   objname name any NAME column New name of the object   oldname name any NAME column Original name of the object   command char  Operation carried out on the object: d - Object was dropped r - Object was rewritten, eg: ALTER TABLE  t - Object was truncated   droptime timestamptz  Timestamp for the dropping of the object    "},{"id":29,"href":"/catalogs/ts-lexicon/","title":"pg_ts_lexicon","parent":"System Catalogs","content":"The pg_ts_lexicon catalog contains word entries defining text search lexicons.\nTable. pg_ts_lexicon Columns\n   Name Type Description     lexfreq int4 The frequency statistics of the word in the text   lextype int2 The type of the word. Possible values are:0: chinese words1: chinese units2: english and chinese mixed words3: chinese and english mixed words4: chinese last name5: chinese second name6: initial word of chinese compound surname7: last word of chinese compound surname8: chinese surname modifiers9: stop words10: english punctuation mixed words11: english words15: other words16: synonym words17: punctuations   lexkind char the part of speech of the word in the sentence, include: noun (n), pronoun (p), adjective (a), adverb (d), verb (v), numeral (m), article (t), preposition (r), conjunction (c), interjection (i)   lexword text literal representation of the word   lexsynonyms text[] synonyms for the word    "},{"id":30,"href":"/catalogs/undo/","title":"pg_undo","parent":"System Catalogs","content":"The catalog pg_undo contains part of the information about undos. The rest is mostly in pg_class.\nTable. pg_undo Columns\n   Name Type Description     undoid oid The ID of this undo, the system assigns sequentially from 1   undrelid oid The OID of the pg_class entry for this undo   undusecount int8 The number of times the transaction table entry in this undo has been reused, this field will only be updated when the undo is deleted   undreusetime logicaltime The logical timestamp of when the transaction table entry in this undo was reused, this field will only be updated when the undo is deleted   undisvalid bool If true, the undo can currently be used to provide read consistency. False means the undo has been completely deleted   undislive bool If false, the undo has been deleted    "},{"id":31,"href":"/tags/","title":"Tags","parent":"Welcome to Redrock Documentation","content":""},{"id":32,"href":"/commands/vacuum/","title":"VACUUM","parent":"SQL Commands","content":"VACUUM — garbage-collect a database\nSynopsis     VACUUM [ ( option [, ...] ) ] [ name [, ...] ]\rVACUUM [ VERBOSE ] [ name [, ...] ]\rwhere option can be one of:\rVERBOSE [ boolean ]\rSKIP_LOCKED [ boolean ]\rTRUNCATE [ boolean ] Description     VACUUM is used to do the following:\n Attempts to truncate off any empty pages at the end of the table and allow the disk space for the truncated pages to be returned to the operating system. Attempts to shrink undos. Clearing dropped objects in the recycle bin.  Without a name list, VACUUM processes every table, materialized view, undo, and dropped object in recycle bin in the current database that the current user has permission to vacuum. With a list, VACUUM processes only those table(s).\nWhen the option list is surrounded by parentheses, the options can be written in any order. Without parentheses, options must be specified in exactly the order shown above.\nParameters       VERBOSE\nPrints a detailed vacuum activity report for each table.\n  SKIP_LOCKED\nSpecifies that VACUUM should not wait for any conflicting locks to be released when beginning work on a relation: if a relation cannot be locked immediately without waiting, the relation is skipped. Note that even with this option, VACUUM may still block when opening the relation\u0026rsquo;s indexes. Also, while VACUUM ordinarily processes all partitions of specified partitioned tables, this option will cause VACUUM to skip all partitions if there is a conflicting lock on the partitioned table.\n  TRUNCATE\nSpecifies that VACUUM should attempt to truncate off any empty pages at the end of the table and allow the disk space for the truncated pages to be returned to the operating system. This is normally the desired behavior and is the default unless the vacuum_truncate option has been set to false for the table to be vacuumed. Setting this option to false may be useful to avoid ACCESS EXCLUSIVE lock on the table that the truncation requires.\n  boolean\nSpecifies whether the selected option should be turned on or off. You can write TRUE, ON, or 1 to enable the option, and FALSE, OFF, or 0 to disable it. The boolean value can also be omitted, in which case TRUE is assumed.\n  name\nThe name (optionally schema-qualified) of a specific table, materialized view or undo to vacuum. If the specified table is a partitioned table, all of its leaf partitions are vacuumed.\n  Outputs     When VERBOSE is specified, VACUUM emits progress messages to indicate which table is currently being processed. Various statistics about the tables are printed as well.\nNotes     To vacuum a table, one must ordinarily be the table\u0026rsquo;s owner or a superuser. However, database owners are allowed to vacuum all tables in their databases, except shared catalogs. (The restriction for shared catalogs means that a true database-wide VACUUM can only be performed by a superuser.) VACUUM will skip over any tables that the calling user does not have permission to vacuum.\nVACUUM cannot be executed inside a transaction block.\nExamples     To clean a single table onek, and print a detailed vacuum activity report:\nVACUUM (VERBOSE) onek; Compatibility     There is no VACUUM statement in the SQL standard.\n"},{"id":33,"href":"/","title":"Welcome to Redrock Documentation","parent":"","content":"Redrock is an object-relational database management system (ORDBMS) based on PostgreSQL, developed by the PostgreSQL Global Development Group. It is intentionally designed as an enterprise grade and cloud native database.\nGetting Started   Feature overview   Undo   Log record for roll back modified data and transactions.  Multitenant   Single database instance runs on a server and serves multiple tenants.  Network Attached Tablespace   Devide computing and storage functions in database through network attached tablespace.   Multithread   Single process with multiple worker threads, less overhead per context switch.  Recycle Bin   Provide recycle bin to reserve dropped objects to prevent mistake.  DDL Event Tracking   Tracking, auditing, publishing and subscribing DDL operations.   "}]