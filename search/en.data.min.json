[{"id":0,"href":"/usage/why-redrock/","title":"Why Redrock Postgres?","parent":"Usage","content":"PostgreSQL is an advanced relational database management system that supports an extended subset of the SQL standard, including transactions, foreign keys, subqueries, triggers, user-defined types and functions. In short, PostgreSQL has already gained a good reputation for data consistency, stability, high performance, feature rich and developer friendly.\nAlthough PostgreSQL has performed well in most senarios, there are still some areas for improvement, and that\u0026rsquo;s exactly what Redrock Postgres has done to achieve:\n   Feature Customer benefits     Implement in-place updates by introducing undo logs and marking index records deleted, and reuse space immediately after a transaction commit that performs a delete or non-in-place update Make the applications perform UPDATE/DELETE operations more stably and improve the storage space utilization   Perform updates to related index columns without updating each index, reducing write amplification Improve storage IO utilization and extend the availability of storage devices   Marks index records with transaction status information SELECT operations based on index scans will perform more stably   The transaction ID is identified by using the undo transaction table record entry to solve the transaction ID wraparound issue The system does not need to trigger high-risk freeze operations periodically, leaving the system in a stable and predictable operating state all year round   Removed the dedicated autovacuum process to perform retail deletes Make your database system run more stable   Subtransactions implemented based on the undo record position You can confidently use the basic features of savepoints, and exception handling code blocks in stored procedures   Supports statement level atomicity, a transaction behavior compatible to those popular relational databases Reduce the risk of migrating to Postgres from other databases   Run on Windows using a multithreaded Postgres model You can easily deploy and run Postgres efficiently on Windows   Revolutionary network attached tablespace technology You can flexibly respond to the growth of business data volume through network attached tablespaces   Multi-tenant database Use the database\u0026rsquo;s multi-tenant capabilities to support your SaaS applications   Sophisticated event trigger mechanism Flexibly auditing, publishing and subscribing the DDL operations    "},{"id":1,"href":"/usage/getting-started/","title":"Getting Started","parent":"Usage","content":"   Install Redrock Postgres Access database using psql Access database using Python What\u0026rsquo;s next?      Install Redrock Postgres     Before you can use Redrock Postgres you need to install it, of course. It is possible that PostgreSQL is already installed at your site, either because it was included in your operating system distribution or because the system administrator already installed it. If that is the case, you should uninstall the installed PostgreSQL first, as it may conflict with Redrock Postgres.\nIf you are installing Redrock Postgres yourself, then refer to Installation Guide for instructions on installation, and return to this guide when the installation is complete. Once you have created a database, you can access it by:\n Running the PostgreSQL interactive terminal program, called psql, which allows you to interactively enter, edit, and execute SQL commands. Using a graphical frontend tool such as DBeaver, pgAdmin 4, Navicat for PostgreSQL to access and manage a database. Choose a programming language you are good at, and write a custom application based on the PostgreSQL driver corresponding to the language.  Access database using psql     You can use the command line tool psql to access the database, for example, execute the following command to access the postgres database:\n$ psql postgres If you do not supply the database name then it will default to your user account name.\nIn psql, you will be greeted with the following message:\npsql (12.1) Type \u0026#34;help\u0026#34; for help. postgres=\u0026gt; The last line could also be:\npostgres=# That would mean you are a database superuser, which is most likely the case if you installed the PostgreSQL instance yourself. Being a superuser means that you are not subject to access controls.\nThe last line printed out by psql is the prompt, and it indicates that psql is listening to you and that you can type SQL queries into a work space maintained by psql. Try out these commands:\npostgres=\u0026gt; SELECT current_date; date ------------ 2016-01-07 (1 row) postgres=\u0026gt; SELECT 2 + 2; ?column? ---------- 4 (1 row) The psql program has a number of internal commands that are not SQL commands. They begin with the backslash character, \u0026ldquo;\\\u0026rdquo;. For example, to get out of psql, type:\npostgres=\u0026gt; \\q and psql will quit and return you to your command shell. (For more internal commands, type \\? at the psql prompt.) The full capabilities of psql are documented in psql.\nAccess database using Python     Python is a very popular scripting language. Here we take the Python programming language as an example to briefly introduce how to access the database. Before you start, you will need Python on your computer.\nAssume that you already have an up to date version of Python installed, then you should install psycopg. Psycopg is a PostgreSQL adapter for the Python programming language. It is a wrapper for the libpq, the official PostgreSQL client library.\nFor most operating systems, the quickest way to install psycopg is using the wheel package available on PyPI:\n$ pip install psycopg2-binary This will install a pre-compiled binary version of the module which does not require the build or runtime prerequisites. Make sure to use an up-to-date version of pip (you can upgrade it using something like pip install -U pip).\nYou may then import the psycopg2 package as usual, start to access database:\nimport psycopg2 # Connect to your postgres DB conn = psycopg2.connect(\u0026#34;dbname=postgres user=postgres password=pgpass\u0026#34;) # Open a cursor to perform database operations cur = conn.cursor() # Execute a query cur.execute(\u0026#34;SELECT * FROM pg_namespace\u0026#34;) # Retrieve query results records = cur.fetchall() More detailed information about psycopg, please refer to Psycopg Documentation.\nWhat\u0026rsquo;s next?     There are a lot more things to discover. To get more information, please refer to PostgreSQL Documentation.\n"},{"id":2,"href":"/installation/windows/","title":"Installing on Windows","parent":"Installation","content":"To perform an installation using the graphical installation wizard, you must have superuser or administrator privileges.\nThe following section walks you through installing PostgreSQL on a Windows host.\nTo start the installation wizard, assume sufficient privileges and double-click the installer icon; if prompted, provide a password.\nNote that in some versions of Windows, to invoke the installer with Administrator privileges, you need to right-click on the installer icon and select Run as Administrator from the context menu.\nThe Redrock 12 Setup Welcome window opens. Click Next to continue.\n    Fig. 1: The Redrock 12 Setup Welcome dialog    The Choose Install Location window opens. Accept the default installation directory, or specify an alternate location and click Next to continue.\n    Fig. 2: Choose Install Location dialog    Click Next to continue.\nThe Choose Data Directory window opens. Accept the default location or specify the name of the alternate directory in which you wish to store the data files.\n    Fig. 3: Choose Data Directory dialog    Click Next to continue.\nThe Server Options window opens.\n    Fig. 4: The Server Options dialog    PostgreSQL uses the Port field to specify the port number on which the server should listen. The default listener port is 5432.\nUse the Locale field to specify the locale that will be used by the new database cluster. The Default is the operating system locale.\nUse the Superuser field to specify the database superuser name. The default superuser name is postgres.\nUse the Password field to specify the database superuser password. The specified password should conform to enough complexities. After entering a password in the Password field, and confirming the password in the Retype Password field.\nClick Next to continue.\n    Fig. 5: Setting Configuration Parameters dialog    The Setting Configuration Parameters dialog will guides you to optimize server performance by setting some important configuration parameters. Maybe you can let the installer tune configuration parameters automatically, and click Next to continue.\n    Fig. 6: The Choose Start Menu Folder    During the installation, the setup wizard confirms the installation progress of PostgreSQL via a series of progress bars.\n    Fig. 7: The Installing dialog    When the Completing Redrock 12 Setup window appears, Congratulations! Click Finish to complete the PostgreSQL installation.\n    Fig. 8: Completing Redrock 12 Setup    "},{"id":3,"href":"/usage/","title":"Usage","parent":"Redrock Documentation","content":""},{"id":4,"href":"/installation/","title":"Installation","parent":"Redrock Documentation","content":"Table of Contents\n   Installing on Windows      Installing on Red Hat      Installing on Debian      The following sections of the chapter describe how to install the distribution that you choose.\n"},{"id":5,"href":"/features/","title":"Features","parent":"Redrock Documentation","content":""},{"id":6,"href":"/installation/redhat/","title":"Installing on Red Hat","parent":"Installation","content":"   Install Packages  Prerequisite Install Redrock Postgres   Post-installation commands  Data Directory Change Data Directory Initialize Startup   Control service After installation      Install Packages     Prerequisite     Log in to the host with your root account, and run the following commands to query installed packages, make sure none postgresql related packages already installed:\n# rpm -qa | grep postgresql Install Redrock Postgres     Download the Redrock Postgres Package for Red Hat or CentOS, and run the following commands to install:\n# tar xf redrock-12.1-1.el8.x86_64-bundle.tar # cd redrock-12.1-1.el8.x86_64-bundle # rpm -ivh redrock-12.1-1.el8.x86_64.rpm redrock-libs-12.1-1.el8.x86_64.rpm redrock-server-12.1-1.el8.x86_64.rpm Other packages can be installed according to your needs.\nPost-installation commands     After installing the packages, a database needs to be initialized and configured.\nIn the commands below, the value of version includes the major version of PostgreSQL, e.g., 12\nData Directory     The PostgreSQL data directory contains all of the data files for the database. The variable PGDATA is used to reference this directory.\nThe default data directory is:\n/var/lib/pgsql/\u0026lt;version\u0026gt;/data For example:\n/var/lib/pgsql/12/data Change Data Directory     If you do not want to use the default data directory, you can go to a custom mount point (eg: /u01) and create a folder pgdata with postgres permissions:\n# cd /u01 # mkdir pgdata # chown postgres:postgres pgdata Then, edit the postgresql service:\n# systemctl edit postgresql-12.service Go to the custom mount point that has the majority of the disk space, copy and paste the following into that file:\n[Service] Environment=PGDATA=/u01/pgdata Initialize     The first command (only needed once) is to initialize the database in PGDATA.\nIf the previous command did not work, try directly calling the setup binary, located in a similar naming scheme:\n# /usr/pgsql-\u0026lt;version\u0026gt;/bin/postgresql-\u0026lt;version\u0026gt;-setup initdb For versions 12, use:\n# /usr/pgsql-12/bin/postgresql-12-setup initdb Startup     If you want PostgreSQL to start automatically when the OS starts, do the following:\n# systemctl enable postgresql-12.service Control service     To control the database service, use:\n# systemctl \u0026lt;command\u0026gt; postgresql-12.service where command can be:\n enable : enable automatical start start : start the database stop : stop the database restart : stop/start the database; used to read changes to core configuration files reload : reload configuration files while keeping database running  E.g. to control version 12 database service, use:\n# systemctl enable postgresql-12.service # systemctl start postgresql-12.service After installation     Modify the pg_hba.conf file in data directory to define what authentication method should be used from all networks to the PostgreSQL server and modify the localhost authentication method (change from indent to md5 and change from localhost to accept all incoming requests). Find the lines below:\n# IPv4 local connections: host all all 127.0.0.1/32 ident And change it to:\n# IPv4 local connections: host all all 0.0.0.0/0 md5 Modify the postgresql.conf file in data directory to allow connections from all hosts by uncommenting the following line and adding an * instead of localhost:\nlisten_addresses = \u0026#39;*\u0026#39; Restart the database service to reload configurations:\n# systemctl restart postgresql-12.service In a production environment, you should also set up TLS-secured communication, and you should consider setting up data replication or snapshot-based backups. Consult the PostgreSQL online manual for these settings.\n"},{"id":7,"href":"/admin/configuration/","title":"Configuration","parent":"Server Administration","content":"Overview of all modified and new server configuration options based on PostgreSQL.\n   Resource Consumption Write Ahead Log Run-time Statistics Automatic Analyzing      Resource Consumption     These settings control the resource consumption of the database server.\n  threaded_execution (boolean)\nControls whether to enable the multithreaded Postgres model. In Redrock Postgres, the multithreaded Postgres model enables Postgres processes on UNIX and Linux to run as operating system threads in separate address spaces.\nBy default, session related backend processes on Windows always use threaded execution; the remaining background processes run as operating system processes. Thus, an \u0026ldquo;Postgres process\u0026rdquo; is not always equivalent to an \u0026ldquo;operating system process.\u0026rdquo;\n  Write Ahead Log     There are several WAL-related configuration parameters that affect database performance.\n  data_checksums (boolean)\nDetermines whether to calculate a checksum when writing a data block (a number calculated from all the bytes stored in the block) and store it in the page header of every data block when writing it to disk. Checksums are verified when a block is read - only if this parameter is on and the last write of the block stored a checksum. Besides, Postgres also verifies the checksum before a change application from update/delete statements and recomputes it after the change is applied. In addition, Postgres gives every WAL record a checksum before writing it to disk.\nIf this parameter is set to off, data checksums are turned off.\nChecksums allow Postgres to detect corruption caused by underlying disks, storage systems, or I/O systems. Turning on data checksums causes only an additional 1% to 2% overhead.\n  Run-time Statistics     These parameters control server-wide statistics collection features. When statistics collection is enabled, the data that is produced can be accessed via the pg_stat and pg_statio family of system views. Refer to Statistics Views for more information.\n  track_wait_events (boolean)\nEnables the collection of information on the wait events occured in each session, include occured times and timing information. This parameter is on by default. Note that even when enabled, this information is not visible to all users, only to superusers and the user owning the session being reported on, so it should not represent a security risk. Only superusers can change this setting.\n  Automatic Analyzing     These settings control the behavior of the autoanalyze feature. Refer to Section 24.1.6 for more information. Note that many of these settings can be overridden on a per-table basis; see Storage Parameters.\n  autoanalyze (boolean)\nControls whether the server should run the autoanalyze launcher daemon. This is on by default; however, track_counts must also be enabled for autoanalyze to work. This parameter can only be set in the postgresql.conf file or on the server command line; however, autoanalyzing can be disabled for individual tables by changing table storage parameters.\n  log_autoanalyze_min_duration (integer)\nCauses each action executed by autoanalyze to be logged if it ran for at least the specified amount of time. Setting this to zero logs all autoanalyze actions. -1 (the default) disables logging autoanalyze actions. If this value is specified without units, it is taken as milliseconds. For example, if you set this to 250ms then all automatic analyzes that run 250ms or longer will be logged. In addition, when this parameter is set to any value other than -1, a message will be logged if an autoanalyze action is skipped due to a conflicting lock or a concurrently dropped relation. Enabling this parameter can be helpful in tracking autoanalyze activity. This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.\n  autoanalyze_max_workers (integer)\nSpecifies the maximum number of autoanalyze processes (other than the autoanalyze launcher) that may be running at any one time. The default is three. This parameter can only be set at server start.\n  autoanalyze_naptime (integer)\nSpecifies the minimum delay between autoanalyze runs on any given database. In each round the daemon examines the database and issues ANALYZE commands as needed for tables in that database. If this value is specified without units, it is taken as seconds. The default is one minute (1min). This parameter can only be set in the postgresql.conf file or on the server command line.\n  autoanalyze_base_threshold (integer)\nSpecifies the minimum number of inserted, updated or deleted tuples needed to trigger an ANALYZE in any one table. The default is 50 tuples. This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.\n  autoanalyze_scale_factor (floating point)\nSpecifies a fraction of the table size to add to autoanalyze_base_threshold when deciding whether to trigger an ANALYZE. The default is 0.1 (10% of table size). This parameter can only be set in the postgresql.conf file or on the server command line; but the setting can be overridden for individual tables by changing table storage parameters.\n  "},{"id":8,"href":"/develop/","title":"Developer Guide","parent":"Redrock Documentation","content":""},{"id":9,"href":"/installation/debian/","title":"Installing on Debian","parent":"Installation","content":"   Install Packages  Prerequisite Install Redrock Postgres   Control service After installation      Install Packages     Prerequisite     Log in to the host with your root account, and run the following commands to install dependent packages:\n# apt install postgresql-common postgresql-client-common Install Redrock Postgres     Download the Redrock Postgres Package for Debian, and run the following commands to install:\n# tar xf redrock-12.1-1.debian10.x86_64-bundle.tar # cd redrock-12.1-1.debian10.x86_64-bundle # dpkg -i libpq5_12.1-1_amd64.deb redrock-client_12.1-1_amd64.deb redrock_12.1-1_amd64.deb Other packages can be installed according to your needs.\nControl service     To control the database service, use:\n# systemctl \u0026lt;command\u0026gt; postgresql where command can be:\n enable : enable automatical start start : start the database stop : stop the database restart : stop/start the database; used to read changes to core configuration files reload : reload configuration files while keeping database running  After installation     Modify the pg_hba.conf file in configuration directory (default: /etc/postgresql/12/main/) to define what authentication method should be used from all networks to the PostgreSQL server and modify the localhost authentication method (change from indent to md5 and change from localhost to accept all incoming requests). Find the lines below:\n# IPv4 local connections: host all all 127.0.0.1/32 ident And change it to:\n# IPv4 local connections: host all all 0.0.0.0/0 md5 Modify the postgresql.conf file in configuration directory (default: /etc/postgresql/12/main/ ) to allow connections from all hosts by uncommenting the following line and adding an * instead of localhost:\nlisten_addresses = \u0026#39;*\u0026#39; Restart the database service to reload configurations:\n# systemctl restart postgresql In a production environment, you should also set up TLS-secured communication, and you should consider setting up data replication or snapshot-based backups. Consult the PostgreSQL online manual for these settings.\n"},{"id":10,"href":"/features/transaction/","title":"Transactions","parent":"Features","content":"   Introduction to Transactions  Sample Transaction: Account Debit and Credit Structure of a Transaction  Beginning of a Transaction End of a Transaction   Statement Level Atomicity Logical Timestamp   Overview of Transaction Control  Active Transactions Savepoints Rollback to Savepoint Rollback of Transactions Committing Transactions        An RDBMS must be able to group SQL statements so that they are either all committed, which means they are applied to the database, or all rolled back, which means they are undone. A transaction is a logical, atomic unit of work that contains one or more SQL statements.\nAn illustration of the need for transactions is a funds transfer from a savings account to a checking account. The transfer consists of the following separate operations:\n Decrease the savings account. Increase the checking account. Record the transaction in the transaction journal.  PostgreSQL Database guarantees that all three operations succeed or fail as a unit. For example, if a hardware failure prevents a statement in the transaction from executing, then the other statements must be rolled back.\nTransactions are one of the features that sets PostgreSQL Database apart from a file system. If you perform an atomic operation that updates several files, and if the system fails halfway through, then the files will not be consistent. In contrast, a transaction moves an Postgres database from one consistent state to another. The basic principle of a transaction is \u0026ldquo;all or nothing\u0026rdquo;: an atomic operation succeeds or fails as a whole.\nIntroduction to Transactions     A transaction is a logical, atomic unit of work that contains one or more SQL statements. A transaction groups SQL statements so that they are either all committed, which means they are applied to the database, or all rolled back, which means they are undone from the database. PostgreSQL Database assigns every transaction a unique identifier called a transaction ID.\nAll PostgreSQL transactions comply with the basic properties of a database transaction, known as ACID properties. ACID is an acronym for the following:\n  Atomicity\nAll tasks of a transaction are performed or none of them are. There are no partial transactions. For example, if a transaction starts updating 100 rows, but the system fails after 20 updates, then the database rolls back the changes to these 20 rows.\n  Consistency\nThe transaction takes the database from one consistent state to another consistent state. For example, in a banking transaction that debits a savings account and credits a checking account, a failure must not cause the database to credit only one account, which would lead to inconsistent data.\n  Isolation\nThe effect of a transaction is not visible to other transactions until the transaction is committed. For example, one user updating the hr.employees table does not see the uncommitted changes to employees made concurrently by another user. Thus, it appears to users as if transactions are executing serially.\n  Durability\nChanges made by committed transactions are permanent. After a transaction completes, the database ensures through its recovery mechanisms that changes from the transaction are not lost.\n  The use of transactions is one of the most important ways that a database management system differs from a file system.\nSample Transaction: Account Debit and Credit     To illustrate the concept of a transaction, consider a banking database. When a customer transfers money from a savings account to a checking account, the transaction must consist of three separate operations:\n  Decrement the savings account\n  Increment the checking account\n  Record the transaction in the transaction journal\n  PostgreSQL must allow for two situations. If all three SQL statements maintain the accounts in proper balance, then the effects of the transaction can be applied to the database. However, if a problem such as insufficient funds, invalid account number, or a hardware failure prevents one or two of the statements in the transaction from completing, then the database must roll back the entire transaction so that the balance of all accounts is correct.\nFigure 1 illustrates a banking transaction. The first statement subtracts $500 from savings account 3209. The second statement adds $500 to checking account 3208. The third statement inserts a record of the transfer into the journal table. The final statement commits the transaction.\n    Figure 1 A Banking Transaction    Structure of a Transaction     A database transaction consists of one or more statements that together constitute an atomic change to the database. The statements in it can be either Data Manipulation Language (DML) statements or Data Definition Language (DDL) statements.\nA transaction has a beginning and an end.\nBeginning of a Transaction     In PostgreSQL, a transaction is set up by surrounding the SQL commands of the transaction with BEGIN and COMMIT commands. So our banking transaction would actually look like:\nBEGIN; UPDATE accounts SET balance = balance - 100.00 WHERE name = \u0026#39;Alice\u0026#39;; -- etc etc COMMIT; If, partway through the transaction, we decide we do not want to commit (perhaps we just noticed that Alice\u0026rsquo;s balance went negative), we can issue the command ROLLBACK instead of COMMIT, and all our updates so far will be canceled.\nPostgreSQL actually treats every SQL statement as being executed within a transaction. If you do not issue a BEGIN command, then each individual statement has an implicit BEGIN and (if successful) COMMIT wrapped around it. A group of statements surrounded by BEGIN and COMMIT is sometimes called a transaction block.\n Some client libraries issue BEGIN and COMMIT commands automatically, so that you might get the effect of transaction blocks without asking. Check the documentation for the interface you are using.  When a transaction begins, Redrock Postgres assigns the transaction to an available undo segment to record the undo entries for the new transaction. A transaction ID is not allocated until an undo segment and transaction table slot are allocated, which occurs during the first DML/DDL statement. A transaction ID is unique to a transaction and represents the undo segment number, slot, and sequence number.\nThe following example execute an UPDATE statement to begin a transaction and queries for details about the transaction:\nBEGIN; UPDATE hr.employees SET salary = salary; SELECT xid, pg_xact_commit_status(xid) AS status FROM current_xid() AS xid; xid | status ----------+-------------  (5,16,1) | in progress End of a Transaction     A transaction ends when any of the following actions occurs:\n  A user issues a COMMIT or ROLLBACK statement without a SAVEPOINT clause.\nIn a commit, a user explicitly or implicitly requested that the changes in the transaction be made permanent. Changes made by the transaction are permanent and visible to other users only after a transaction commits. The transaction shown in Figure 1 ends with a commit.\n  A user exits normally from most PostgreSQL Database utilities and tools, causing the current transaction to be implicitly committed. The commit behavior when a user disconnects is application-dependent and configurable.\nNote: Applications should always explicitly commit or rollback transactions before program termination.\n  A client process terminates abnormally, causing the transaction to be implicitly rolled back using metadata stored in the transaction table and the undo.\n  After one transaction ends, the next executable SQL statement automatically starts the following transaction. The following example executes an UPDATE to start a transaction, ends the transaction with a ROLLBACK statement, and then executes an UPDATE to start a new transaction (note that the transaction IDs are different):\nBEGIN; UPDATE hr.employees SET salary = salary; SELECT xid, pg_xact_commit_status(xid) AS status FROM current_xid() AS xid; xid | status ----------+-------------  (6,16,1) | in progress ROLLBACK; SELECT current_xid() AS xid; xid ---------  (0,0,0) BEGIN; UPDATE hr.employees SET last_name = last_name; SELECT xid, pg_xact_commit_status(xid) AS status FROM current_xid() AS xid; xid | status ----------+-------------  (7,16,1) | in progress Statement Level Atomicity     Redrock Postgres supports statement-level atomicity, which means that a SQL statement is an atomic unit of work and either completely succeeds or completely fails.\nA successful statement is different from a committed transaction. A single SQL statement executes successfully if the database parses and runs it without error as an atomic unit, as when all rows are changed in a multirow update.\nIf a SQL statement causes an error during execution, then it is not successful and so all effects of the statement are rolled back. This operation is a statement-level rollback. This operation has the following characteristics:\n  A SQL statement that does not succeed causes the loss only of work it would have performed itself.\nThe unsuccessful statement does not cause the loss of any work that preceded it in the current transaction. For example, if the execution of the second UPDATE statement in Figure 1 causes an error and is rolled back, then the work performed by the first UPDATE statement is not rolled back. The first UPDATE statement can be committed or rolled back explicitly by the user.\n  The effect of the rollback is as if the statement had never been run.\nAny side effects of an atomic statement, for example, triggers invoked upon execution of the statement, are considered part of the atomic statement. Either all work generated as part of the atomic statement succeeds or none does.\n  An example of an error causing a statement-level rollback is an attempt to insert a duplicate primary key. Single SQL statements involved in a deadlock, which is competition for the same data, can also cause a statement-level rollback. However, errors discovered during SQL statement parsing, such as a syntax error, have not yet been run and so do not cause a statement-level rollback.\nLogical Timestamp     A logical timestamp (logicaltime) is a logical, internal time stamp used by Redrock Postgres Database. logicaltimes order events that occur within the database, which is necessary to satisfy the ACID properties of a transaction. Redrock Postgres uses logicaltimes to mark the logicaltime before which all changes are known to be on disk so that recovery avoids applying unnecessary redo. The database also uses logicaltimes to mark the point at which no redo exists for a set of data so that recovery can stop.\nlogicaltimes occur in a monotonically increasing sequence. Redrock Postgres can use an logicaltime like a clock because an observed logicaltime indicates a logical point in time and repeated observations return equal or greater values. If one event has a lower logicaltime than another event, then it occurred at an earlier time with respect to the database. Several events may share the same logicaltime, which means that they occurred at the same time with respect to the database.\nEvery transaction has an logicaltime. For example, if a transaction updates a row, then the database records the logicaltime at which this update occurred. Other modifications in this transaction have the same logicaltime. When a transaction commits, the database records an logicaltime for this commit.\nRedrock Postgres increments logicaltimes in the shared memory area. When a transaction modifies data, the database writes a new logicaltime to the undo assigned to the transaction. The log writer process then writes the commit record of the transaction immediately to the online redo log. The commit record has the unique logicaltime of the transaction. Redrock Postgres also uses logicaltimes as part of its instance recovery and media recovery mechanisms.\nOverview of Transaction Control     Transaction control is the management of changes made by SQL statements and the grouping of SQL statements into transactions. In general, application designers are concerned with transaction control so that work is accomplished in logical units and data is kept consistent.\nTransaction control involves using the following statements:\n BEGIN initiates a transaction block, that is, all statements after a BEGIN command will be executed in a single transaction until an explicit COMMIT or ROLLBACK is given. The COMMIT statement ends the current transaction and makes all changes performed in the transaction permanent. COMMIT also erases all savepoints in the transaction and releases transaction locks. The ROLLBACK statement reverses the work done in the current transaction; it causes all data changes since the last COMMIT or ROLLBACK to be discarded. The ROLLBACK TO SAVEPOINT statement undoes the changes since the last savepoint but does not end the entire transaction. The SAVEPOINT statement identifies a point in a transaction to which you can later roll back.  The session in Table 1 illustrates the basic concepts of transaction control.\nTable 1 Transaction Control\n   Time Session Explanation     t0 BEGIN; This statement begins a transaction block.   t1 UPDATE employees SET salary = 7000 WHERE last_name = \u0026lsquo;Banda\u0026rsquo;; This statement updates the salary for Banda to 7000.   t2 SAVEPOINT after_banda_sal; This statement creates a savepoint named after_banda_sal, enabling changes in this transaction to be rolled back to this point.   t3 UPDATE employees SET salary = 12000 WHERE last_name = \u0026lsquo;Greene\u0026rsquo;; This statement updates the salary for Greene to 12000.   t4 SAVEPOINT after_greene_sal; This statement creates a savepoint named after_greene_sal, enabling changes in this transaction to be rolled back to this point.   t5 ROLLBACK TO SAVEPOINT after_banda_sal; This statement rolls back the transaction to t2, undoing the update to Greene\u0026rsquo;s salary at t3. The transaction has not ended.   t6 UPDATE employees SET salary = 11000 WHERE last_name = \u0026lsquo;Greene\u0026rsquo;; This statement updates the salary for Greene to 11000 in transaction.   t7 ROLLBACK; This statement rolls back all changes in transaction, ending the transaction.   t8 BEGIN; This statement begins a new transaction block.   t9 UPDATE employees SET salary = 7050 WHERE last_name = \u0026lsquo;Banda\u0026rsquo;; This statement updates the salary for Banda to 7050.   t10 UPDATE employees SET salary = 10950 WHERE last_name = \u0026lsquo;Greene\u0026rsquo;; This statement updates the salary for Greene to 10950.   t11 COMMIT; This statement commits all changes made in transaction, ending the transaction. The commit guarantees that the changes are saved in the online redo log files.    Active Transactions     An active transaction has started but not yet committed or rolled back. In Table 1, the first statement to modify data in the transaction is the update to Banda\u0026rsquo;s salary. From the successful execution of this update until the ROLLBACK statement ends the transaction, the transaction is active.\nData changes made by a transaction are temporary until the transaction is committed or rolled back. Before the transaction ends, the state of the data is as follows:\n  Redrock Postgres has generated undo data information in the shared buffers.\nThe undo data contains the old data values changed by the SQL statements of the transaction.\n  Redrock Postgres has generated redo in the shared WAL buffers.\nThe redo log record contains the change to the data block and the change to the undo block.\n  Changes have been made to the database pages in shared buffers.\nThe data changes for a committed transaction, stored in the database pages in shared buffers, are not necessarily written immediately to the data files by the database writer (bgwriter). The disk write can happen before or after the commit.\n  The rows affected by the data change are locked.\nOther users cannot change the data in the affected rows, nor can they see the uncommitted changes.\n  Savepoints     A savepoint is a user-declared intermediate marker within the context of a transaction. Internally, this marker resolves to an undo record position. Savepoints divide a long transaction into smaller parts.\nIf you use savepoints in a long transaction, then you have the option later of rolling back work performed before the current point in the transaction but after a declared savepoint within the transaction. Thus, if you make an error, you do not need to resubmit every statement. Table 1 creates savepoint after_banda_sal so that the update to the Greene salary can be rolled back to this savepoint.\nRollback to Savepoint     A rollback to a savepoint in an uncommitted transaction means undoing any changes made after the specified savepoint, but it does not mean a rollback of the transaction itself. When a transaction is rolled back to a savepoint, as when the ROLLBACK TO SAVEPOINT after_banda_sal is run in Table 1, the following occurs:\n  Redrock Postgres rolls back only the statements run after the savepoint.\nIn Table 1, the ROLLBACK TO SAVEPOINT causes the UPDATE for Greene to be rolled back, but not the UPDATE for Banda.\n  Redrock Postgres preserves the savepoint specified in the ROLLBACK TO SAVEPOINT statement, but all subsequent savepoints are lost.\nIn Table 1, the ROLLBACK TO SAVEPOINT causes the after_greene_sal savepoint to be lost.\n  Redrock Postgres releases all table and row locks acquired after the specified savepoint but retains all data locks acquired previous to the savepoint.\nThe transaction remains active and can be continued.\n  Rollback of Transactions     A rollback of an uncommitted transaction undoes any changes to data that have been performed by SQL statements within the transaction. After a transaction has been rolled back, the effects of the work done in the transaction no longer exist.\nIn rolling back an entire transaction, without referencing any savepoints, Redrock Postgres performs the following actions:\n  Undoes all changes made by all the SQL statements in the transaction by using the corresponding undo segments\nThe transaction table entry for every active transaction contains a pointer to all the undo data (in reverse order of application) for the transaction. The database reads the data from the undo segment, reverses the operation, and then marks the undo entry as applied. Thus, if a transaction inserts a row, then a rollback deletes it. If a transaction updates a row, then a rollback reverses the update. If a transaction deletes a row, then a rollback reinserts it. In Table 1, the ROLLBACK reverses the updates to the salaries of Greene and Banda.\n  Releases all the locks of data held by the transaction\n  Erases all savepoints in the transaction\nIn Table 1, the ROLLBACK deletes the savepoint after_banda_sal. The after_greene_sal savepoint was removed by the ROLLBACK TO SAVEPOINT statement.\n  Ends the transaction\nIn Table 1, the ROLLBACK leaves the database in the same state as it was after the initial COMMIT was executed.\n  The duration of a rollback is a function of the amount of data modified.\nCommitting Transactions     A commit ends the current transaction and makes permanent all changes performed in the transaction. In Table 1, a second transaction begins with BEGIN statement and ends with an explicit COMMIT statement. The changes that resulted from the two UPDATE statements are now made permanent.\nWhen a transaction commits, the following actions occur:\n  A logical timestamp (logicaltime) is generated for the COMMIT.\nThe internal transaction table for the associated undo tablespace records that the transaction has committed. The corresponding unique logicaltime of the transaction is assigned and recorded in the transaction table.\n  The session process writes remaining WAL records in the WAL buffers to the WAL log and writes the transaction logicaltime to the WAL log. This atomic event constitutes the commit of the transaction.\n  Redrock Postgres releases locks held on rows and tables.\nUsers who were enqueued waiting on locks held by the uncommitted transaction are allowed to proceed with their work.\n  Redrock Postgres deletes savepoints.\nIn Table 1, no savepoints existed in the transaction so no savepoints were erased.\n  Redrock Postgres performs a commit cleanout.\nIf modified blocks containing data from the committed transaction are still in the shared buffers, and if no other session is modifying them, then the database removes lock-related transaction information from the blocks. Ideally, the COMMIT cleans out the blocks so that a subsequent SELECT does not have to perform this task.\nNote: Because a block cleanout generates redo, a query may generate redo and thus cause blocks to be written during the next checkpoint.\n  Redrock Postgres marks the transaction complete.\n  After a transaction commits, users can view the changes.\nTypically, a commit is a fast operation, regardless of the transaction size. The speed of a commit does not increase with the size of the data modified in the transaction. The lengthiest part of the commit is the physical disk I/O occured in WAL writes. However, the amount of time spent in transaction commit is reduced because walwriter has been incrementally writing the contents of the WAL buffer in the background.\nThe default behavior is for session to write redo to the WAL log synchronously and for transactions to wait for the buffered redo to be on disk before returning a commit to the user. However, for lower transaction commit latency, application developers can specify that redo be written asynchronously so that transactions need not wait for the redo to be on disk and can return from the COMMIT call immediately.\n"},{"id":11,"href":"/admin/monitor-stats/","title":"Statistics Views","parent":"Server Administration","content":"   Statistics Collection Configuration Viewing Statistics pg_stat_activity pg_stat_wait_event pg_stat_replication pg_stat_database pg_stat_all_tables pg_stat_all_undos pg_stat_all_indexes pg_statio_all_tables pg_statio_all_undos pg_statio_all_indexes pg_statio_all_sequences      Redrock Postgres\u0026rsquo;s statistics collector is a subsystem that supports collection and reporting of information about server activity. Presently, the collector can count accesses to tables and indexes in both disk-block and individual-row terms. It also tracks the total number of rows in each table, and information about analyze actions for each table. It can also count calls to user-defined functions and the total time spent in each one.\nRedrock Postgres also supports reporting dynamic information about exactly what is going on in the system right now, such as the exact command currently being executed by other server processes, and which other connections exist in the system. This facility is independent of the collector process.\nStatistics Collection Configuration     Since collection of statistics adds some overhead to query execution, the system can be configured to collect or not collect information. This is controlled by configuration parameters that are normally set in postgresql.conf. (See Chapter 19 for details about setting configuration parameters.)\nThe parameter track_activities enables monitoring of the current command being executed by any server process.\nThe parameter track_counts controls whether statistics are collected about table and index accesses.\nThe parameter track_functions enables tracking of usage of user-defined functions.\nThe parameter track_io_timing enables monitoring of block read and write times.\nThe parameter track_wait_events controls whether statistics are collected about wait events.\nNormally these parameters are set in postgresql.conf so that they apply to all server processes, but it is possible to turn them on or off in individual sessions using the SET command. (To prevent ordinary users from hiding their activity from the administrator, only superusers are allowed to change these parameters with SET.)\nThe statistics collector transmits the collected information to other PostgreSQL processes through temporary files. These files are stored in the directory named by the stats_temp_directory parameter, pg_tmp/stats by default. For better performance, stats_temp_directory can be pointed at a RAM-based file system, decreasing physical I/O requirements. When the server shuts down cleanly, a permanent copy of the statistics data is stored in the pg_stat subdirectory, so that statistics can be retained across server restarts. When recovery is performed at server start (e.g., after immediate shutdown, server crash, and point-in-time recovery), all statistics counters are reset.\nViewing Statistics     Several predefined views, listed in Table 1, are available to show the current state of the system. There are also several other views, listed in Table 2, available to show the results of statistics collection. Alternatively, one can build custom views using the underlying statistics functions, as discussed in Section 27.2.3.\nWhen using the statistics to monitor collected data, it is important to realize that the information does not update instantaneously. Each individual server process transmits new statistical counts to the collector just before going idle; so a query or transaction still in progress does not affect the displayed totals. Also, the collector itself emits a new report at most once per PGSTAT_STAT_INTERVAL milliseconds (500 ms unless altered while building the server). So the displayed information lags behind actual activity. However, current-query information collected by track_activities is always up-to-date.\nAnother important point is that when a server process is asked to display any of these statistics, it first fetches the most recent report emitted by the collector process and then continues to use this snapshot for all statistical views and functions until the end of its current transaction. So the statistics will show static information as long as you continue the current transaction. Similarly, information about the current queries of all sessions is collected when any such information is first requested within a transaction, and the same information will be displayed throughout the transaction. This is a feature, not a bug, because it allows you to perform several queries on the statistics and correlate the results without worrying that the numbers are changing underneath you. But if you want to see new results with each query, be sure to do the queries outside any transaction block. Alternatively, you can invoke pg_stat_clear_snapshot(), which will discard the current transaction\u0026rsquo;s statistics snapshot (if any). The next use of statistical information will cause a new snapshot to be fetched.\nA transaction can also see its own statistics (as yet untransmitted to the collector) in the views pg_stat_xact_all_tables, pg_stat_xact_all_undos, pg_stat_xact_sys_tables, pg_stat_xact_user_tables, and pg_stat_xact_user_functions. These numbers do not act as stated above; instead they update continuously throughout the transaction.\nSome of the information in the dynamic statistics views shown in Table 1 is security restricted. Ordinary users can only see all the information about their own sessions (sessions belonging to a role that they are a member of). In rows about other sessions, many columns will be null. Note, however, that the existence of a session and its general properties such as its sessions user and database are visible to all users. Superusers and members of the built-in role pg_read_all_stats (see also Section 21.5) can see all the information about all sessions.\nTable 1. Dynamic Statistics Views\n   View Name Description     pg_stat_activity One row per server process, showing information related to the current activity of that process, such as state and current query. See pg_stat_activity for details.   pg_stat_wait_event One row per wait event, showing statistics about wait information of every occured wait events. See pg_stat_wait_event for details.   pg_stat_replication One row per WAL sender process, showing statistics about replication to that sender\u0026rsquo;s connected standby server. See pg_stat_replication for details.    Table 2. Collected Statistics Views\n   View Name Description     pg_stat_database One row per database, showing database-wide statistics. See pg_stat_database for details.   pg_stat_all_tables One row for each table in the current database, showing statistics about accesses to that specific table. See pg_stat_all_tables for details.   pg_stat_sys_tables Same as pg_stat_all_tables, except that only system tables are shown.   pg_stat_user_tables Same as pg_stat_all_tables, except that only user tables are shown.   pg_stat_xact_all_tables Similar to pg_stat_all_tables, but counts actions taken so far within the current transaction (which are not yet included in pg_stat_all_tables and related views). The columns for numbers of live and dead rows and analyze actions are not present in this view.   pg_stat_xact_sys_tables Same as pg_stat_xact_all_tables, except that only system tables are shown.   pg_stat_xact_user_tables Same as pg_stat_xact_all_tables, except that only user tables are shown.   pg_stat_all_undos One row for each undo in the current database, showing statistics about accesses to that specific undo. See pg_stat_all_undos for details.   pg_stat_xact_all_undos Similar to pg_stat_all_undos, but counts actions taken so far within the current transaction (which are not yet included in pg_stat_all_undos and related views). The columns for numbers of live and dead records are not present in this view.   pg_stat_all_indexes One row for each index in the current database, showing statistics about accesses to that specific index. See pg_stat_all_indexes for details.   pg_stat_sys_indexes Same as pg_stat_all_indexes, except that only indexes on system tables are shown.   pg_stat_user_indexes Same as pg_stat_all_indexes, except that only indexes on user tables are shown.   pg_statio_all_tables One row for each table in the current database, showing statistics about I/O on that specific table. See pg_statio_all_tables for details.   pg_statio_sys_tables Same as pg_statio_all_tables, except that only system tables are shown.   pg_statio_user_tables Same as pg_statio_all_tables, except that only user tables are shown.   pg_statio_all_undos One row for each undo in the current database, showing statistics about I/O on that specific undo. See pg_statio_all_undos for details.   pg_statio_all_indexes One row for each index in the current database, showing statistics about I/O on that specific index. See pg_statio_all_indexes for details.   pg_statio_sys_indexes Same as pg_statio_all_indexes, except that only indexes on system tables are shown.   pg_statio_user_indexes Same as pg_statio_all_indexes, except that only indexes on user tables are shown.   pg_statio_all_sequences One row for each sequence in the current database, showing statistics about I/O on that specific sequence. See pg_statio_all_sequences for details.   pg_statio_sys_sequences Same as pg_statio_all_sequences, except that only system sequences are shown. (Presently, no system sequences are defined, so this view is always empty.)   pg_statio_user_sequences Same as pg_statio_all_sequences, except that only user sequences are shown.    The per-index statistics are particularly useful to determine which indexes are being used and how effective they are.\nThe pg_statio_ views are primarily useful to determine the effectiveness of the buffer cache. When the number of actual disk reads is much smaller than the number of buffer hits, then the cache is satisfying most read requests without invoking a kernel call. However, these statistics do not give the entire story: due to the way in which PostgreSQL handles disk I/O, data that is not in the PostgreSQL buffer cache might still reside in the kernel\u0026rsquo;s I/O cache, and might therefore still be fetched without requiring a physical read. Users interested in obtaining more detailed information on PostgreSQL I/O behavior are advised to use the PostgreSQL statistics collector in combination with operating system utilities that allow insight into the kernel\u0026rsquo;s handling of I/O.\npg_stat_activity     The pg_stat_activity view will have one row per server process, showing information related to the current activity of that process.\nTable 3. pg_stat_activity View\n   Column Type Description     datid oid OID of the database this backend is connected to   datname name Name of the database this backend is connected to   pid integer Database Process ID of this backend   spid integer System Process ID of this backend   tid integer Thread ID of this backend   usesysid oid OID of the user logged into this backend   usename name Name of the user logged into this backend   application_name text Name of the application that is connected to this backend   client_addr inet IP address of the client connected to this backend. If this field is null, it indicates either that the client is connected via a Unix socket on the server machine or that this is an internal process such as autoanalyze.   client_hostname text Host name of the connected client, as reported by a reverse DNS lookup of client_addr. This field will only be non-null for IP connections, and only when log_hostname is enabled.   client_port integer TCP port number that the client is using for communication with this backend, or -1 if a Unix socket is used   backend_start timestamp with time zone Time when this process was started. For client backends, this is the time the client connected to the server.   xact_start timestamp with time zone Time when this process\u0026rsquo; current transaction was started, or null if no transaction is active. If the current query is the first of its transaction, this column is equal to the query_start column.   query_start timestamp with time zone Time when the currently active query was started, or if state is not active, when the last query was started   state_change timestamp with time zone Time when the state was last changed   wait_event_type text The type of event for which the backend is waiting, if any; otherwise NULL. Possible values are:LWLock: The backend is waiting for a lightweight lock. Each such lock protects a particular data structure in shared memory. wait_event will contain a name identifying the purpose of the lightweight lock. (Some locks have specific names; others are part of a group of locks each with a similar purpose.)Lock: The backend is waiting for a heavyweight lock. Heavyweight locks, also known as lock manager locks or simply locks, primarily protect SQL-visible objects such as tables. However, they are also used to ensure mutual exclusion for certain internal operations such as relation extension. wait_event will identify the type of lock awaited.BufferPin: The server process is waiting to access to a data buffer during a period when no other process can be examining that buffer. Buffer pin waits can be protracted if another process holds an open cursor which last read data from the buffer in question.Activity: The server process is idle. This is used by system processes waiting for activity in their main processing loop. wait_event will identify the specific wait point.Extension: The server process is waiting for activity in an extension module. This category is useful for modules to track custom waiting points.Client: The server process is waiting for some activity on a socket from user applications, and that the server expects something to happen that is independent from its internal processes. wait_event will identify the specific wait point.IPC: The server process is waiting for some activity from another process in the server. wait_event will identify the specific wait point.Timeout: The server process is waiting for a timeout to expire. wait_event will identify the specific wait point.IO: The server process is waiting for a IO to complete. wait_event will identify the specific wait point.   wait_event text Wait event name if backend is currently waiting, otherwise NULL. See Table 27.4 for details.   memory_used bigint Memory used by this backend.   state text Current overall state of this backend. Possible values are:active: The backend is executing a query.idle: The backend is waiting for a new client command.idle in transaction: The backend is in a transaction, but is not currently executing a query.idle in transaction (aborted): This state is similar to idle in transaction, except one of the statements in the transaction caused an error.fastpath function call: The backend is executing a fast-path function.disabled: This state is reported if track_activities is disabled in this backend.   backend_xid xid Top-level transaction identifier of this backend, if any.   backend_mintime logicaltime The current backend\u0026rsquo;s mintime horizon.   query text Text of this backend\u0026rsquo;s most recent query. If state is active this field shows the currently executing query. In all other states, it shows the last query that was executed. By default the query text is truncated at 1024 bytes; this value can be changed via the parameter track_activity_query_size.   backend_type text Type of current backend. Possible types are autoanalyze launcher, autoanalyze worker, logical replication launcher, logical replication worker, parallel worker, background writer, client backend, checkpointer, startup, walreceiver, walsender and walwriter. In addition, background workers registered by extensions may have additional types.     The wait_event and state columns are independent. If a backend is in the active state, it may or may not be waiting on some event. If the state is active and wait_event is non-null, it means that a query is being executed, but is being blocked somewhere in the system.  Table 4. Wait Event Types\n   Wait Event Type Description     Activity The server process is idle. This event type indicates a process waiting for activity in its main processing loop. wait_event will identify the specific wait point; see Table 5.   BufferPin The server process is waiting for exclusive access to a data buffer. Buffer pin waits can be protracted if another process holds an open cursor that last read data from the buffer in question. See Table 6.   Client The server process is waiting for activity on a socket connected to a user application. Thus, the server expects something to happen that is independent of its internal processes. wait_event will identify the specific wait point; see Table 7.   Extension The server process is waiting for some condition defined by an extension module. See Table 8.   IO The server process is waiting for an I/O operation to complete. wait_event will identify the specific wait point; see Table 9.   IPC The server process is waiting for some interaction with another server process. wait_event will identify the specific wait point; see Table 10.   Lock The server process is waiting for a heavyweight lock. Heavyweight locks, also known as lock manager locks or simply locks, primarily protect SQL-visible objects such as tables. However, they are also used to ensure mutual exclusion for certain internal operations such as relation extension. wait_event will identify the type of lock awaited; see Table 11.   LWLock The server process is waiting for a lightweight lock. Most such locks protect a particular data structure in shared memory. wait_event will contain a name identifying the purpose of the lightweight lock. (Some locks have specific names; others are part of a group of locks each with a similar purpose.) See Table 12.   Timeout The server process is waiting for a timeout to expire. wait_event will identify the specific wait point; see Table 13.    Table 5. Wait Events of Type Activity\n   Activity Wait Event Description     ArchiverMain Waiting in main loop of archiver process.   AnalyzeLauncherMain Waiting in main loop of autoanalyze launcher process.   BgWriterHibernate Waiting in background writer process, hibernating.   BgWriterMain Waiting in main loop of background writer process.   CheckpointerMain Waiting in main loop of checkpointer process.   LogicalApplyMain Waiting in main loop of logical replication apply process.   LogicalLauncherMain Waiting in main loop of logical replication launcher process.   PgStatMain Waiting in main loop of statistics collector process.   RecoveryWalStream Waiting in main loop of startup process for WAL to arrive, during streaming recovery.   SysLoggerMain Waiting in main loop of syslogger process.   WalReceiverMain Waiting in main loop of WAL receiver process.   WalSenderMain Waiting in main loop of WAL sender process.   WalWriterMain Waiting in main loop of WAL writer process.    Table 6. Wait Events of Type BufferPin\n   BufferPin Wait Event Description     BufferPin Waiting to acquire an exclusive pin on a buffer.    Table 7. Wait Events of Type Client\n   Client Wait Event Description     ClientRead Waiting to read data from the client.   ClientWrite Waiting to write data to the client.   GSSOpenServer Waiting to read data from the client while establishing a GSSAPI session.   LibPQWalReceiverConnect Waiting in WAL receiver to establish connection to remote server.   LibPQWalReceiverReceive Waiting in WAL receiver to receive data from remote server.   LibPQStorageConnect Waiting in compute server to establish connection to remote storage server.   LibPQStorageReceive Waiting in compute server to receive data from remote storage server.   SSLOpenServer Waiting for SSL while attempting connection.   WalSenderWaitForWAL Waiting for WAL to be flushed in WAL sender process.   WalSenderWriteData Waiting for any activity when processing replies from WAL receiver in WAL sender process.    Table 8. Wait Events of Type Extension\n   Extension Wait Event Description     Extension Waiting in an extension.    Table 9. Wait Events of Type IO\n   IO Wait Event Description     BaseBackupRead Waiting for base backup to read from a file.   BufFileRead Waiting for a read from a buffered file.   BufFileWrite Waiting for a write to a buffered file.   BufFileTruncate Waiting for a buffered file to be truncated.   ControlFileRead Waiting for a read from the pg_control file.   ControlFileSync Waiting for the pg_control file to reach durable storage.   ControlFileSyncUpdate Waiting for an update to the pg_control file to reach durable storage.   ControlFileWrite Waiting for a write to the pg_control file.   ControlFileWriteUpdate Waiting for a write to update the pg_control file.   CopyFileRead Waiting for a read during a file copy operation.   CopyFileWrite Waiting for a write during a file copy operation.   DSMFillZeroWrite Waiting to fill a dynamic shared memory backing file with zeroes.   DataFileExtend Waiting for a relation data file to be extended.   DataFileFlush Waiting for a relation data file to reach durable storage.   DataFileImmediateSync Waiting for an immediate synchronization of a relation data file to durable storage.   DataFilePrefetch Waiting for an asynchronous prefetch from a relation data file.   DataFileRead Waiting for a read from a relation data file.   DataFileSync Waiting for changes to a relation data file to reach durable storage.   DataFileTruncate Waiting for a relation data file to be truncated.   DataFileWrite Waiting for a write to a relation data file.   LockFileAddToDataDirRead Waiting for a read while adding a line to the data directory lock file.   LockFileAddToDataDirSync Waiting for data to reach durable storage while adding a line to the data directory lock file.   LockFileAddToDataDirWrite Waiting for a write while adding a line to the data directory lock file.   LockFileCreateRead Waiting to read while creating the data directory lock file.   LockFileCreateSync Waiting for data to reach durable storage while creating the data directory lock file.   LockFileCreateWrite Waiting for a write while creating the data directory lock file.   LockFileReCheckDataDirRead Waiting for a read during recheck of the data directory lock file.   LogicalRewriteCheckpointSync Waiting for logical rewrite mappings to reach durable storage during a checkpoint.   LogicalRewriteMappingSync Waiting for mapping data to reach durable storage during a logical rewrite.   LogicalRewriteMappingWrite Waiting for a write of mapping data during a logical rewrite.   LogicalRewriteSync Waiting for logical rewrite mappings to reach durable storage.   LogicalRewriteTruncate Waiting for truncate of mapping data during a logical rewrite.   LogicalRewriteWrite Waiting for a write of logical rewrite mappings.   RelationMapRead Waiting for a read of the relation map file.   RelationMapSync Waiting for the relation map file to reach durable storage.   RelationMapWrite Waiting for a write to the relation map file.   ReorderBufferRead Waiting for a read during reorder buffer management.   ReorderBufferWrite Waiting for a write during reorder buffer management.   ReorderLogicalMappingRead Waiting for a read of a logical mapping during reorder buffer management.   ReplicationSlotRead Waiting for a read from a replication slot control file.   ReplicationSlotRestoreSync Waiting for a replication slot control file to reach durable storage while restoring it to memory.   ReplicationSlotSync Waiting for a replication slot control file to reach durable storage.   ReplicationSlotWrite Waiting for a write to a replication slot control file.   SLRUFlushSync Waiting for SLRU data to reach durable storage during a checkpoint or database shutdown.   SLRURead Waiting for a read of an SLRU page.   SLRUSync Waiting for SLRU data to reach durable storage following a page write.   SLRUWrite Waiting for a write of an SLRU page.   SnapbuildRead Waiting for a read of a serialized historical catalog snapshot.   SnapbuildSync Waiting for a serialized historical catalog snapshot to reach durable storage.   SnapbuildWrite Waiting for a write of a serialized historical catalog snapshot.   TimelineHistoryFileSync Waiting for a timeline history file received via streaming replication to reach durable storage.   TimelineHistoryFileWrite Waiting for a write of a timeline history file received via streaming replication.   TimelineHistoryRead Waiting for a read of a timeline history file.   TimelineHistorySync Waiting for a newly created timeline history file to reach durable storage.   TimelineHistoryWrite Waiting for a write of a newly created timeline history file.   WALBootstrapSync Waiting for WAL to reach durable storage during bootstrapping.   WALBootstrapWrite Waiting for a write of a WAL page during bootstrapping.   WALCopyRead Waiting for a read when creating a new WAL segment by copying an existing one.   WALCopySync Waiting for a new WAL segment created by copying an existing one to reach durable storage.   WALCopyWrite Waiting for a write when creating a new WAL segment by copying an existing one.   WALInitSync Waiting for a newly initialized WAL file to reach durable storage.   WALInitWrite Waiting for a write while initializing a new WAL file.   WALRead Waiting for a read from a WAL file.   WALSenderTimelineHistoryRead Waiting for a read from a timeline history file during a walsender timeline command.   WALSync Waiting for a WAL file to reach durable storage.   WALSyncMethodAssign Waiting for data to reach durable storage while assigning a new WAL sync method.   WALWrite Waiting for a write to a WAL file.   LogicalChangesRead Waiting for a read from a logical changes file.   LogicalChangesWrite Waiting for a write to a logical changes file.   LogicalSubxactRead Waiting for a read from a logical subxact file.   LogicalSubxactWrite Waiting for a write to a logical subxact file.    Table 10. Wait Events of Type IPC\n   IPC Wait Event Description     AppendReady Waiting for subplan nodes of an Append plan node to be ready.   BackendTermination Waiting for the termination of another backend.   BackupWaitWalArchive Waiting for WAL files required for a backup to be successfully archived.   BgWorkerShutdown Waiting for background worker to shut down.   BgWorkerStartup Waiting for background worker to start up.   BtreePage Waiting for the page number needed to continue a parallel B-tree scan to become available.   BufferIO Waiting for buffer I/O to complete.   CheckpointDone Waiting for a checkpoint to complete.   CheckpointStart Waiting for a checkpoint to start.   ExecuteGather Waiting for activity from a child process while executing a Gather plan node.   LogicalSyncData Waiting for a logical replication remote server to send data for initial table synchronization.   LogicalSyncStateChange Waiting for a logical replication remote server to change state.   MessageQueueInternal Waiting for another process to be attached to a shared message queue.   MessageQueuePutMessage Waiting to write a protocol message to a shared message queue.   MessageQueueReceive Waiting to receive bytes from a shared message queue.   MessageQueueSend Waiting to send bytes to a shared message queue.   ParallelBitmapScan Waiting for parallel bitmap scan to become initialized.   ParallelCreateIndexScan Waiting for parallel CREATE INDEX workers to finish heap scan.   ParallelFinish Waiting for parallel workers to finish computing.   ProcArrayGroupUpdate Waiting for the group leader to clear the transaction ID at end of a parallel operation.   ProcSignalBarrier Waiting for a barrier event to be processed by all backends.   Promote Waiting for standby promotion.   RecoveryPause Waiting for recovery to be resumed.   ReplicationOriginDrop Waiting for a replication origin to become inactive so it can be dropped.   ReplicationSlotDrop Waiting for a replication slot to become inactive so it can be dropped.   SafeSnapshot Waiting to obtain a valid snapshot for a READ ONLY DEFERRABLE transaction.   SyncRep Waiting for confirmation from a remote server during synchronous replication.   WalReceiverExit Waiting for the WAL receiver to exit.   WalReceiverWaitStart Waiting for startup process to send initial data for streaming replication.   XactGroupUpdate Waiting for the group leader to update transaction status at end of a parallel operation.    Table 11. Wait Events of Type Lock\n   Lock Wait Event Description     advisory Waiting to acquire an advisory user lock.   extend Waiting to extend a relation.   object Waiting to acquire a lock on a non-relation database object.   page Waiting to acquire a lock on a page of a relation.   relation Waiting to acquire a lock on a relation.   transactionid Waiting for a transaction to finish.   tuple Waiting to acquire a lock on a tuple.   userlock Waiting to acquire a user lock.   virtualxid Waiting to acquire a virtual transaction ID lock.    Table 12. Wait Events of Type LWLock\n   LWLock Wait Event Description     AddinShmemInit Waiting to manage an extension\u0026rsquo;s space allocation in shared memory.   AutoFile Waiting to update the pgsql.auto.conf file.   Autoanalyze Waiting to read or update the current state of autoanalyze workers.   AutoanalyzeSchedule Waiting to ensure that a table selected for autoanalyze still needs analyzing.   BackgroundWorker Waiting to read or update background worker state.   BufferContent Waiting to access a data page in memory.   BufferIO Waiting to read or write a data page in disk.   BufferMapping Waiting to associate a data block with a buffer in the buffer pool.   CheckpointerComm Waiting to manage fsync requests.   ControlFile Waiting to read or update the pg_control file or create a new WAL file.   DatabaseState Waiting to update state of databases.   DynamicSharedMemoryControl Waiting to read or update dynamic shared memory allocation information.   LockFastPath Waiting to read or update a process\u0026rsquo; fast-path lock information.   LockManager Waiting to read or update information about heavyweight locks.   LogicalRepWorker Waiting to read or update the state of logical replication workers.   NotifyBuffer Waiting for I/O on a NOTIFY message SLRU buffer.   NotifyQueue Waiting to read or update NOTIFY messages.   NotifyQueueTail Waiting to update limit on NOTIFY message storage.   NotifySLRU Waiting to access the NOTIFY message SLRU cache.   OidGen Waiting to allocate a new OID.   OldSnapshotTimeMap Waiting to read or update old snapshot control information.   PLogSpace Waiting to update state of network tablespaces.   PLogWrite Waiting for PLOG buffers to be written to disk.   PredicateLockManager Waiting to access predicate lock information used by serializable transactions.   ProcArray Waiting to access the shared per-process data structures (typically, to get a snapshot or report a session\u0026rsquo;s transaction ID).   RelCacheInit Waiting to read or update a pg_internal.init relation cache initialization file.   ReplicationOrigin Waiting to create, drop or use a replication origin.   ReplicationOriginState Waiting to read or update the progress of one replication origin.   ReplicationSlotAllocation Waiting to allocate or free a replication slot.   ReplicationSlotControl Waiting to read or update replication slot state.   ReplicationSlotIO Waiting for I/O on a replication slot.   SerializableFinishedList Waiting to access the list of finished serializable transactions.   SerializablePredicateList Waiting to access the list of predicate locks held by serializable transactions.   SerializableXactHash Waiting to read or update information about serializable transactions.   ShmemIndex Waiting to find or allocate space in shared memory.   SInvalRead Waiting to retrieve messages from the shared catalog invalidation queue.   SInvalWrite Waiting to add a message to the shared catalog invalidation queue.   SyncRep Waiting to read or update information about the state of synchronous replication.   SyncScan Waiting to select the starting location of a synchronized table scan.   TablespaceMap Waiting to create or drop a tablespace.   TwoPhaseState Waiting to read or update the state of prepared transactions.   WALBufMapping Waiting to replace a page in WAL buffers.   WALInsert Waiting to insert WAL data into a memory buffer.   WALWrite Waiting for WAL buffers to be written to disk.     Extensions can add LWLock types to the list shown in Table 12. In some cases, the name assigned by an extension will not be available in all server processes; so an LWLock wait event might be reported as just extension rather than the extension-assigned name.  Table 13. Wait Events of Type Timeout\n   Timeout Wait Event Description     BaseBackupThrottle Waiting during base backup when throttling activity.   CheckpointWriteDelay Waiting between writes while performing a checkpoint.   PgSleep Waiting due to a call to pg_sleep or a sibling function.   RecoveryApplyDelay Waiting to apply WAL during recovery because of a delay setting.   RecoveryRetrieveRetryInterval Waiting during recovery when WAL data is not available from any source (pg_wal, archive or stream).   RegisterSyncRequest Waiting while sending synchronization requests to the checkpointer, because the request queue is full.    Here is an example of how wait events can be viewed:\nSELECT pid, wait_event_type, wait_event FROM pg_stat_activity WHERE wait_event is NOT NULL;\rpid | wait_event_type | wait_event ------+-----------------+------------\r2540 | Lock | relation\r6644 | LWLock | ProcArray\r(2 rows) pg_stat_wait_event     The pg_stat_wait_event view will contain one row for each wait event in the current database, showing statistics about wait information of every occured wait events.\nTable 14. pg_stat_wait_event View\n   Column Type Description     wait_event_type text The type of event for which the backend is waiting, if any; otherwise NULL. See Table 4 for details.   wait_event text Wait event name if backend is currently waiting, otherwise NULL. See Table 4 ~ 13 for details.   waits bigint Number of times this event has occured   total_time double precision Total time spent in the event, in milliseconds   min_time double precision Minimum time spent in the event, in milliseconds   max_time double precision Maximum time spent in the event, in milliseconds   mean_time double precision Mean time spent in the event, in milliseconds    pg_stat_replication     The pg_stat_replication view will contain one row per WAL sender process, showing statistics about replication to that sender\u0026rsquo;s connected standby server. Only directly connected standbys are listed; no information is available about downstream standby servers.\nTable 15. pg_stat_replication View\n   Column Type Description     pid integer Process ID of a WAL sender process   usesysid oid OID of the user logged into this WAL sender process   usename name Name of the user logged into this WAL sender process   application_name text Name of the application that is connected to this WAL sender   client_addr inet IP address of the client connected to this WAL sender. If this field is null, it indicates that the client is connected via a Unix socket on the server machine.   client_hostname text Host name of the connected client, as reported by a reverse DNS lookup of client_addr. This field will only be non-null for IP connections, and only when log_hostname is enabled.   client_port integer TCP port number that the client is using for communication with this WAL sender, or -1 if a Unix socket is used   backend_start timestamp with time zone Time when this process was started, i.e., when the client connected to this WAL sender   backend_mintime logicaltime This standby\u0026rsquo;s mintime horizon reported by hot_standby_feedback.   state text Current WAL sender state. Possible values are:startup: This WAL sender is starting up.catchup: This WAL sender\u0026rsquo;s connected standby is catching up with the primary.streaming: This WAL sender is streaming changes after its connected standby server has caught up with the primary.backup: This WAL sender is sending a backup.stopping: This WAL sender is stopping.   sent_lsn pg_lsn Last write-ahead log location sent on this connection   write_lsn pg_lsn Last write-ahead log location written to disk by this standby server   flush_lsn pg_lsn Last write-ahead log location flushed to disk by this standby server   replay_lsn pg_lsn Last write-ahead log location replayed into the database on this standby server   write_lag interval Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written it (but not yet flushed it or applied it). This can be used to gauge the delay that synchronous_commit level remote_write incurred while committing if this server was configured as a synchronous standby.   flush_lag interval Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written and flushed it (but not yet applied it). This can be used to gauge the delay that synchronous_commit level on incurred while committing if this server was configured as a synchronous standby.   replay_lag interval Time elapsed between flushing recent WAL locally and receiving notification that this standby server has written, flushed and applied it. This can be used to gauge the delay that synchronous_commit level remote_apply incurred while committing if this server was configured as a synchronous standby.   sync_priority integer Priority of this standby server for being chosen as the synchronous standby in a priority-based synchronous replication. This has no effect in a quorum-based synchronous replication.   sync_state text Synchronous state of this standby server. Possible values are:async: This standby server is asynchronous.potential: This standby server is now asynchronous, but can potentially become synchronous if one of current synchronous ones fails.sync: This standby server is synchronous.quorum: This standby server is considered as a candidate for quorum standbys.   reply_time timestamp with time zone Send time of last reply message received from standby server    The lag times reported in the pg_stat_replication view are measurements of the time taken for recent WAL to be written, flushed and replayed and for the sender to know about it. These times represent the commit delay that was (or would have been) introduced by each synchronous commit level, if the remote server was configured as a synchronous standby. For an asynchronous standby, the replay_lag column approximates the delay before recent transactions became visible to queries. If the standby server has entirely caught up with the sending server and there is no more WAL activity, the most recently measured lag times will continue to be displayed for a short time and then show NULL.\nLag times work automatically for physical replication. Logical decoding plugins may optionally emit tracking messages; if they do not, the tracking mechanism will simply display NULL lag.\n The reported lag times are not predictions of how long it will take for the standby to catch up with the sending server assuming the current rate of replay. Such a system would show similar times while new WAL is being generated, but would differ when the sender becomes idle. In particular, when the standby has caught up completely, pg_stat_replication shows the time taken to write, flush and replay the most recent reported WAL location rather than zero as some users might expect. This is consistent with the goal of measuring synchronous commit and transaction visibility delays for recent write transactions. To reduce confusion for users expecting a different model of lag, the lag columns revert to NULL after a short time on a fully replayed idle system. Monitoring systems should choose whether to represent this as missing data, zero or continue to display the last known value.  pg_stat_database     The pg_stat_database view will contain one row for each database in the cluster, plus one for the shared objects, showing database-wide statistics.\nTable 16. pg_stat_database View\n   Column Type Description     datid oid OID of this database, or 0 for objects belonging to a shared relation   datname name Name of this database, or NULL for the shared objects.   numbackends integer Number of backends currently connected to this database, or NULL for the shared objects. This is the only column in this view that returns a value reflecting current state; all other columns return the accumulated values since the last reset.   xact_commit bigint Number of transactions in this database that have been committed   xact_rollback bigint Number of transactions in this database that have been rolled back   blks_read bigint Number of disk blocks read in this database   blks_cloned bigint Number of data buffers cloned in this database   blks_hit bigint Number of times disk blocks were found already in the buffer cache, so that a read was not necessary (this only includes hits in the PostgreSQL buffer cache, not the operating system\u0026rsquo;s file system cache)   tup_returned bigint Number of rows returned by queries in this database   tup_fetched bigint Number of rows fetched by queries in this database   tup_inserted bigint Number of rows inserted by queries in this database   tup_updated bigint Number of rows updated by queries in this database   tup_deleted bigint Number of rows deleted by queries in this database   conflicts bigint Number of queries canceled due to conflicts with recovery in this database. (Conflicts occur only on standby servers; see pg_stat_database_conflicts for details.)   temp_files bigint Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.   temp_bytes bigint Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting.   deadlocks bigint Number of deadlocks detected in this database   checksum_failures bigint Number of data page checksum failures detected in this database (or on a shared object), or NULL if data checksums are not enabled.   checksum_last_failure timestamp with time zone Time at which the last data page checksum failure was detected in this database (or on a shared object), or NULL if data checksums are not enabled.   blk_read_time double precision Time spent reading data file blocks by backends in this database, in milliseconds   blk_clone_time double precision Time spent cloning data buffers by backends in this database, in milliseconds   blk_write_time double precision Time spent writing data file blocks by backends in this database, in milliseconds   stats_reset timestamp with time zone Time at which these statistics were last reset    pg_stat_all_tables     The pg_stat_all_tables view will contain one row for each table in the current database (including TOAST tables), showing statistics about accesses to that specific table. The pg_stat_user_tables and pg_stat_sys_tables views contain the same information, but filtered to only show user and system tables respectively.\nTable 17. pg_stat_all_tables View\n   Column Type Description     relid oid OID of a table   schemaname name Name of the schema that this table is in   relname name Name of this table   seq_scan bigint Number of sequential scans initiated on this table   seq_tup_read bigint Number of live rows fetched by sequential scans   idx_scan bigint Number of index scans initiated on this table   idx_tup_fetch bigint Number of live rows fetched by index scans   n_tup_ins bigint Number of rows inserted   n_tup_upd bigint Number of rows updated (includes HOT updated rows)   n_tup_del bigint Number of rows deleted   n_tup_hot_upd bigint Number of rows HOT updated (i.e., with no separate index update required)   n_live_tup bigint Estimated number of live rows   n_dead_tup bigint Estimated number of dead rows   n_mod_since_analyze bigint Estimated number of rows modified since this table was last analyzed   last_analyze timestamp with time zone Last time at which this table was manually analyzed   last_autoanalyze timestamp with time zone Last time at which this table was analyzed by the autoanalyze daemon   analyze_count bigint Number of times this table has been manually analyzed   autoanalyze_count bigint Number of times this table has been analyzed by the autoanalyze daemon    pg_stat_all_undos     The pg_stat_all_undos view will contain one row for each undo in the current database, showing statistics about accesses to that specific undo.\nTable 18. pg_stat_all_undos View\n   Column Type Description     relid oid OID of this undo   schemaname name Name of the schema that this undo is in   relname name Name of this undo   n_tup_fetch bigint Number of records fetched   n_tup_ins bigint Number of records inserted   n_tup_upd bigint Number of records updated   n_tup_del bigint Number of records deleted   n_tup_hot_upd bigint Number of records HOT updated   n_live_tup bigint Estimated number of live records   n_dead_tup bigint Estimated number of dead records    pg_stat_all_indexes     The pg_stat_all_indexes view will contain one row for each index in the current database, showing statistics about accesses to that specific index. The pg_stat_user_indexes and pg_stat_sys_indexes views contain the same information, but filtered to only show user and system indexes respectively.\nTable 19. pg_stat_all_indexes View\n   Column Type Description     relid oid OID of the table for this index   indexrelid oid OID of this index   schemaname name Name of the schema this index is in   relname name Name of the table for this index   indexrelname name Name of this index   idx_scan bigint Number of index scans initiated on this index   idx_tup_read bigint Number of index entries returned by scans on this index   idx_tup_fetch bigint Number of live table rows fetched by simple index scans using this index    Indexes can be used by simple index scans, bitmap index scans, and the optimizer. In a bitmap scan the output of several indexes can be combined via AND or OR rules, so it is difficult to associate individual heap row fetches with specific indexes when a bitmap scan is used. Therefore, a bitmap scan increments the pg_stat_all_indexes.idx_tup_read count(s) for the index(es) it uses, and it increments the pg_stat_all_tables.idx_tup_fetch count for the table, but it does not affect pg_stat_all_indexes.idx_tup_fetch. The optimizer also accesses indexes to check for supplied constants whose values are outside the recorded range of the optimizer statistics because the optimizer statistics might be stale.\n The idx_tup_read and idx_tup_fetch counts can be different even without any use of bitmap scans, because idx_tup_read counts index entries retrieved from the index while idx_tup_fetch counts live rows fetched from the table. The latter will be less if any dead or not-yet-committed rows are fetched using the index, or if any heap fetches are avoided by means of an index-only scan.  pg_statio_all_tables     The pg_statio_all_tables view will contain one row for each table in the current database (including TOAST tables), showing statistics about I/O on that specific table. The pg_statio_user_tables and pg_statio_sys_tables views contain the same information, but filtered to only show user and system tables respectively.\nTable 20. pg_statio_all_tables View\n   Column Type Description     relid oid OID of a table   schemaname name Name of the schema that this table is in   relname name Name of this table   heap_blks_read bigint Number of disk blocks read from this table   heap_blks_cloned bigint Number of data buffers cloned from this table   heap_blks_hit bigint Number of buffer hits in this table   idx_blks_read bigint Number of disk blocks read from all indexes on this table   idx_blks_cloned bigint Number of data buffers cloned from all indexes on this table   idx_blks_hit bigint Number of buffer hits in all indexes on this table   toast_blks_read bigint Number of disk blocks read from this table\u0026rsquo;s TOAST table (if any)   toast_blks_cloned bigint Number of data buffers cloned from this table\u0026rsquo;s TOAST table (if any)   toast_blks_hit bigint Number of buffer hits in this table\u0026rsquo;s TOAST table (if any)   tidx_blks_read bigint Number of disk blocks read from this table\u0026rsquo;s TOAST table indexes (if any)   tidx_blks_cloned bigint Number of data buffers cloned from this table\u0026rsquo;s TOAST table indexes (if any)   tidx_blks_hit bigint Number of buffer hits in this table\u0026rsquo;s TOAST table indexes (if any)    pg_statio_all_undos     The pg_statio_all_undos view will contain one row for each undo in the current database, showing statistics about I/O on that specific undo.\nTable 21. pg_statio_all_undos View\n   Column Type Description     relid oid OID of the table for this index   schemaname name Name of the schema this undois in   relname name Name of the table for this undo   undo_blks_read bigint Number of disk blocks read from this undo   undo_blks_cloned bigint Number of data buffers cloned from this undo   undo_blks_hit bigint Number of buffer hits in this undo    pg_statio_all_indexes     The pg_statio_all_indexes view will contain one row for each index in the current database, showing statistics about I/O on that specific index. The pg_statio_user_indexes and pg_statio_sys_indexes views contain the same information, but filtered to only show user and system indexes respectively.\nTable 22. pg_statio_all_indexes View\n   Column Type Description     relid oid OID of the table for this index   indexrelid oid OID of this index   schemaname name Name of the schema this index is in   relname name Name of the table for this index   indexrelname name Name of this index   idx_blks_read bigint Number of disk blocks read from this index   idx_blks_cloned bigint Number of data buffers cloned from this index   idx_blks_hit bigint Number of buffer hits in this index    pg_statio_all_sequences     The pg_statio_all_sequences view will contain one row for each sequence in the current database, showing statistics about I/O on that specific sequence.\nTable 23. pg_statio_all_sequences View\n   Column Type Description     relid oid OID of a sequence   schemaname name Name of the schema this sequence is in   relname name Name of this sequence   blks_read bigint Number of disk blocks read from this sequence   blks_cloned bigint Number of data buffers cloned from this sequence   blks_hit bigint Number of buffer hits in this sequence    "},{"id":12,"href":"/features/undo/","title":"Undo Segments","parent":"Features","content":"   Undo Segments and Transactions Transaction Rollback Manage Undo Segments      Redrock Postgres maintains records of the actions of transactions, collectively known as undo data. Redrock Postgres uses undo data to do the following:\n Roll back an active transaction Recover a terminated transaction Provide read consistency  Redrock Postgres stores undo data inside the database rather than in external logs. Undo data is stored in blocks that are updated just like data blocks, with changes to these blocks generating redo. In this way, Redrock Postgres can efficiently access undo data without needing to read external logs.\nUndo data is stored in an independent tablespace. Redrock Postgres provides a fully automated mechanism, known as automatic undo management mode, for managing undos and space.\nUndo Segments and Transactions     When a transaction starts, the database binds (assigns) the transaction to an undo segment, and therefore to a transaction table.\nMultiple active transactions can write concurrently to the same undo or to different undo segments. For example, transactions T1 and T2 can both write to undo segment U1, or T1 can write to U1 while T2 writes to undo segment U2.\nTransaction Rollback     When a ROLLBACK statement is issued, the database uses undo records to roll back changes made to the database by the uncommitted transaction. During recovery, the database rolls back any uncommitted changes applied from the online redo log to the data files. Undo records provide read consistency by maintaining the before image of the data for users accessing data at the same time that another user is changing it.\nManage Undo Segments     After the database cluster data directory is initialized, 4 cluster-level undo segments and 8 database-level undo segments are generated by default in the database. Users can use the following statement to view the undo segment information in the current database:\nSELECT c.relname, c.relisshared, u.undoid, u.undrelid, pg_size_pretty(pg_relation_size(c.oid)) AS undsize FROM pg_undo u LEFT JOIN pg_class c ON u.undrelid = c.oid; Result:\n relname | relisshared | undoid | undrelid | undsize ------------+-------------+--------+----------+--------- pg_undo_1 | t | 1 | 4141 | 1096 kB pg_undo_2 | t | 2 | 4142 | 1040 kB pg_undo_3 | t | 3 | 4143 | 1040 kB pg_undo_4 | t | 4 | 4144 | 1040 kB pg_undo_5 | f | 5 | 11528 | 1040 kB pg_undo_6 | f | 6 | 11530 | 1040 kB pg_undo_7 | f | 7 | 11532 | 1096 kB pg_undo_8 | f | 8 | 11534 | 1040 kB pg_undo_9 | f | 9 | 11536 | 1040 kB pg_undo_10 | f | 10 | 11538 | 1040 kB pg_undo_11 | f | 11 | 11540 | 1040 kB pg_undo_12 | f | 12 | 11542 | 1040 kB "},{"id":13,"href":"/admin/","title":"Server Administration","parent":"Redrock Documentation","content":"This part covers topics that are of interest to a Redrock Postgres database administrator.\nTable of Contents\n   Configuration      Statistics Views      Network Attached Tablespace      Subscribe DDL Commands      Audit Logging      Online Migration      "},{"id":14,"href":"/develop/search-engine/","title":"Search Engine","parent":"Developer Guide","content":"   Create Content Table Create Content Index Table Store Web Page Data Search Web Content      PostgreSQL provides the full text search capability. Through full text search, you can build a text content search engine. This section describes how to quickly set up a search engine for website pages in PostgreSQL.\nCreate Content Table     Create a web page content table to store the content data of web pages.\nCREATE TABLE IF NOT EXISTS site_page ( id serial PRIMARY KEY, url text, image text, title text, content text ); The site_page table contains the following columns:\n   Name Description     id Identifier of the web page.   url URL of the web page.   image URL of a web page image.   title Title of the web page.   content The body of the web page.    Create Content Index Table     Create a web page content index table to store the index data of the web page content:\nCREATE TABLE IF NOT EXISTS site_page_tsi ( id integer, title tsvector, content tsvector ); CREATE INDEX site_page_title_idx ON site_page_tsi USING GIN (title); CREATE INDEX site_page_content_idx ON site_page_tsi USING GIN (content); The site_page_tsi table contains the following columns:\n   Name Description     id Identifier of the web page.   title Lexemes list of the page title.   content Lexemes list of the page body.     Of course, you can also combine the tables site_page and site_page_tsi into one table. Here, we recommend that you store webpage content data and content index data separately, which can improve the query performance of webpage content data and facilitate the maintenance and cleaning of index data separately.\nBy storing the content index data into a single table, we can view the storage space usage of this part of the data. In addition, we can also optimize and adjust the content-related lexemes list when adding web page data, and remove those words that we are not interested in. This can not only save storage space, but also improve the efficiency of full text search.\n  Store Web Page Data     You can add a page data record to the site_page table and add a lexemes index record to the site_page_tsi table.\nBEGIN; INSERT INTO site_page (url, image, title, content) VALUES (\u0026#39;https://doc.rockdata.net/\u0026#39;, \u0026#39;https://doc.rockdata.net/brand.svg\u0026#39;, \u0026#39;Redrock Postgres Documentation\u0026#39;, \u0026#39;Redrock is an object-relational database management system based on PostgreSQL, It is intentionally designed as an enterprise grade and cloud native database.\u0026#39;) RETURNING id; id ----  1 INSERT INTO site_page_tsi (id, title, content) VALUES (1, to_tsvector(\u0026#39;english\u0026#39;, \u0026#39;Redrock Postgres Documentation\u0026#39;), to_tsvector(\u0026#39;english\u0026#39;, \u0026#39;Redrock is an object-relational database management system based on PostgreSQL, It is intentionally designed as an enterprise grade and cloud native database.\u0026#39;)); COMMIT; In the above example, we add a piece of webpage content data to the table site_page, get the identifier of the webpage content, and then generate the lexemes list of the webpage content using the function to_tsvector, specify the corresponding webpage identifier, and insert it into the content Index table site_page_tsi.\nSearch Web Content     Of course, we can search for matching web content based on search keywords. For example, we can execute the following query to search for web page content whose title matches the keyword \u0026ldquo;Redrock Documentation\u0026rdquo;:\nSELECT sp.id, sp.url, sp.title FROM site_page AS sp LEFT JOIN site_page_tsi AS tsi ON sp.id = tsi.id WHERE tsi.title @@ plainto_tsquery(\u0026#39;english\u0026#39;, \u0026#39;Redrock Documentation\u0026#39;); Result:\n id | url | title\r----+---------------------------+--------------------------------\r1 | https://doc.rockdata.net/ | Redrock Postgres Documentation The above example is very simple. Generally, when you specify keywords to search web page content, many matching web page data may be returned. In this case, you may consider specifying LIMIT \u0026hellip; OFFSET \u0026hellip; to display the returned result set in pagination.\n"},{"id":15,"href":"/develop/plscheme/","title":"PL/Scheme","parent":"Developer Guide","content":"   Functions and Arguments Data Values  Data Type Mapping NULL Arrays, Vectors   Standard Modules      PL/Scheme is a PostgreSQL procedural language handler for Scheme programming language. PL/Scheme uses Chibi Scheme in the background as its Scheme interpreter. With lots of builtin SRFIs and complete R7RS compliancy of Chibi Scheme, PL/Scheme can power up PostgreSQL procedures in a Lisp style.\nFunctions and Arguments     To create a function in the PL/Scheme language, use the standard CREATE FUNCTION syntax:\nCREATE FUNCTION funcname (argument-types) RETURNS return-type -- function attributes can go here AS $$ #;\u0026#34;PL/Scheme function body goes here\u0026#34; $$ LANGUAGE plscheme; The body of the function is ordinary Scheme code. In fact, the PL/Scheme glue code wraps it inside a Scheme subroutine. A PL/Scheme function is called in a scalar context, so it can\u0026rsquo;t return a list. You can return non-scalar values (arrays, records, and sets) by returning a reference, as discussed below.\nIn a PL/Scheme procedure, any return value from the Scheme code is ignored.\nPL/Scheme also supports anonymous code blocks called with the DO statement:\nDO $$ #;\u0026#34;PL/Scheme code\u0026#34; $$ LANGUAGE plscheme; An anonymous code block receives no arguments, and whatever value it might return is discarded. Otherwise it behaves just like a function.\nThe syntax of the CREATE FUNCTION command requires the function body to be written as a string constant. It is usually most convenient to use dollar quoting (see Section 4.1.2.4) for the string constant. If you choose to use escape string syntax E'', you must double any single quote marks (') and backslashes (\\) used in the body of the function (see Section 4.1.2.1).\nArguments and results are handled as in any other Scheme subroutine: arguments are passed as Scheme variables, and a result value is returned as the last expression evaluated in the function.\nFor example, a function returning the greater of two integer values could be defined as:\nCREATE FUNCTION scheme_max (a integer, b integer) RETURNS integer AS $$ (if (\u0026gt; a b) a b) $$ LANGUAGE plscheme; Data Values     Generally speaking, the aim of PL/Scheme is to provide a natural mapping between the PostgreSQL and the Scheme worlds. This informs the data mapping rules described below.\nData Type Mapping     When a PL/Scheme function is called, its arguments are converted from their PostgreSQL data type to a corresponding Scheme type:\n PostgreSQL boolean is converted to Scheme boolean. PostgreSQL smallint and int are converted to Scheme fixnum. PostgreSQL bigint and oid are converted to fixnum in Scheme. PostgreSQL real and double are converted to Scheme flonum. PostgreSQL numeric is converted to Scheme flonum. PostgreSQL bytea is converted to Scheme bytevector. All other data types, including the PostgreSQL character string types, are converted to a Scheme string. For nonscalar data types, see below.  When a PL/Scheme function returns, its return value is converted to the function\u0026rsquo;s declared PostgreSQL return data type as follows:\n When the PostgreSQL return type is boolean, the return value will be evaluated for truth according to the Scheme rules. When the PostgreSQL return type is bytea, the return value will be converted to bytes using the respective Scheme built-ins, with the result being converted to bytea. For all other PostgreSQL return types, the return value is converted to a string using the Scheme built-in string, and the result is passed to the input function of the PostgreSQL data type. For nonscalar data types, see below.  Note that logical mismatches between the declared PostgreSQL return type and the Scheme data type of the actual return object are not flagged; the value will be converted in any case.\nNULL     If an SQL null value is passed to a function, the argument value will appear as #() in Scheme. For example, the function definition of scheme_max shown in Functions and Arguments will return the wrong answer for null inputs. We could add STRICT to the function definition to make PostgreSQL do something more reasonable: if a null value is passed, the function will not be called at all, but will just return a null result automatically. Alternatively, we could check for null inputs in the function body:\nCREATE FUNCTION scheme_max (a integer, b integer) RETURNS integer AS $$ (if (or (null? a) (null? b)) \u0026#39;#() (if (\u0026gt; a b) a b)) $$ LANGUAGE plscheme; As shown above, to return an SQL null value from a PL/Scheme function, return the value #(). This can be done whether the function is strict or not.\nArrays, Vectors     SQL array values are passed into PL/Scheme as a Scheme vector. To return an SQL array value out of a PL/Scheme function, return a Scheme vector:\nCREATE OR REPLACE FUNCTION return_arr() RETURNS int[] AS $$ (vector 1 2 \u0026#39;() 3) $$ LANGUAGE plscheme; SELECT return_arr(); return_arr --------------------  [0:3]={1,2,NULL,3} (1 row) Standard Modules     We can import standard modules provided by Scheme in PL/Scheme functions, for example:\nDO $$ (import (scheme small) (srfi 1)) $$ LANGUAGE plscheme; A number of SRFIs are provided in Chibi Scheme. Note that SRFIs 0, 6, 23, 46 and 62 are built into the default environment so there\u0026rsquo;s no need to import them. This list includes popular SRFIs or SRFIs used in standard Chibi modules:\n (srfi 0) - cond-expand: feature-based conditional expansion construct (srfi 1) - list library (srfi 2) - and-let* (srfi 6) - basic string ports (srfi 8) - receive: binding to multiple values (srfi 9) - define-record-type: defining record types (srfi 11) - let-values/let*-values: syntax for receiving multiple values (srfi 14) - character-set library (srfi 16) - case-lambda: syntax for procedures with a variable number of arguments (srfi 18) - multi-threading support (srfi 23) - error reporting mechanism (srfi 26) - cut/cute partial application (srfi 27) - sources of random bits (srfi 33) - bitwise operators (srfi 38) - read/write shared structures (srfi 39) - parameter objects (srfi 41) - streams (srfi 46) - basic syntax-rules extensions (srfi 55) - require-extension (srfi 62) - s-expression comments (srfi 69) - basic hash tables (srfi 95) - sorting and merging (srfi 98) - environment access (srfi 99) - ERR5RS records (srfi 101) - purely functional random-access pairs and lists (srfi 111) - boxes (srfi 113) - sets and bags (srfi 115) - Scheme regular expressions (srfi 116) - immutable list library (srfi 117) - mutable queues (srfi 121) - generators (srfi 124) - ephemerons (srfi 125) - intermediate hash tables (srfi 127) - lazy sequences (srfi 128) - comparators (reduced) (srfi 129) - titlecase procedures (srfi 130) - cursor-based string library (srfi 132) - sort libraries (srfi 133) - vector library (srfi 134) - immutable deques (srfi 135) - immutable texts (srfi 139) - syntax parameters (srfi 141) - integer division (srfi 142) - bitwise operations (srfi 143) - fixnums (srfi 144) - flonums (srfi 145) - assumptions (srfi 147) - custom macro transformers (srfi 151) - bitwise operators (srfi 154) - first-class dynamic extents (srfi 158) - generators and accumulators (srfi 160) - homogeneous numeric vector libraries (srfi 165) - the environment Monad (srfi 166) - monadic formatting (srfi 188) - splicing binding constructs for syntactic keywords  Additional non-standard modules are put in the (chibi) module namespace.\n (chibi app) - Unified option parsing and config (chibi ast) - Abstract Syntax Tree and other internal data types (chibi base64) - Base64 encoding and decoding (chibi bytevector) - Bytevector Utilities (chibi config) - General configuration management (chibi crypto md5) - MD5 hash (chibi crypto rsa) - RSA public key encryption (chibi crypto sha2) - SHA-2 hash (chibi diff) - LCS Algorithm and diff utilities (chibi disasm) - Disassembler for the virtual machine (chibi doc) - Chibi documentation utilities (chibi edit-distance) - A levenshtein distance implementation (chibi equiv) - A version of equal? which is guaranteed to terminate (chibi filesystem) - Interface to the filesystem and file descriptor objects (chibi generic) - Generic methods for CLOS-style object oriented programming (chibi heap-stats) - Utilities for gathering statistics on the heap (chibi io) - Various I/O extensions and custom ports (chibi iset base) - Compact integer sets (chibi iset constructors) - Compact integer set construction (chibi iset iterators) - Iterating over compact integer sets (chibi json) - JSON reading and writing (chibi loop) - Fast and extensible loop syntax (chibi match) - Intuitive and widely supported pattern matching syntax (chibi math prime) - Prime number utilities (chibi memoize) - Procedure memoization (chibi mime) - Parse MIME files into SXML (chibi modules) - Introspection for the module system itself (chibi net) - Simple networking interface (chibi net http-server) - Simple http-server with servlet support (chibi net servlet) - HTTP servlets for http-server or CGI (chibi parse) - Parser combinators with convenient syntax (chibi pathname) - Utilities to decompose and manipulate pathnames (chibi process) - Interface to spawn processes and handle signals (chibi repl) - A full-featured Read/Eval/Print Loop (chibi scribble) - A parser for the scribble syntax used to write this manual (chibi string) - Cursor-based string library (predecessor to SRFI 130) (chibi stty) - A high-level interface to ioctl (chibi sxml) - SXML utilities (chibi system) - Access to the host system and current user information (chibi temp-file) - Temporary file and directory creation (chibi test) - A simple unit testing framework (chibi time) - An interface to the current system time (chibi trace) - A utility to trace procedure calls (chibi type-inference) - An easy-to-use type inference system (chibi uri) - Utilities to parse and construct URIs (chibi weak) - Data structures with weak references  "},{"id":16,"href":"/features/tablespace/","title":"Tablespaces","parent":"Features","content":"   Local Tablespace Network Attached Tablespace Using Tablespaces Managing Tablespaces      Tablespaces in Redrock Postgres allow database administrators to define locations where the files representing database objects can be stored. Once created, a tablespace can be referred to by name when creating database objects.\nBy using tablespaces, an administrator can control the storage layout of a Redrock Postgres installation. This is useful in at least two ways. First, if the partition or volume on which the cluster was initialized runs out of space and cannot be extended, a tablespace can be created on a different partition and used until the system can be reconfigured.\nSecond, tablespaces allow an administrator to use knowledge of the usage pattern of database objects to optimize performance. For example, an index which is very heavily used can be placed on a very fast, highly available disk, such as an expensive solid state device. At the same time a table storing archived data which is rarely used or not performance critical could be stored on a less expensive, slower disk system.\n Even though located outside the main PostgreSQL data directory, tablespaces are an integral part of the database cluster and cannot be treated as an autonomous collection of data files. They are dependent on metadata contained in the main data directory, and therefore cannot be attached to a different database cluster or backed up individually. Similarly, if you lose a tablespace (file deletion, disk failure, etc), the database cluster might become unreadable or unable to start. Placing a tablespace on a temporary file system like a RAM disk risks the reliability of the entire cluster.  Local Tablespace     PostgreSQL requires symbolic links provided by the operating system to use local tablespaces. Redrock Postgres does not have this restriction. This means that you can use local tablespaces on any operating system. To define a local tablespace, use the CREATE TABLESPACE command, for example:\nCREATE TABLESPACE fastspace LOCATION \u0026#39;/ssd1/postgresql/data\u0026#39;; The directory must belong to the PostgreSQL operating system user. In addition, it is recommended that the directory be an existing empty directory. All objects subsequently created within the tablespace will be stored in files underneath this directory. The location must not be on removable or transient storage, as the cluster might fail to function if the tablespace is missing or lost.\n There is usually not much point in making more than one tablespace per logical file system, since you cannot control the location of individual files within a logical file system. However, PostgreSQL does not enforce any such limitation, and indeed it is not directly aware of the file system boundaries on your system. It just stores files in the directories you tell it to use.  Creation of the tablespace itself must be done as a database superuser, but after that you can allow ordinary database users to use it. To do that, grant them the CREATE privilege on it.\nNetwork Attached Tablespace     Redrock Postgres supports database servers running in storage server mode and can provide storage access services to other database servers connected to the network. We call these servers used to store and access data as database storage servers, and those servers that are mainly used for computing and processing data are called database computing servers. The computing server communicates with the storage server using the PostgreSQL frontend/backend protocol.\nWe can create a network tablespace on the storage server and then refer to it by tablespace name when creating database objects. If you have finished deploying the storage server, you can create a network tablespace on the storage server. To define a network attached tablespace, use the CREATE TABLESPACE command, for example:\nCREATE TABLESPACE netspace STORAGE remote LOCATION \u0026#39;host=192.168.1.50 port=5432 user=postgres password=pgpass\u0026#39;; All objects created in this tablespace are stored in files in the data directory of the storage server. During database service running, you need to ensure that the storage server is always accessible, as the cluster might fail to function if the tablespace is missing or lost.\nUsing Tablespaces     Tables, indexes, undos and entire databases can be assigned to particular tablespaces. To do so, a user with the CREATE privilege on a given tablespace must pass the tablespace name as a parameter to the relevant command. For example, the following creates a table in the tablespace space1:\nCREATE TABLE foo(i int) TABLESPACE space1; Alternatively, use the default_tablespace parameter:\nSET default_tablespace = space1; CREATE TABLE foo(i int); When default_tablespace is set to anything but an empty string, it supplies an implicit TABLESPACE clause for CREATE TABLE and CREATE INDEX commands that do not have an explicit one.\nThere is also a temp_tablespaces parameter, which determines the placement of temporary tables and indexes, as well as temporary files that are used for purposes such as sorting large data sets. This can be a list of tablespace names, rather than only one, so that the load associated with temporary objects can be spread over multiple tablespaces. A random member of the list is picked each time a temporary object is to be created.\nIn Redrock Postgres, when a database is created, a default tablespace is created in the database. The name of the tablespace is pg_default. This tablespace will be used to store the system catalogs of the database. Furthermore, it is the default tablespace used for tables, indexes, undos, and temporary files created within the database, if no TABLESPACE clause is given and no other selection is specified by default_tablespace or temp_tablespaces (as appropriate). If a database is created without specifying tablespace or location information, it will create a new tablespace with the same location information as its template database.\nWhen the database cluster is initialized, two tablespaces are automatically created for each database. The pg_global tablespace is used for cluster shared system catalogs, and the initial location of this tablespace is in the $PGDATA/global directory. The pg_default tablespace is used for database-level system catalogs and is initially located in the $PGDATA/base directory.\n In PostgreSQL, tablespaces are cluster-level objects, while in Redrock Postgres, tablespaces are database-level objects. This means that when you access different database objects in the same cluster, although you may see tablespaces with the same name or location information, in fact, they are different tablespace objects in their respective databases. When Redrock Postgres creates a tablespace, it will create a directory named after the database object identifier under the specified location, so that different databases can also have their own tablespaces under the same location.  Managing Tablespaces     Once created, a tablespace can be used by any user in the database, provided the requesting user has sufficient privilege. This means that a tablespace cannot be dropped until all objects using the tablespace have been removed.\nTo remove an empty tablespace, use the DROP TABLESPACE command.\nTo determine the set of existing tablespaces, examine the pg_tablespace system catalog, for example\nSELECT spcname FROM pg_tablespace; The psql program\u0026rsquo;s \\db meta-command is also useful for listing the existing tablespaces.\nThe $PGDATA/pg_tblspc directory stores the storage location configuration files of tablespaces in each database. These files are named in the format of [db_oid]_tsm. Although not recommended, it is possible to adjust the tablespace layout by manually modifying these storage location profiles. Under no circumstances perform this operation while the server is running.\n"},{"id":17,"href":"/features/recyclebin/","title":"Recycle Bin","parent":"Features","content":"   What Is the Recycle Bin?  Object Naming in the Recycle Bin   Viewing and Querying Objects in the Recycle Bin Purging Objects in the Recycle Bin Restoring Tables from the Recycle Bin      When you drop a table, the database does not immediately remove the space associated with the table. The database renames the table and places it and any associated objects in a recycle bin, where, in case the table was dropped in error, it can be recovered at a later time.\nWhat Is the Recycle Bin?     The recycle bin is actually a data dictionary table containing information about dropped objects. Dropped tables and any associated objects such as indexes, constraints, types, and so on are not removed and still occupy space.\nThey continue to count against user space quotas, until specifically purged from the recycle bin or the unlikely situation where they must be purged by the database because of tablespace space constraints.\nEach user can be thought of as having his own recycle bin, because, unless a user has the SUPERUSER privilege, the only objects that the user has access to in the recycle bin are those that the user owns. A user can view the objects in his schema in the recycle bin using the following statement:\nSELECT * FROM pg_catalog.pg_recyclebin WHERE namespace = to_regnamespace(CURRENT_SCHEMA)::oid; Objects dropped by the DDL command are moved to the recycle bin. Dropped objects that are moved to the recycle bin can include the following types of objects:\n Tables Undos Indexes Constraints Types Functions  When you drop a tablespace, the objects in the tablespace are not placed in the recycle bin and the database purges any entries in the recycle bin for objects located in the tablespace. Likewise:\n When you drop a user, any objects belonging to the user are not placed in the recycle bin and any objects in the recycle bin are purged. When you drop a schema, any objects belonging to the schema are not placed in the recycle bin and any objects in the recycle bin are purged.  Object Naming in the Recycle Bin     When a dropped table is moved to the recycle bin, the table and its associated objects are given system-generated names. This is necessary to avoid name conflicts that may arise if multiple tables have the same name. This could occur under the following circumstances:\n A user drops a table, re-creates it with the same name, then drops it again. Two users have tables with the same name, and both users drop their tables.  The renaming convention is as follows:\npg_dropped_[object_type]_[object_id] where:\n object_type is the object type name, eg: table, index, constraint, type, function. object_id is a unique identifier for this object, which makes the recycle bin name unique in current database.  Viewing and Querying Objects in the Recycle Bin     You can query system catalog pg_recyclebin to identify the name that the database has assigned to a dropped object, as shown in the following example:\nSELECT objname, oldname FROM pg_catalog.pg_recyclebin WHERE namespace = to_regnamespace(\u0026#39;hr\u0026#39;)::oid; objname | oldname -----------------------+----------- pg_dropped_table_18762 | employees You can also view the contents of the recycle bin using the GUI management tool like DBeaver, pgAdmin 4, expand the system schema pg_recyclebin under a specified database to view all the dropped objects.\nYou can query objects that are in the recycle bin, just as you can query other objects. However, you must specify the name of the object as it is identified in the recycle bin. For example:\nSELECT * FROM pg_recyclebin.pg_dropped_table_18762; Purging Objects in the Recycle Bin     If you decide that you are never going to restore an item from the recycle bin, then you can use the VACUUM statement to remove the items and their associated objects from the recycle bin and release their storage space. You need the same privileges as if you were dropping the item.\nWhen you use the VACUUM statement to purge a table, you can use the name that the table is known by in the recycle bin or the original name of the table. The recycle bin name can be obtained from system catalog pg_recyclebin as shown in \u0026ldquo;Viewing and Querying Objects in the Recycle Bin\u0026rdquo;. The following hypothetical example purges the table hr.employees, which was renamed to pg_recyclebin.pg_dropped_table_18762 when it was placed in the recycle bin:\nVACUUM pg_recyclebin.pg_dropped_table_18762; You can achieve the same result with the following statement:\nVACUUM hr.employees; If you have the SUPERUSER privilege or you are the current database owner, then you can purge the entire recycle bin, and release space for objects, by using the following statement:\nVACUUM; You can also use the VACUUM statement to purge an index or undo from the recycle bin.\nRestoring Tables from the Recycle Bin     Use the CREATE TABLE \u0026hellip; LIKE statement to recover objects from the recycle bin.\nYou should specify the name of the table in the recycle bin. The recycle bin name can be obtained from system catalog pg_recyclebin as shown in \u0026ldquo;Viewing and Querying Objects in the Recycle Bin\u0026rdquo;. To use the CREATE TABLE \u0026hellip; LIKE statement, you need the SELECT privileges required to access the dropped table.\nThe following example restores employees table and assigns to it a new name:\nCREATE TABLE hr.employees2 (LIKE pg_recyclebin.pg_dropped_table_18762 INCLUDING ALL); INSERT INTO hr.employees2 SELECT * FROM pg_recyclebin.pg_dropped_table_18762; The system-generated recycle bin name is very useful if you have dropped a table multiple times. For example, suppose you have three versions of the employees table in the recycle bin and you want to recover the second version. You can query the recycle bin and then restore to the appropriate system-generated name, as shown in the following example. Including the drop time in the query can help you verify that you are restoring the correct table.\nSELECT objname, oldname, droptime FROM pg_catalog.pg_recyclebin; Result:\n objname | oldname | droptime -----------------------+-----------+-------------------- pg_dropped_table_18762 | employees | 2016-02-05 21:05:52 pg_dropped_table_18924 | employees | 2016-02-05 21:25:13 pg_dropped_table_19510 | employees | 2016-02-05 22:05:53 Restore the dropped table:\nCREATE TABLE hr.employees2 (LIKE pg_recyclebin.pg_dropped_table_18924 INCLUDING ALL); INSERT INTO hr.employees2 SELECT * FROM pg_recyclebin.pg_dropped_table_18924; "},{"id":18,"href":"/admin/network-tablespace/","title":"Network Attached Tablespace","parent":"Server Administration","content":"   Planning Deploying Compute Server Deploying Storage Server Create a Network Attached Database Create a Network Attached Tablespace      Redrock Postgres supports network attached tablespaces. Based on this capability, Redrock Postgres can deploy database computing and storage separately.\nPlanning     Prepare the computing server and storage server environments. The recommended environment configurations are as follows:\n Computing servers should use high processor and memory configurations. The storage server should use the storage device with high I/O throughput, and the storage space must meet the data storage requirements. Use high-bandwidth and low-latency networks between computing servers and storage servers.  If you need to create multiple network tablespaces on the database server, you need to prepare multiple storage servers.\nDeploying Compute Server     To deploy a compute server, perform the following steps:\n Install the software package by referring to Redrock Postgres Installation Guide. Stop the server, if it\u0026rsquo;s running. Perform the backup, using any convenient file-system-backup tool such as tar or cpio (not pg_dump or pg_dumpall). Start the server.  Deploying Storage Server     To deploy the storage server, perform the following steps:\n Install the software package by referring to Redrock Postgres Installation Guide. Stop the server, if it\u0026rsquo;s running. Remove all existing files and subdirectories under the cluster data directory. Restore the database files from your file system backup. Be sure that they are restored with the right ownership (the database system user, not root!) and with the right permissions. Set the listening address and port in postgresql.conf and create a storage.signal file in the cluster data directory. Modify pg_hba.conf to allow connections from the compute server. Start the server.  Create a Network Attached Database     Log in to the computing server, connect to the postgres database in the database service as the database superuser, and create a network attached database. The following is a simple configuration example:\nCREATE DATABASE netdb STORAGE remote LOCATION \u0026#39;host=192.168.1.50 port=5432 user=postgres password=pgpass\u0026#39;;   In Redrock Postgres, creating a database creates a default tablespace in the database, and the newly created database in the example above produces a default network tablespace.  Create a Network Attached Tablespace     Use a database superuser to connect to the newly created database netdb in the database service to create a network attached tablespace. The following is a simple example of a configuration that creates two network attached tablespaces using two additional storage servers:\nCREATE TABLESPACE netts1 STORAGE remote LOCATION \u0026#39;host=192.168.1.51 port=5432 user=postgres password=pgpass\u0026#39;; CREATE TABLESPACE netts2 STORAGE remote LOCATION \u0026#39;host=192.168.1.52 port=5432 user=postgres password=pgpass\u0026#39;; You can then use these network attached tablespaces to create tables and indexes.\n"},{"id":19,"href":"/commands/","title":"SQL Commands","parent":"Redrock Documentation","content":"This part contains reference information for the SQL commands supported by Redrock Postgres. By SQL the language in general is meant; information about the standards conformance and compatibility of each command can be found on the respective reference page.\nTable of Contents\n ALTER UNDO  change the definition of an undo CREATE UNDO  define a new undo DROP UNDO  remove an undo VACUUM  garbage-collect a database  "},{"id":20,"href":"/features/textsearch/","title":"Full Text Search","parent":"Features","content":"   Parsers Configurations Testing and Debugging Text Search  Configuration Testing Parser Testing        Full Text Searching (or just text search) provides the capability to identify natural-language documents that satisfy a query, and optionally to sort them by relevance to the query. The most common type of search is to find all documents containing given query terms and return them in order of their similarity to the query. Notions of query and similarity are very flexible and depend on the specific application. The simplest search considers query as a set of words and similarity as the frequency of query words in the document.\nParsers     Text search parsers are responsible for splitting raw document text into tokens and identifying each token\u0026rsquo;s type, where the set of possible types is defined by the parser itself. Note that a parser does not modify the text at all  it simply identifies plausible word boundaries. Because of this limited scope, there is less need for application-specific custom parsers than there is for custom dictionaries. At present Redrock Postgres provides just two built-in parser, which has been found to be useful for a wide range of applications.\nThere are two built-in text parsers, a default text parser called pg_catalog.default, and a chinese text parser called pg_catalog.cjkparser.\nConfigurations     Full text search functionality includes the ability to do many more things: skip indexing certain words (stop words), process synonyms, and use sophisticated parsing, e.g., parse based on more than just white space. This functionality is controlled by text search configurations. PostgreSQL comes with predefined configurations for many languages, and you can easily create your own configurations. (psql\u0026rsquo;s \\dF command shows all available configurations.)\nGenerally, the built-in text search configuration is named after the name of the corresponding language, and the built-in english text search configuration is called pg_catalog.english.\nTesting and Debugging Text Search     The behavior of a custom text search configuration can easily become confusing. The functions described in this section are useful for testing text search objects. You can test a complete configuration, or test parsers and dictionaries separately.\nConfiguration Testing     The function ts_debug allows easy testing of a text search configuration.\nSELECT * FROM ts_debug(\u0026#39;english\u0026#39;, \u0026#39;relational database management system\u0026#39;); Result:\n alias | description | token | dictionaries | dictionary | lexemes\r-----------+-----------------+------------+----------------+--------------+-----------\rasciiword | Word, all ASCII | relational | {english_stem} | english_stem | {relat}\rblank | Space symbols | | {} | |\rasciiword | Word, all ASCII | database | {english_stem} | english_stem | {databas}\rblank | Space symbols | | {} | |\rasciiword | Word, all ASCII | management | {english_stem} | english_stem | {manag}\rblank | Space symbols | | {} | |\rasciiword | Word, all ASCII | system | {english_stem} | english_stem | {system} Parser Testing     The function ts_parse allows direct testing of a text search parser.\nSELECT * FROM ts_parse(\u0026#39;default\u0026#39;, \u0026#39;relational database management system\u0026#39;); Result:\n tokid | token\r-------+------------\r1 | relational\r12 |\r1 | database\r12 |\r1 | management\r12 |\r1 | system "},{"id":21,"href":"/features/event-triggers/","title":"Event Triggers","parent":"Features","content":"   Overview of Event Trigger Behavior Event Trigger Firing Matrix Event Trigger Functions  Capturing Changes at Command End Processing Objects Dropped by a DDL Command Handling a Table Rewrite Event Deparsing a DDL command        Redrock Postgres provides event triggers. Unlike regular triggers, which are attached to a single table and capture only DML events, event triggers are global to a particular database and are capable of capturing DDL events.\nLike regular triggers, event triggers can be written in any procedural language that includes event trigger support, but not in plain SQL.\nOverview of Event Trigger Behavior     An event trigger fires whenever the event with which it is associated occurs in the database in which it is defined. Currently, the only supported events are ddl_command_start, ddl_command_end, table_rewrite and sql_drop. Support for additional events may be added in future releases.\nThe ddl_command_start event occurs just before the execution of a CREATE, ALTER, DROP, SECURITY LABEL, COMMENT, GRANT or REVOKE command. No check whether the affected object exists or doesn\u0026rsquo;t exist is performed before the event trigger fires. As an exception, however, this event does not occur for DDL commands targeting shared objects  databases  or for commands targeting event triggers themselves. The event trigger mechanism does not support these object types. ddl_command_start also occurs just before the execution of a SELECT INTO command, since this is equivalent to CREATE TABLE AS.\nThe ddl_command_end event occurs just after the execution of this same set of commands. To obtain more details on the DDL operations that took place, use the set-returning function pg_event_trigger_ddl_commands() from the ddl_command_end event trigger code (see Event Trigger Functions). Note that the trigger fires after the actions have taken place (but before the transaction commits), and thus the system catalogs can be read as already changed.\nThe sql_drop event occurs just before the ddl_command_end event trigger for any operation that drops database objects. To list the objects that have been dropped, use the set-returning function pg_event_trigger_dropped_objects() from the sql_drop event trigger code (see Event Trigger Functions). Note that the trigger is executed after the objects have been deleted from the system catalogs, so it\u0026rsquo;s not possible to look them up anymore.\nThe table_rewrite event occurs just before a table is rewritten by some actions of the commands ALTER TABLE and ALTER TYPE. While other control statements are available to rewrite a table, like CLUSTER and VACUUM, the table_rewrite event is not triggered by them.\nEvent triggers (like other functions) cannot be executed in an aborted transaction. Thus, if a DDL command fails with an error, any associated ddl_command_end triggers will not be executed. Conversely, if a ddl_command_start trigger fails with an error, no further event triggers will fire, and no attempt will be made to execute the command itself. Similarly, if a ddl_command_end trigger fails with an error, the effects of the DDL statement will be rolled back, just as they would be in any other case where the containing transaction aborts.\nFor a complete list of commands supported by the event trigger mechanism, see Event Trigger Firing Matrix.\nEvent triggers are created using the command CREATE EVENT TRIGGER. In order to create an event trigger, you must first create a function with the special return type event_trigger. This function need not (and may not) return a value; the return type serves merely as a signal that the function is to be invoked as an event trigger.\nIf more than one event trigger is defined for a particular event, they will fire in alphabetical order by trigger name.\nA trigger definition can also specify a WHEN condition so that, for example, a ddl_command_start trigger can be fired only for particular commands which the user wishes to intercept. A common use of such triggers is to restrict the range of DDL operations which users may perform.\nEvent Trigger Firing Matrix     Table 1 lists all commands for which event triggers are supported.\nTable 1. Event Trigger Support by Command Tag\n   Command Tag ddl_command_start ddl_command_end sql_drop table_rewrite Notes     ALTER AGGREGATE X X - -    ALTER COLLATION X X - -    ALTER CONVERSION X X - -    ALTER DOMAIN X X - -    ALTER DEFAULT PRIVILEGES X X - -    ALTER EXTENSION X X - -    ALTER FOREIGN DATA WRAPPER X X - -    ALTER FOREIGN TABLE X X X -    ALTER FUNCTION X X - -    ALTER LANGUAGE X X - -    ALTER LARGE OBJECT X X - -    ALTER MATERIALIZED VIEW X X - -    ALTER OPERATOR X X - -    ALTER OPERATOR CLASS X X - -    ALTER OPERATOR FAMILY X X - -    ALTER POLICY X X - -    ALTER PROCEDURE X X - -    ALTER PUBLICATION X X - -    ALTER ROLE X X - -    ALTER ROUTINE X X - -    ALTER SCHEMA X X - -    ALTER SEQUENCE X X - -    ALTER SERVER X X - -    ALTER STATISTICS X X - -    ALTER SUBSCRIPTION X X - -    ALTER TABLE X X X X    ALTER TABLESPACE X X - -    ALTER TEXT SEARCH CONFIGURATION X X - -    ALTER TEXT SEARCH DICTIONARY X X - -    ALTER TEXT SEARCH PARSER X X - -    ALTER TEXT SEARCH TEMPLATE X X - -    ALTER TRIGGER X X - -    ALTER TYPE X X - X    ALTER USER X X - -    ALTER USER MAPPING X X - -    ALTER VIEW X X - -    COMMENT X X - - Only for local objects   CREATE ACCESS METHOD X X - -    CREATE AGGREGATE X X - -    CREATE CAST X X - -    CREATE COLLATION X X - -    CREATE CONVERSION X X - -    CREATE DOMAIN X X - -    CREATE EXTENSION X X - -    CREATE FOREIGN DATA WRAPPER X X - -    CREATE FOREIGN TABLE X X - -    CREATE FUNCTION X X - -    CREATE INDEX X X - -    CREATE LANGUAGE X X - -    CREATE MATERIALIZED VIEW X X - -    CREATE OPERATOR X X - -    CREATE OPERATOR CLASS X X - -    CREATE OPERATOR FAMILY X X - -    CREATE POLICY X X - -    CREATE PROCEDURE X X - -    CREATE PUBLICATION X X - -    CREATE ROLE X X - -    CREATE RULE X X - -    CREATE SCHEMA X X - -    CREATE SEQUENCE X X - -    CREATE SERVER X X - -    CREATE STATISTICS X X - -    CREATE SUBSCRIPTION X X - -    CREATE TABLE X X - -    CREATE TABLE AS X X - -    CREATE TABLESPACE X X - -    CREATE TEXT SEARCH CONFIGURATION X X - -    CREATE TEXT SEARCH DICTIONARY X X - -    CREATE TEXT SEARCH PARSER X X - -    CREATE TEXT SEARCH TEMPLATE X X - -    CREATE TRIGGER X X - -    CREATE TYPE X X - -    CREATE USER X X - -    CREATE USER MAPPING X X - -    CREATE VIEW X X - -    DROP ACCESS METHOD X X X -    DROP AGGREGATE X X X -    DROP CAST X X X -    DROP COLLATION X X X -    DROP CONVERSION X X X -    DROP DOMAIN X X X -    DROP EXTENSION X X X -    DROP FOREIGN DATA WRAPPER X X X -    DROP FOREIGN TABLE X X X -    DROP FUNCTION X X X -    DROP INDEX X X X -    DROP LANGUAGE X X X -    DROP MATERIALIZED VIEW X X X -    DROP OPERATOR X X X -    DROP OPERATOR CLASS X X X -    DROP OPERATOR FAMILY X X X -    DROP OWNED X X X -    DROP POLICY X X X -    DROP PROCEDURE X X X -    DROP PUBLICATION X X X -    DROP ROLE X X X -    DROP ROUTINE X X X -    DROP RULE X X X -    DROP SCHEMA X X X -    DROP SEQUENCE X X X -    DROP SERVER X X X -    DROP STATISTICS X X X -    DROP SUBSCRIPTION X X X -    DROP TABLE X X X -    DROP TABLESPACE X X X -    DROP TEXT SEARCH CONFIGURATION X X X -    DROP TEXT SEARCH DICTIONARY X X X -    DROP TEXT SEARCH PARSER X X X -    DROP TEXT SEARCH TEMPLATE X X X -    DROP TRIGGER X X X -    DROP TYPE X X X -    DROP USER X X X -    DROP USER MAPPING X X X -    DROP VIEW X X X -    GRANT X X - - Only for local objects   IMPORT FOREIGN SCHEMA X X - -    REFRESH MATERIALIZED VIEW X X - -    REVOKE X X - - Only for local objects   SECURITY LABEL X X - - Only for local objects   SELECT INTO X X - -     Event Trigger Functions     Redrock Postgres provides these helper functions to retrieve information from event triggers.\nCapturing Changes at Command End     pg_event_trigger_ddl_commands returns a list of DDL commands executed by each user action, when invoked in a function attached to a ddl_command_end event trigger. If called in any other context, an error is raised. pg_event_trigger_ddl_commands returns one row for each base command executed; some commands that are a single SQL sentence may return more than one row. This function returns the following columns:\n   Name Type Description     classid oid OID of catalog the object belongs in   objid oid OID of the object itself   objsubid integer Sub-object ID (e.g., attribute number for a column)   command_tag text Command tag   object_type text Type of the object   schema_name text Name of the schema the object belongs in, if any; otherwise NULL. No quoting is applied.   object_identity text Text rendering of the object identity, schema-qualified. Each identifier included in the identity is quoted if necessary.   in_extension bool True if the command is part of an extension script   command pg_ddl_command A complete representation of the command, in internal format. This cannot be output directly, but it can be passed to other functions to obtain different pieces of information about the command.    Processing Objects Dropped by a DDL Command     pg_event_trigger_dropped_objects returns a list of all objects dropped by the command in whose sql_drop event it is called. If called in any other context, pg_event_trigger_dropped_objects raises an error. pg_event_trigger_dropped_objects returns the following columns:\n   Name Type Description     classid oid OID of catalog the object belonged in   objid oid OID of the object itself   objsubid integer Sub-object ID (e.g., attribute number for a column)   original bool True if this was one of the root object(s) of the deletion   normal bool True if there was a normal dependency relationship in the dependency graph leading to this object   is_temporary bool True if this was a temporary object   object_type text Type of the object   schema_name text Name of the schema the object belonged in, if any; otherwise NULL. No quoting is applied.   object_name text Name of the object, if the combination of schema and name can be used as a unique identifier for the object; otherwise NULL. No quoting is applied, and name is never schema-qualified.   object_identity text Text rendering of the object identity, schema-qualified. Each identifier included in the identity is quoted if necessary.   address_names text[] An array that, together with object_type and address_args, can be used by the pg_get_object_address() function to recreate the object address in a remote server containing an identically named object of the same kind   address_args text[] Complement for address_names    The pg_event_trigger_dropped_objects function can be used in an event trigger like this:\nCREATE FUNCTION test_event_trigger_for_drops() RETURNS event_trigger AS $$DECLAREobjrecord;BEGINFORobjINSELECT*FROMpg_event_trigger_dropped_objects()LOOPRAISENOTICE\u0026#39;% dropped object: % %.% %\u0026#39;,tg_tag,obj.object_type,obj.schema_name,obj.object_name,obj.object_identity;ENDLOOP;END;$$ LANGUAGE plpgsql; CREATE EVENT TRIGGER test_event_trigger_for_drops ON sql_drop EXECUTE PROCEDURE test_event_trigger_for_drops(); Handling a Table Rewrite Event     The functions shown in Table 2 provide information about a table for which a table_rewrite event has just been called. If called in any other context, an error is raised.\nTable 2. Table Rewrite Information\n   Name Return Type Description     pg_event_trigger_table_rewrite_oid() Oid The OID of the table about to be rewritten.   pg_event_trigger_table_rewrite_reason() int The reason code(s) explaining the reason for rewriting. The exact meaning of the codes is release dependent.    The pg_event_trigger_table_rewrite_oid function can be used in an event trigger like this:\nCREATE FUNCTION test_event_trigger_table_rewrite_oid() RETURNS event_trigger AS $$BEGINRAISENOTICE\u0026#39;rewriting table % for reason %\u0026#39;,pg_event_trigger_table_rewrite_oid()::regclass,pg_event_trigger_table_rewrite_reason();END;$$ LANGUAGE plpgsql; CREATE EVENT TRIGGER test_table_rewrite_oid ON table_rewrite EXECUTE PROCEDURE test_event_trigger_table_rewrite_oid(); Deparsing a DDL command     pg_ddl_command_deparse deparses a command from internal format of type pg_ddl_command, and returns a SQL statement in textual representation of type text.\nThe pg_ddl_command_deparse function can be used in an event trigger like this:\nCREATE TABLE ddl_history( ord int, op_time timestamp, username text, command_tag text, object_type text, schema_name text, object_identity text, command text ); CREATE OR REPLACE FUNCTION ddl_trigger_func() RETURNS event_trigger AS $$BEGININSERTINTOddl_historySELECTordinality,now(),current_user,command_tag,object_type,schema_name,object_identity,pg_ddl_command_deparse(command)FROMpg_event_trigger_ddl_commands()WITHORDINALITY;END;$$ LANGUAGE plpgsql; CREATE EVENT TRIGGER deparse_event_trigger ON ddl_command_end EXECUTE PROCEDURE ddl_trigger_func(); "},{"id":22,"href":"/posts/","title":"News","parent":"Redrock Documentation","content":""},{"id":23,"href":"/admin/ddl-subscription/","title":"Subscribe DDL Commands","parent":"Server Administration","content":"   Record and Publish DDL Commands  Create DDL history table Publish DDL history table Create event trigger capturing DDL commands Configure the recording and publishing   Subscribe and Replicate DDL Commands  Create the same DDL history table Subscribe DDL history table Replicate DDL commands with table level triggers Configure the DDL replication        PostgreSQL provides event triggers. Event triggers are global to a particular database and are capable of capturing DDL events occured in the database. Redrock Postgres makes the following improvements to PostgreSQL so that users can subscribe DDL commands with event triggers:\n In Redrock Postgres, users and tablespaces are database-level objects. Event triggers also support DDL commands for users and tablespace objects. For details, see Event Trigger Firing Matrix. Support capturing events of dropping objects at the end of a command (corresponding to the ddl_command_end event). Add a function pg_ddl_command_deparse to deparse a command from internal format of type pg_ddl_command, and returns a SQL statement in textual representation of type text.  The following example shows how to subscribe and replicate DDL commands occured on the publisher side. You can modify the commands in the example according to the actual situation.\nRecord and Publish DDL Commands     We need to record DDL commands in a log table with event trigger on the publisher side, and publish the log table. Before proceeding, make sure the value of the configuration parameter wal_level on the publisher side is logical, and the database server needs to be restarted to take effect after the parameter is modified. Then, you can login to the database using a superuser or database owner and do the following:\nCreate DDL history table     Creating a log table to record DDL commands:\nCREATE SCHEMA IF NOT EXISTS audit; REVOKE ALL ON SCHEMA audit FROM public; -- create table for ddl record CREATE TABLE IF NOT EXISTS audit.ddl_history( ord int, event_time timestamp, username text, object_type text, schema_name text, object_identity text, command_tag text, command text ); -- grant privileges to all user GRANT INSERT ON TABLE audit.ddl_history TO PUBLIC; The table audit.ddl_history includes the following columns:\n   Name Description     ord The number of DDL subcommand decomposed from the current DDL query   event_time The time when the DDL command was executed   username The user who executed the DDL command   object_type Type of the object. Possible types are table, index, sequence, view, materialized view, foreign table, aggregate, function, type, cast, collation, rule, trigger, schema, role, tablespace, foreign data wrapper, server, user mapping, extension, policy, publication and subscription.   schema_name Name of the schema the object belongs in, if any; otherwise NULL. No quoting is applied.   object_identity Text rendering of the object identity, schema-qualified. Each identifier included in the identity is quoted if necessary.   command_tag Command tag. Possible values see Event Trigger Support by Command Tag.   command the DDL command in textual representation    Publish DDL history table     Publish the log table:\nCREATE PUBLICATION ddl_publication FOR TABLE ONLY audit.ddl_history; Create event trigger capturing DDL commands     -- create function for event triggers CREATE OR REPLACE FUNCTION audit.ddl_pub_trigger_func() RETURNS event_trigger AS $$BEGININSERTINTOaudit.ddl_historySELECTordinality,now(),current_user,object_type,schema_name,object_identity,command_tag,pg_ddl_command_deparse(command)FROMpg_event_trigger_ddl_commands()WITHORDINALITY;END;$$ LANGUAGE plpgsql; -- create ddl_command_end event trigger CREATE EVENT TRIGGER ddl_event_trigger ON ddl_command_end EXECUTE PROCEDURE audit.ddl_pub_trigger_func();   After the preceding commands are executed, your DDL commands will be recorded in the table audit.ddl_history.  Configure the recording and publishing     You can configure the recording and publishing of DDL commands by modifying the DDL event trigger function definition. Referring to the definition of the table audit.ddl_history in above, you can configure the recording and publishing of DDL commands by adding filter conditions to the definition of DDL event trigger functions based on the user who executed the DDL command, the type of object affected, the schema the object belongs in, the object name, and the command tag. For example, we can modify the definition of the above event trigger function audit.ddl_pub_trigger_func to limit the schemas and object types involved in the recorded and published DDL commands as follows:\nCREATE OR REPLACE FUNCTION audit.ddl_pub_trigger_func() RETURNS event_trigger AS $$BEGININSERTINTOaudit.ddl_historySELECTordinality,now(),current_user,object_type,schema_name,object_identity,command_tag,pg_ddl_command_deparse(command)FROMpg_event_trigger_ddl_commands()WITHORDINALITYWHEREschema_nameIN(\u0026#39;testschema\u0026#39;,\u0026#39;myschema\u0026#39;)ANDobject_typeIN(\u0026#39;table\u0026#39;,\u0026#39;index\u0026#39;);END;$$ LANGUAGE plpgsql; In the example above, we limit the schemas involved in DDL commands to testschema and myschema, and limit the object types to table and index.\nSubscribe and Replicate DDL Commands     On the publisher side, we recorded the executed DDL commands in the log table audit.ddl_history, and published the table. Subscribers can replicate and execute the DDL commands in the log table. Login to the database using a superuser or database owner and do the following:\nCreate the same DDL history table     On the subscriber side, create a DDL history table same as the table on the publisher side:\nCREATE SCHEMA IF NOT EXISTS audit; REVOKE ALL ON SCHEMA audit FROM public; -- create table for ddl record CREATE TABLE IF NOT EXISTS audit.ddl_history( ord int, event_time timestamp, username text, object_type text, schema_name text, object_identity text, command_tag text, command text ); -- grant privileges to all user GRANT INSERT ON TABLE audit.ddl_history TO PUBLIC; Subscribe DDL history table     Subscribe the log table:\nCREATE SUBSCRIPTION ddl_subscriptin CONNECTION \u0026#39;host=192.168.1.50 port=5432 dbname=testdb user=testuser password=pgpass\u0026#39; PUBLICATION ddl_publication;   You need to modify the access information of the publisher in the above SQL command according to the actual deployment environment.  Replicate DDL commands with table level triggers     Create a trigger for the table audit.ddl_history on the subscriber side to replicate and execute DDL commands.\nCREATE OR REPLACE FUNCTION audit.ddl_sub_trigger_func() RETURNS trigger AS $$BEGINEXECUTENEW.command;RETURNNEW;END;$$ LANGUAGE plpgsql; CREATE TRIGGER ddl_sub_trigger AFTER INSERT ON audit.ddl_history FOR EACH ROW EXECUTE PROCEDURE audit.ddl_sub_trigger_func(); Configure the DDL replication     You can configure the replication of DDL commands by modifying the table level trigger function definition. Referring to the definition of the table audit.ddl_history in above, you can configure the replication of DDL commands by adding filter conditions to the definition of DDL event trigger functions based on the user who executed the DDL command, the type of object affected, the schema the object belongs in, the object name, and the command tag. For example, we can modify the definition of the above table level trigger function audit.ddl_sub_trigger_func to limit the schemas and object types involved in the replicated DDL commands as follows:\nCREATE OR REPLACE FUNCTION audit.ddl_sub_trigger_func() RETURNS trigger AS $$BEGINIFNEW.schema_nameIN(\u0026#39;testschema\u0026#39;,\u0026#39;myschema\u0026#39;)ANDNEW.object_typeIN(\u0026#39;table\u0026#39;,\u0026#39;index\u0026#39;)THENEXECUTENEW.command;ENDIF;RETURNNEW;END;$$ LANGUAGE plpgsql; In the example above, we limit the schemas involved in DDL commands to testschema and myschema, and limit the object types to table and index.\n"},{"id":24,"href":"/catalogs/","title":"System Catalogs","parent":"Redrock Documentation","content":"Table of Contents\n   pg_recyclebin      pg_ts_lexicon      pg_undo      The system catalogs are the place where a relational database management system stores schema metadata, such as information about tables and columns, and internal bookkeeping information. Redrock Postgres\u0026rsquo;s system catalogs are regular tables. You can drop and recreate the tables, add columns, insert and update values, and severely mess up your system that way. Normally, one should not change the system catalogs by hand, there are normally SQL commands to do that. (For example, CREATE DATABASE inserts a row into the pg_database catalog  and actually creates the database on disk.) There are some exceptions for particularly esoteric operations, but many of those have been made available as SQL commands over time, and so the need for direct manipulation of the system catalogs is ever decreasing.\n"},{"id":25,"href":"/features/postgres-compatibility/","title":"Compatibility with PostgreSQL","parent":"Features","content":"   Unsupported features Features different from PostgreSQL  User Tablespace PID   Unsupported Configuration Options Unsupported Storage Parameters      Redrock Postgres is developed based on PostgreSQL. Redrock Postgres is compatible with PostgreSQL on client protocols and SQL syntax. If your application is developed based on PostgreSQL, then:\n Based on the binary compatibility with PostgreSQL on client protocols, Redrock Postgres can be accessed using the same drivers as PostgreSQL for various programming languages, such as Java, C/C++, .Net, Python, Ruby, Perl, Go, ODBC. Due to SQL compatibility with PostgreSQL, you can migrate the application software to Redrock Postgres without any modification. You can use PostgreSQL-related desktop management tools to access Redrock Postgres, such as DBeaver, pgAdmin 4, and Navicat for PostgreSQL.  That means in most cases, you can use pg_dump to export data from PostgreSQL, uninstall PostgreSQL and install Redrock Postgres, import the exported data into Redrock Postgres, and you are good to go.\nUnsupported features      VACUUM FULL is deprecated and only syntax remains VACUUM FREEZE is deprecated and only syntax remains System Columns related to transaction: xmin, cmin, xmax, cmax Serializable Isolation Level SP-GiST Indexes  Features different from PostgreSQL     User     In PostgreSQL, users are cluster-level objects, while in Redrock Postgres, users are database-level objects. That means when you access different database objects in the same cluster, you may see users with the same name, in fact, they are different user objects belonging to different databases.\n DBA should pay attention to this difference, while developers may not.  Tablespace     In PostgreSQL, tablespaces are cluster-level objects, while in Redrock Postgres, tablespaces are database-level objects. That means when you access different database objects in the same cluster, you may see tablespaces with the same name, in fact, they are different tablespace objects in the respective databases.\n DBA should pay attention to this difference, while developers may not.  PID     Some statistics views, such as pg_stat_activity and pg_locks, contain fields named pid. In PostgreSQL, these fields indicate the operating system process ID. In Redrock Postgres, these fields indicate the database process ID.\n There is a field named spid in pg_stat_activity view, indicating the actual operating system process ID.  Unsupported Configuration Options     If you are using any of the following options in your postgresql.conf file you should remove or rename them.\n vacuum_freeze_table_age vacuum_freeze_min_age vacuum_multixact_freeze_table_age vacuum_multixact_freeze_min_age vacuum_cleanup_index_scale_factor autovacuum renamed to autoanalyze log_autovacuum_min_duration renamed to log_autoanalyze_min_duration autovacuum_max_workers renamed to autoanalyze_max_workers autovacuum_naptime renamed to autoanalyze_naptime autovacuum_vacuum_threshold autovacuum_analyze_threshold renamed to autoanalyze_base_threshold autovacuum_vacuum_scale_factor autovacuum_analyze_scale_factor renamed to autoanalyze_scale_factor autovacuum_freeze_max_age autovacuum_multixact_freeze_max_age autovacuum_vacuum_cost_delay autovacuum_vacuum_cost_limit  Unsupported Storage Parameters     If you are using any of the following Storage Parameters in your table definitions you should remove or rename them.\n autovacuum_enabled, toast.autovacuum_enabled renamed to autoanalyze_enabled, toast.autoanalyze_enabled vacuum_index_cleanup, toast.vacuum_index_cleanup vacuum_truncate, toast.vacuum_truncate autovacuum_vacuum_threshold, toast.autovacuum_vacuum_threshold autovacuum_vacuum_scale_factor, toast.autovacuum_vacuum_scale_factor autovacuum_analyze_threshold renamed to autoanalyze_base_threshold autovacuum_analyze_scale_factor renamed to autoanalyze_scale_factor autovacuum_vacuum_cost_delay, toast.autovacuum_vacuum_cost_delay autovacuum_vacuum_cost_limit, toast.autovacuum_vacuum_cost_limit autovacuum_freeze_min_age, toast.autovacuum_freeze_min_age autovacuum_freeze_max_age, toast.autovacuum_freeze_max_age autovacuum_freeze_table_age, toast.autovacuum_freeze_table_age autovacuum_multixact_freeze_min_age, toast.autovacuum_multixact_freeze_min_age autovacuum_multixact_freeze_max_age, toast.autovacuum_multixact_freeze_max_age autovacuum_multixact_freeze_table_age, toast.autovacuum_multixact_freeze_table_age log_autovacuum_min_duration, toast.log_autovacuum_min_duration renamed to log_autoanalyze_min_duration, toast.log_autoanalyze_min_duration  "},{"id":26,"href":"/admin/audit-trigger/","title":"Audit Logging","parent":"Server Administration","content":"   Create Audit Table Audit DDL Commands  Create event trigger capturing DDL commands Configure auditing of DDL commands   Audit DML Commands  Create trigger function Audit DML commands on tables        PostgreSQL provides event triggers. Event triggers are global to a particular database and are capable of capturing DDL events occured in the database. Redrock Postgres makes the following improvements to PostgreSQL so that users can audit DDL commands with event triggers:\n In Redrock Postgres, users and tablespaces are database-level objects. Event triggers also support DDL commands for users and tablespace objects. For details, see Event Trigger Firing Matrix. Support capturing events of dropping objects at the end of a command (corresponding to the ddl_command_end event). Add a function pg_ddl_command_deparse to deparse a command from internal format of type pg_ddl_command, and returns a SQL statement in textual representation of type text.  The following example shows how to audit DDL and DML commands occured in the database. You can modify the commands in the example according to the actual situation.\nCreate Audit Table     Login to the database using a superuser or database owner, and create the audit log table to record events:\nCREATE SCHEMA IF NOT EXISTS audit; REVOKE ALL ON SCHEMA audit FROM public; -- create table for audit logging CREATE TABLE IF NOT EXISTS audit.logged_actions( event_time timestamp, username text, object_type text, schema_name text, object_identity text, application_name text, client_addr inet, client_port integer, command_tag text, command text ); -- grant privileges to all user GRANT INSERT ON TABLE audit.logged_actions TO PUBLIC; The table audit.logged_actions includes the following columns:\n   Name Description     event_time When the operation is performed   username The user who performed the operation   object_type Type of the object. Possible types are table, index, sequence, view, materialized view, foreign table, aggregate, function, type, cast, collation, rule, trigger, schema, role, tablespace, foreign data wrapper, server, user mapping, extension, policy, publication and subscription.   schema_name Name of the schema the object belongs in, if any; otherwise NULL. No quoting is applied.   object_identity Text rendering of the object identity, schema-qualified. Each identifier included in the identity is quoted if necessary.   application_name The name of the client application that performed the operation   client_addr The client network address from where the operation is performed   client_port The client network port from where the operation is performed   command_tag Command tag. Possible values see Event Trigger Support by Command Tag.   command The SQL command in textual representation    Audit DDL Commands     We can use event trigger to capture DDL events that occur in the database, and record DDL commands into the audit log table.\nCreate event trigger capturing DDL commands     -- create function for event trigger CREATE OR REPLACE FUNCTION audit.ddl_trigger_func() RETURNS event_trigger AS $$BEGININSERTINTOaudit.logged_actionsSELECTnow(),current_user,object_type,schema_name,object_identity,current_setting(\u0026#39;application_name\u0026#39;),inet_client_addr(),inet_client_port(),command_tag,pg_ddl_command_deparse(command)FROMpg_event_trigger_ddl_commands();END;$$ LANGUAGE plpgsql; -- create ddl_command_end event trigger CREATE EVENT TRIGGER audit_event_trigger ON ddl_command_end EXECUTE PROCEDURE audit.ddl_trigger_func();   After the preceding commands are executed, your DDL commands will be recorded in the table audit.logged_actions.  Configure auditing of DDL commands     You can configure the auditing of DDL commands by modifying the DDL event trigger function definition. Referring to the definition of the table audit.logged_actions in above, you can configure the auditing of DDL commands according to the user who executed the DDL command, the type of object affected, the schema the object belongs in, the object name, the command tag, as well as the client application name, the client network address, the client network port, etc. For example, we can modify the definition of the above event trigger function audit.ddl_trigger_func to limit the object types involved in the audited DDL commands as follows:\nCREATE OR REPLACE FUNCTION audit.ddl_trigger_func() RETURNS event_trigger AS $$BEGININSERTINTOaudit.logged_actionsSELECTnow(),current_user,object_type,schema_name,object_identity,current_setting(\u0026#39;application_name\u0026#39;),inet_client_addr(),inet_client_port(),command_tag,pg_ddl_command_deparse(command)FROMpg_event_trigger_ddl_commands();WHEREobject_typeIN(\u0026#39;table\u0026#39;,\u0026#39;role\u0026#39;);END;$$ LANGUAGE plpgsql; In the example above, we limit the object types involved in DDL commands to table and role.\nAudit DML Commands     We can create a generic trigger function used for recording changes to tables into an audit log table. It will record the SQL statement, the table affected, the user who made the change, and a timestamp for each change.\nCreate trigger function     CREATE OR REPLACE FUNCTION audit.if_modified_func() RETURNS TRIGGER AS $$BEGINIFTG_WHEN\u0026lt;\u0026gt;\u0026#39;AFTER\u0026#39;ORTG_LEVEL\u0026lt;\u0026gt;\u0026#39;STATEMENT\u0026#39;THENRAISEEXCEPTION\u0026#39;audit.if_modified_func() may only run as statement level AFTER trigger\u0026#39;;ENDIF;INSERTINTOaudit.logged_actionsVALUES(now(),current_user,\u0026#39;table\u0026#39;,TG_TABLE_SCHEMA,TG_TABLE_NAME,current_setting(\u0026#39;application_name\u0026#39;),inet_client_addr(),inet_client_port(),TG_OP,current_query());END;$$ LANGUAGE plpgsql; Audit DML commands on tables     Create triggers on tables that need to be audited to audit table data modification operations:\nCREATE TRIGGER trigger_audit_testtbl AFTER INSERT OR UPDATE OR DELETE OR TRUNCATE ON testtbl FOR EACH STATEMENT EXECUTE PROCEDURE audit.if_modified_func();   The above SQL command audits INSERT, UPDATE, DELETE, and TRUNCATE operations on the table testtbl. You need to modify the table name and trigger name in the SQL command according to your actual case. Alternatively, you can choose to audit only some kinds of operations on the table, for example, only DELETE and TRUNCATE operations on the table.  "},{"id":27,"href":"/admin/online-migration/","title":"Online Migration","parent":"Server Administration","content":"   Initial Checks Configuration Changes Schema Migration Migrate table data Verification Clean up      PostgreSQL provides the ability of logical replication. We can make an zero-downtime online migration through logical replication, and online migration can be applied in the following scenarios:\n Upgrade the version of the database software; Upgrade the version of the operating system software, or change the operating system type, such as from Windows to Linux; Upgrade the specifications of the hardware, or change the architecture of the hardware, such as changing from X86 to ARM; Adjust the database deployment schema, such as splitting a large database into multiple smaller databases; Migrate a database deployment environment, such as from a server in one public cloud to another;  If your database is large, it may take some time to transfer over the network. In addition, online migration based on logical replication supports only database-based or table-based migrations. We\u0026rsquo;ll cover database-based migrations in the following steps. In this section, we will perform an online migration of the lrtest database.\nInitial Checks     If you don\u0026rsquo;t already have a primary key for each table that you plan to replicate, create a primary key. The following query shows which tables do not have a primary key. The query should appear as 0 tables before you start the migration.\nSELECT n.nspname AS schema, c.relname AS table FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE c.relkind = \u0026#39;r\u0026#39; AND NOT EXISTS ( SELECT 1 FROM pg_constraint con WHERE con.conrelid = c.oid AND con.contype = \u0026#39;p\u0026#39;) AND n.nspname NOT IN (\u0026#39;pg_catalog\u0026#39;, \u0026#39;pg_recyclebin\u0026#39;, \u0026#39;information_schema\u0026#39;); If you do not get a value of row 0 here, you need to create a primary key on the queried table to replicate the table.\nConfiguration Changes     An important thing to note is that if you have adjusted postgresql.conf for memory, maximum number of connections, etc., these parameters will not be copied during logical copying. Before migration, execute the following queries on the original and target databases, and compare the query results in detail.\nSELECT name, setting, unit, boot_val FROM pg_settings; Where the view pg_settings provides parameter information when the database server is running. If you find that there are differences between the parameter values between the original database and the target database during the parameter comparison process, you may need to modify the corresponding parameter configuration in the target database and restart the target database so that the configuration changes take effect.\nSince logical replication requires a replicating user, we will execute the following query on the original database to create a user with replication permission.\nCREATE USER logicalrep REPLICATION PASSWORD \u0026#39;pgpass\u0026#39;; After creating the replication user, we need to change the pg_hba.conf configuration file on the original database to allow the target database to connect to the replication data on the original database. Since the database server reads \u0026lsquo;pg_hba.conf\u0026rsquo; from top to bottom, this means that you need to place these rows appropriately according to the configuration rules:\nhost lrtest logicalrep 192.168.1.51/32 md5 After changing the pg_hba.conf, we need to set the wal_level parameter of the original database to logical.\nALTER SYSTEM SET wal_level TO logical; After we have changed the configuration above, we need to restart the original database for the configuration changes to take effect.\nSchema Migration     For the original database to be migrated, use pg_dump for schema dump. This will allow us to recover the schema information in the target database.\npg_dump -Fc -s -h 192.168.1.50 -d lrtest -U postgres -f /tmp/lrtest_schema.dmp Use pg_restore to import a schema dump to the target database.\npg_restore -C -h 192.168.1.51 -d lrtest -U postgres /tmp/lrtest_schema.dmp Setup Record and Publish DDL Operations in the original database and Subscribe and Synchronize DDL Operations in the target database. Note that if you need to migrate all table data in the database in the steps of migrating table data below, you do not need to publish the table audit.ddl_history on the original database, nor do you need to subscribe to the table audit.ddl_history on the target database.\nMigrate table data     Now that the schema information has been imported into the target database, we can create a publication by running the following command on the original database. As mentioned above, we are about to migrate all the tables in the database. If you only want to copy certain tables in the database, you can pass the FOR TABLE when using the CREATE PUBLICATION command.\nCREATE PUBLICATION lrtest_migrate FOR ALL TABLES; Run the following command on the target database to subscribe all tables data and changes on the original database:\nCREATE SUBSCRIPTION lrtest_migrate CONNECTION \u0026#39;host=192.168.1.50 dbname=lrtest user=logicalrep password=pgpass\u0026#39; PUBLICATION lrtest_migrate; Verification     Next, we need to verify that the data of the original database and the target database are consistent. Run the following command on the original database to create the aggregate function used to calculate the checksum of the table data.\nCREATE FUNCTION md5_agg_sfunc(text, text) RETURNS text AS \u0026#39;SELECT md5($1 || $2)\u0026#39; LANGUAGE sql; CREATE AGGREGATE md5_agg ( BASETYPE = text, STYPE = text, SFUNC = md5_agg_sfunc, INITCOND = \u0026#39;\u0026#39; );   Since we have already set the synchronization of DDL operations in the above steps of migrating schema, the functions created on the original database will be automatically synchronized to the target database, so there is no need to run the above commands on the target database.  Then, for each table in the database, we can execute the following query on the original and target databases separately and confirm that the results of the queries on both sides are consistent.\nSELECT md5_agg(t::text) AS checksum FROM (SELECT * FROM t_table ORDER BY table_key) AS t; If you find that there is a difference between the original database and the target database, you can execute the following query on each of the two databases to verify the data in the table in batches.\nSELECT md5_agg(t::text) AS checksum, max(table_key) AS maxval FROM ( SELECT * FROM t_table WHERE table_key \u0026gt; key_val ORDER BY table_key LIMIT 100000 ) AS t;   In the query above, table_key is the primary key field in the table t_table and key_val is the primary key column value. When you start your first query, set key_val to be less than the minimum primary key column value in the table, and in the subsequent batch validation process, set key_val to the maximum primary key column value returned by the previous query.  Clean up     The data from the original database was successfully copied to the target database. We went even further and confirmed that the original and target database data were consistent. At this point the logical replication is running, and you can leave it in place until you are ready to switch. If all your data has been copied and you have completed the conversion, you can delete the subscription.\nDROP SUBSCRIPTION lrtest_migrate; Now that the online migration process has completed successfully, we can remove the original database if you no longer need it.\n"},{"id":28,"href":"/posts/initial-release/","title":"Redrock Postgres version 12.1-1 Released","parent":"News","content":"The Redrock Development Team is pleased to announce Redrock Postgres 12 version 12.1-1.\nRedrock is an object-relational database management system (ORDBMS) based on PostgreSQL, developed by the PostgreSQL Global Development Group. It is intentionally designed as an enterprise grade and cloud native database.\nFor more information, please see the website.\nFeature overview:\n Undo: Log record for roll back modified data and transactions. Multitenant: Single database instance runs on a server and serves multiple tenants. Network based tablespace: Devide computing and storage functions in database through network based tablespace. Multithread: The multithreaded database model enables server processes to execute as operating system threads in separate address spaces. Recyclebin: Provide recycle bin to reserve dropped objects to prevent mistake. DDL event tracking: Tracking, recording, publishing and subscribing DDL operations.  "},{"id":29,"href":"/commands/alterundo/","title":"ALTER UNDO","parent":"SQL Commands","content":"ALTER UNDO  change the definition of an undo\nSynopsis     ALTER UNDO [ IF EXISTS ] name RENAME TO new_name\rALTER UNDO [ IF EXISTS ] name SET TABLESPACE tablespace_name\rALTER UNDO name OWNER TO { new_owner | CURRENT_USER | SESSION_USER }\rALTER UNDO [ IF EXISTS ] name SET ( storage_parameter = value [, ... ] )\rALTER UNDO [ IF EXISTS ] name RESET ( storage_parameter [, ... ] ) Description     ALTER UNDO changes the definition of an existing undo. There are several subforms described below. Note that the lock level required may differ for each subform. An ACCESS EXCLUSIVE lock is held unless explicitly noted. When multiple subcommands are listed, the lock held will be the strictest one required from any subcommand.\n  RENAME\nThe RENAME form changes the name of the undo. There is no effect on the stored data.\nRenaming an index acquires a SHARE UPDATE EXCLUSIVE lock.\n  SET TABLESPACE\nThis form changes the undo\u0026rsquo;s tablespace to the specified tablespace and moves the data file(s) associated with the undo to the new tablespace. To change the tablespace of an undo, you must own the undo and have CREATE privilege on the new tablespace.\n  OWNER TO\nThis form changes the undo\u0026rsquo;s owner to the specified owner. To alter the owner, you must also be a direct or indirect member of the new owning role. (Note that superusers have all these privileges automatically.)\n  SET ( storage_parameter=value [, ... ] )\nThis form changes one or more index-method-specific storage parameters for the undo. See CREATE UNDO for details on the available parameters. Note that the undo contents will not be modified immediately by this command; depending on the parameter you might need to clean the undo with VACUUM to get the desired effects.\n  RESET ( storage_parameter [, ... ] )\nThis form resets one or more undo-specific storage parameters to their defaults. As with SET, a VACUUM might be needed to clean the undo entirely.\n  Parameters       IF EXISTS\nDo not throw an error if the undo does not exist. A notice is issued in this case.\n  name\nThe name (possibly schema-qualified) of an existing undo to alter.\n  new_name\nThe new name for the undo.\n  tablespace_name\nThe tablespace to which the undo will be moved.\n  new_owner\nThe new owner of the undo.\n  storage_parameter\nThe name of an undo-specific storage parameter.\n  value\nThe new value for an undo-specific storage parameter. This might be a number or a word depending on the parameter.\n  Examples     To rename an existing undo:\nALTER UNDO undo_abc RENAME TO undo_test; To move an undo to a different tablespace:\nALTER UNDO undo_15 SET TABLESPACE undospace; To change an undo\u0026rsquo;s minimum pages:\nALTER UNDO undo_15 SET (minpages = 131072); Change the owner of undo undo_15:\nALTER UNDO undo_15 OWNER TO mary; Compatibility     ALTER UNDO is a Redrock Postgres extension.\n"},{"id":30,"href":"/commands/createundo/","title":"CREATE UNDO","parent":"SQL Commands","content":"CREATE UNDO  define a new undo\nSynopsis     CREATE UNDO [ IF NOT EXISTS ] name\r[ WITH ( storage_parameter = value [, ... ] ) ]\r[ TABLESPACE tablespace_name ] Description     CREATE UNDO constructs a new undo in the current database. The undo name must be distinct from the name of any existing undo in the current database. Undos are primarily used to hold old version data that has been modified in tables and indexes.\nTo create a undo, the invoking user must have the CREATE privilege for the current database. (Of course, superusers bypass this check.)\nParameters       IF NOT EXISTS\nDo not throw an error if a relation with the same name already exists. A notice is issued in this case. Note that there is no guarantee that the existing undo is anything like the one that would have been created.\n  name\nThe name of the undo to be created. No schema name can be included here; the undo is always created in the schema pg_catalog.\n  storage_parameter\nThe name of an undo-specific storage parameter. See Undo Storage Parameters for details.\n  tablespace_name\nThe tablespace in which to create the undo. If not specified, default_tablespace is consulted.\n  Undo Storage Parameters     The optional WITH clause specifies storage parameters for the undo. Accepted parameters include:\n  initialpages\nNumber of initial pages in undo. The default is 128.\n  minpages\nMinimum number of pages in undo. The default is 128.\n  maxpages\nMaximum number of pages in undo. The default is unlimited. It is just a hint, in fact, Redrock Postgres provides a fully automated mechanism, known as automatic undo management mode, for managing undos and space.\n  Notes     After the database cluster data directory is initialized, 4 cluster-level undos and 8 database-level undos are generated in the database by default, the cluster-level undos is mainly used to modify the data of the shared system catalogs, such as pg_database. The first page of the undo is used to store transaction table items, and a single undo can hold up to about 200 transaction items at the same time.\nHowever, if a single undo processes a large number of transactions at the same time (for example, more than 8), a corresponding page access wait event will occur due to concurrent access violations of the transaction table. In this case, you can use the CREATE UNDO command to create more undos in the current database to improve the transaction processing performance of the system.\nExamples     Create a undo:\nCREATE UNDO undo_13; To create an undo and have the undo reside in the tablespace undospace:\nCREATE UNDO undo_14 TABLESPACE undospace; Compatibility     CREATE UNDO is a Redrock Postgres language extension. There are no provisions for undos in the SQL standard.\n"},{"id":31,"href":"/commands/dropundo/","title":"DROP UNDO","parent":"SQL Commands","content":"DROP UNDO  remove an undo\nSynopsis     DROP UNDO [ IF EXISTS ] name [, ...] [ CASCADE | RESTRICT ] Description     DROP UNDO drops an existing undo from the database system. To execute this command you must be the owner of the undo.\nParameters       IF EXISTS\nDo not throw an error if the undo does not exist. A notice is issued in this case.\n  name\nThe name (optionally schema-qualified) of an undo to remove.\n  CASCADE\nAutomatically drop objects that depend on the undo, and in turn all objects that depend on those objects (see Section 5.14).\n  RESTRICT\nRefuse to drop the undo if any objects depend on it. This is the default.\n  Examples     This command will remove the undo undo_15:\nDROP UNDO undo_15; Compatibility     DROP UNDO is a Redrock Postgres language extension. There are no provisions for undos in the SQL standard.\n"},{"id":32,"href":"/catalogs/recyclebin/","title":"pg_recyclebin","parent":"System Catalogs","content":"The catalog pg_recyclebin contains part of the information about dropped objects. The rest is mostly in pg_classpg_typepg_proc and pg_constraint.\nTable. pg_recyclebin Columns\n   Name Type References Description     classid oid pg_class.oid The OID of the system catalog the dropped object is in   objid oid any OID column The OID of the specific dropped object   objsubid oid  When classid is the OID of the system catalog pg_namespace, this is the user OID of the schema owner. For all other object types, this column is 0.   namespace oid pg_namespace.oid The OID of the schema the dropped object is in   objname name any NAME column New name of the object   oldname name any NAME column Original name of the object   command char  Operation carried out on the object: d - Object was dropped r - Object was rewritten, eg: ALTER TABLE  t - Object was truncated   droptime timestamptz  Timestamp for the dropping of the object    "},{"id":33,"href":"/catalogs/ts-lexicon/","title":"pg_ts_lexicon","parent":"System Catalogs","content":"The pg_ts_lexicon catalog contains word entries defining text search lexicons.\nTable. pg_ts_lexicon Columns\n   Name Type Description     lexfreq int4 The frequency statistics of the word in the text   lextype int2 The type of the word. Possible values are:0: chinese words1: chinese units2: english and chinese mixed words3: chinese and english mixed words4: chinese last name5: chinese second name6: initial word of chinese compound surname7: last word of chinese compound surname8: chinese surname modifiers9: stop words10: english punctuation mixed words11: english words15: other words16: synonym words17: punctuations   lexkind char the part of speech of the word in the sentence, include: noun (n), pronoun (p), adjective (a), adverb (d), verb (v), numeral (m), article (t), preposition (r), conjunction (c), interjection (i)   lexword text literal representation of the word   lexsynonyms text[] synonyms for the word    "},{"id":34,"href":"/catalogs/undo/","title":"pg_undo","parent":"System Catalogs","content":"The catalog pg_undo contains part of the information about undo segments. The rest is mostly in pg_class.\nTable. pg_undo Columns\n   Name Type Description     undoid oid The ID of this undo segment, the system assigns sequentially from 1   undrelid oid The OID of the pg_class entry for this undo segment   undusecount int8 The number of times the transaction table entry in this undo segment has been reused, this field will only be updated when the undo segment is deleted   undreusetime logicaltime The logical timestamp of when the transaction table entry in this undo segment was reused, this field will only be updated when the undo segment is deleted   undisvalid bool If true, the undo segment can currently be used to provide read consistency. False means the undo segment has been completely deleted   undislive bool If false, the undo segment has been deleted    "},{"id":35,"href":"/","title":"Redrock Documentation","parent":"","content":"Redrock is an object-relational database management system (ORDBMS) based on PostgreSQL, developed by the PostgreSQL Global Development Group. It is intentionally designed as an enterprise grade and cloud native database.\nGetting Started   Feature overview   Undo   Log record for roll back modified data and transactions.  Multitenant   Single database instance runs on a server and serves multiple tenants.  Network Attached Tablespace   Devide computing and storage functions in database through network attached tablespace.   Multithread   Single process with multiple worker threads, less overhead per context switch.  Recycle Bin   Provide recycle bin to reserve dropped objects to prevent mistake.  DDL Event Tracking   Tracking, auditing, publishing and subscribing DDL operations.   "},{"id":36,"href":"/tags/","title":"Tags","parent":"Redrock Documentation","content":""},{"id":37,"href":"/commands/vacuum/","title":"VACUUM","parent":"SQL Commands","content":"VACUUM  garbage-collect a database\nSynopsis     VACUUM [ ( option [, ...] ) ] [ name [, ...] ]\rVACUUM [ VERBOSE ] [ name [, ...] ]\rwhere option can be one of:\rVERBOSE [ boolean ]\rSKIP_LOCKED [ boolean ]\rTRUNCATE [ boolean ] Description     VACUUM is used to do the following:\n Attempts to truncate off any empty pages at the end of the table and allow the disk space for the truncated pages to be returned to the operating system. Attempts to shrink undos. Clearing dropped objects in the recycle bin.  Without a name list, VACUUM processes every table, materialized view, undo, and dropped object in recycle bin in the current database that the current user has permission to vacuum. With a list, VACUUM processes only those table(s).\nWhen the option list is surrounded by parentheses, the options can be written in any order. Without parentheses, options must be specified in exactly the order shown above.\nParameters       VERBOSE\nPrints a detailed vacuum activity report for each table.\n  SKIP_LOCKED\nSpecifies that VACUUM should not wait for any conflicting locks to be released when beginning work on a relation: if a relation cannot be locked immediately without waiting, the relation is skipped. Note that even with this option, VACUUM may still block when opening the relation\u0026rsquo;s indexes. Also, while VACUUM ordinarily processes all partitions of specified partitioned tables, this option will cause VACUUM to skip all partitions if there is a conflicting lock on the partitioned table.\n  TRUNCATE\nSpecifies that VACUUM should attempt to truncate off any empty pages at the end of the table and allow the disk space for the truncated pages to be returned to the operating system. This is normally the desired behavior and is the default unless the vacuum_truncate option has been set to false for the table to be vacuumed. Setting this option to false may be useful to avoid ACCESS EXCLUSIVE lock on the table that the truncation requires.\n  boolean\nSpecifies whether the selected option should be turned on or off. You can write TRUE, ON, or 1 to enable the option, and FALSE, OFF, or 0 to disable it. The boolean value can also be omitted, in which case TRUE is assumed.\n  name\nThe name (optionally schema-qualified) of a specific table, materialized view or undo to vacuum. If the specified table is a partitioned table, all of its leaf partitions are vacuumed.\n  Outputs     When VERBOSE is specified, VACUUM emits progress messages to indicate which table is currently being processed. Various statistics about the tables are printed as well.\nNotes     To vacuum a table, one must ordinarily be the table\u0026rsquo;s owner or a superuser. However, database owners are allowed to vacuum all tables in their databases, except shared catalogs. (The restriction for shared catalogs means that a true database-wide VACUUM can only be performed by a superuser.) VACUUM will skip over any tables that the calling user does not have permission to vacuum.\nVACUUM cannot be executed inside a transaction block.\nExamples     To clean a single table onek, and print a detailed vacuum activity report:\nVACUUM (VERBOSE) onek; Compatibility     There is no VACUUM statement in the SQL standard.\n"}]